import '../models/course.dart';

// Java Topics
final List<Topic> javaTopics = [
  Topic(
    id: 'java_intro',
    title: '1. Introduction to Java',
    explanation: '''Java is a high-level, class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible. It is a general-purpose programming language intended to let application developers write once, run anywhere (WORA), meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.

## Java Architecture
The Java architecture consists of several components:

1. **Java Development Kit (JDK)**: Contains tools needed for Java development, including:
   - Java Runtime Environment (JRE)
   - Compiler (javac)
   - Debugger (jdb)
   - Documentation generator (javadoc)
   - Archive tool (jar)

2. **Java Runtime Environment (JRE)**: Provides the libraries, Java Virtual Machine (JVM), and other components to run Java applications.

3. **Java Virtual Machine (JVM)**: Executes Java bytecode and provides platform independence. Key components include:
   - Class Loader Subsystem: Loads, links, and initializes the class files
   - Runtime Data Areas: Memory areas such as Method Area, Heap, Java Stacks, PC Registers, and Native Method Stacks
   - Execution Engine: Includes interpreter, JIT compiler, and garbage collector

## Java Execution Process
1. Java source code (.java files) is written by a programmer.
2. Compiler (javac) compiles the source code into bytecode (.class files).
3. JVM loads, verifies, and executes the bytecode.
4. The bytecode is platform-independent, but each operating system has its own JVM implementation.

## Memory Management in Java
Java uses automatic memory management through garbage collection:
- **Young Generation**: Newly created objects start here
- **Old Generation**: Objects that survive several garbage collection cycles
- **Permanent Generation/Metaspace**: Class definitions, method data, etc.

Garbage Collection algorithms include:
- Serial GC
- Parallel GC
- Concurrent Mark Sweep (CMS)
- G1 (Garbage-First) Collector

## Java Language Fundamentals
- **Primitive Data Types**: boolean, byte, short, char, int, long, float, double
- **Reference Types**: Classes, interfaces, arrays, enums
- **Type Conversion**: Widening (implicit) and narrowing (explicit) conversions
- **Operators**: Arithmetic, relational, logical, bitwise, assignment, etc.
- **Control Flow**: if-else, switch, loops, break, continue, return
- **Exception Handling**: try-catch-finally, throws, throw
''',
    codeSnippet: '''
// Basic Java program structure
package com.example;

/**
 * Documentation comment for the HelloWorld class
 * @author ExampleDeveloper
 * @version 1.0
 */
public class HelloWorld {
    // Class variable (static)
    private static String message = "Hello, World!";
    
    // Instance variable
    private int count = 0;
    
    /**
     * Main method - entry point of the program
     * @param args Command-line arguments
     */
    public static void main(String[] args) {
        // Local variable declaration
        int localVar = 10;
        
        // Create an instance of the class
        HelloWorld instance = new HelloWorld();
        
        // Call instance method
        instance.displayMessage();
        
        // Demonstrate primitive types
        byte byteVar = 127;
        short shortVar = 32767;
        int intVar = 2147483647;
        long longVar = 9223372036854775807L;
        float floatVar = 3.14f;
        double doubleVar = 3.14159265358979;
        char charVar = 'A';
        boolean boolVar = true;
        
        // String is not primitive but commonly used
        String str = "Java Programming";
        
        System.out.println("Primitive types demonstration: ");
        System.out.println("byte: " + byteVar);
        System.out.println("int: " + intVar);
        System.out.println("String: " + str);
        
        // Demonstrate control flow
        if (boolVar) {
            System.out.println("Condition is true");
        }
        
        // Exception handling
        try {
            // Code that might throw an exception
            int result = intVar / 0;
        } catch (ArithmeticException e) {
            System.out.println("Exception caught: " + e.getMessage());
        } finally {
            System.out.println("Finally block executed");
        }
    }
    
    /**
     * Instance method to display the message
     */
    public void displayMessage() {
        count++;
        System.out.println(message + " (displayed " + count + " times)");
    }
}
''',
    revisionPoints: [
      'Java is platform-independent due to bytecode and JVM',
      'Java is object-oriented with encapsulation, inheritance, polymorphism, and abstraction',
      'Java is strongly-typed, requiring explicit type declaration',
      'Java has automatic memory management through generational garbage collection',
      'Java uses JIT (Just-In-Time) compilation for performance optimization',
      'Java provides built-in multi-threading and concurrency support',
      'Java enforces exception handling for robust error management',
      'Java ensures type safety and memory safety through runtime checks',
      'Java APIs provide extensive libraries for networking, IO, collections, and more',
      'Java supports annotations for metadata and reflection for introspection'
    ],
    quizQuestions: [
      Question(
        question: 'Which of the following statements about JVM is NOT true?',
        options: [
          'JVM is platform-dependent and must be implemented separately for each OS',
          'JVM translates bytecode back into source code for execution',
          'JVM includes a Just-In-Time compiler to improve performance',
          'JVM has a garbage collector to handle memory management'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'In the Java memory model, which area stores method-specific values and local variables?',
        options: ['Heap', 'Method Area', 'Stack', 'Program Counter Register'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the significance of the "public static void main(String[] args)" signature?',
        options: [
          'It allows the method to be called without creating an instance of the class',
          'It makes the main method accessible to the JVM as an entry point',
          'It enables command-line arguments to be passed to the application',
          'All of the above'
        ],
        correctIndex: 3,
      ),
      Question(
        question: 'Which of the following is the correct sequence of events during Java program execution?',
        options: [
          'Compile to bytecode → Run JVM → Load classes → Execute program',
          'Load classes → Compile to bytecode → Run JVM → Execute program',
          'Run JVM → Compile to bytecode → Load classes → Execute program',
          'Compile to bytecode → Load classes → Verify bytecode → Execute program'
        ],
        correctIndex: 3,
      ),
      Question(
        question: 'Which memory region in the JVM is subject to garbage collection?',
        options: [
          'Method Area',
          'Heap',
          'PC Registers',
          'Native Method Stack'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the default value of an instance variable of type double in Java?',
        options: ['0', '0.0d', 'null', 'undefined'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following statements about the Java language is true?',
        options: [
          'Java supports multiple inheritance of classes',
          'Java is a fully interpreted language',
          'Java supports operator overloading directly',
          'Java supports pass-by-reference for primitive data types'
        ],
        correctIndex: 3,
      ),
      Question(
        question: 'What is the time complexity of accessing an element in a HashMap by its key?',
        options: ['O(1) in average case', 'O(n)', 'O(log n)', 'O(n²)'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which statement about Java\'s type system is NOT correct?',
        options: [
          'Java is statically typed, checking types at compile time',
          'Java performs automatic type conversions (coercion) when it\'s safe',
          'In Java, all primitive types can be autoboxed into their wrapper classes',
          'Java allows implicit downcasting without explicit type casting'
        ],
        correctIndex: 3,
      ),
      Question(
        question: 'When was Java first released, and by which company?',
        options: [
          '1991 by Sun Microsystems',
          '1995 by Sun Microsystems',
          '1995 by Oracle Corporation',
          '1998 by Microsoft'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_oop',
    title: '2. OOP Concepts in Java',
    explanation: '''## Object-Oriented Programming in Java

Object-oriented programming (OOP) is a programming paradigm based on the concept of "objects," which can contain data (attributes) and code (methods). Java is fundamentally object-oriented, with everything except primitive types being objects.
''',
    codeSnippet: '',
    conceptSections: [
      ConceptSection(
        heading: 'Encapsulation',
        explanation: '''Encapsulation is the mechanism of wrapping data (variables) and code (methods) together as a single unit. It restricts direct access to some components, preventing unauthorized access and manipulation.

**Key aspects:**
- **Access modifiers**: Control visibility (public, private, protected, default/package-private)
- **Getters and setters**: Control access to private variables
- **Immutability**: Create read-only objects using final variables and methods

**Benefits:**
- Data hiding and protection
- Maintenance flexibility
- Control over data validation
''',
        codeSnippet: '''// Encapsulation example
class BankAccount {
    private String accountNumber;
    private double balance;
    private static final double MIN_BALANCE = 100.0;

    public BankAccount(String accountNumber, double initialBalance) {
        this.accountNumber = accountNumber;
        this.balance = initialBalance;
    }

    public double getBalance() {
        return balance;
    }

    public void deposit(double amount) {
        if (amount > 0) {
            balance += amount;
            System.out.println("Deposited: " + amount);
        } else {
            System.out.println("Invalid deposit amount");
        }
    }

    public boolean withdraw(double amount) {
        if (amount > 0 && (balance - amount) >= MIN_BALANCE) {
            balance -= amount;
            System.out.println("Withdrawn: " + amount);
            return true;
        }
        System.out.println("Withdrawal failed. Insufficient funds or invalid amount.");
        return false;
    }
}
''',
      ),
      ConceptSection(
        heading: 'Inheritance & Polymorphism',
        explanation: '''Inheritance is a mechanism in which one class acquires properties (methods and fields) of another class. It establishes an "is-a" relationship between classes.
Polymorphism allows objects to be treated as instances of their parent class rather than their actual class. It enables one interface to be used for different underlying forms.
''',
        codeSnippet: '''// Inheritance & Polymorphism example
abstract class Vehicle {
    protected String make;
    protected String model;
    protected int year;

    public Vehicle(String make, String model, int year) {
        this.make = make;
        this.model = model;
        this.year = year;
    }

    public abstract void start();

    public void stop() {
        System.out.println("Vehicle stopped");
    }

    public void displayInfo() {
        System.out.println("Vehicle: " + make + " " + model + " (" + year + ")");
    }
}

class Car extends Vehicle {
    private int numDoors;

    public Car(String make, String model, int year, int numDoors) {
        super(make, model, year);
        this.numDoors = numDoors;
    }

    @Override
    public void start() {
        System.out.println("Car started with ignition key");
    }

    @Override
    public void displayInfo() {
        super.displayInfo();
        System.out.println("Number of doors: " + numDoors);
    }

    public void accelerate() {
        System.out.println("Car accelerating");
    }

    public void accelerate(int speedIncrement) {
        System.out.println("Car accelerating by " + speedIncrement + " mph");
    }
}

class Motorcycle extends Vehicle {
    private boolean hasSidecar;

    public Motorcycle(String make, String model, int year, boolean hasSidecar) {
        super(make, model, year);
        this.hasSidecar = hasSidecar;
    }

    @Override
    public void start() {
        System.out.println("Motorcycle started with kickstart or button");
    }

    @Override
    public void displayInfo() {
        super.displayInfo();
        System.out.println("Has sidecar: " + hasSidecar);
    }
}
''',
      ),
      ConceptSection(
        heading: 'Interface & Multiple Inheritance',
        explanation: '''Abstraction focuses on the essential qualities of an object rather than the specific characteristics. It hides complex implementation details and shows only necessary features. Java supports abstraction through abstract classes and interfaces.
''',
        codeSnippet: '''// Interface example
interface ElectricVehicle {
    void charge();
    int getBatteryLevel();

    default void showChargeStatus() {
        System.out.println("Battery level is: " + getBatteryLevel() + "%");
    }
}

class ElectricCar extends Car implements ElectricVehicle {
    private int batteryLevel;

    public ElectricCar(String make, String model, int year, int numDoors) {
        super(make, model, year, numDoors);
        this.batteryLevel = 50; // Default 50%
    }

    @Override
    public void charge() {
        batteryLevel = 100;
        System.out.println("Electric car charged to 100%");
    }

    @Override
    public int getBatteryLevel() {
        return batteryLevel;
    }

    @Override
    public void start() {
        if (batteryLevel > 10) {
            System.out.println("Electric car started silently");
        } else {
            System.out.println("Battery too low to start");
        }
    }
}
''',
      ),
    ],
    revisionPoints: [
      'Encapsulation: Data hiding through private fields, access control via methods, maintains invariants and ensures validation',
      'Inheritance: "is-a" relationship, code reuse, supports single inheritance for classes and multiple inheritance for interfaces',
      'Polymorphism: Compile-time (method overloading) and Runtime (method overriding), enables dynamic method binding and interface-based programming',
      'Abstraction: Implementation hiding via abstract classes and interfaces, reduces complexity, separates interface from implementation',
      'Java class hierarchies: Object as the root class, class inheritance chain, interface hierarchies',
      'Access modifiers: private, protected, default (package-private), public control visibility across different scopes',
      'Method binding: Static binding at compile time, dynamic binding at runtime for overridden methods',
      'Association relationships: Dependency, Association, Aggregation, and Composition define different object relationships',
      'SOLID principles ensure clean, maintainable object-oriented design',
      'Design patterns offer reusable solutions to common OOP design problems'
    ],
    quizQuestions: [
      Question(
        question: 'Which OOP concept allows a class to inherit properties from another class?',
        options: ['Encapsulation', 'Abstraction', 'Inheritance', 'Polymorphism'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which keyword is used to inherit a class in Java?',
        options: ['implements', 'extends', 'inherit', 'super'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which access modifier makes class members visible only within the class?',
        options: ['public', 'protected', 'private', 'default'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is method overriding an example of?',
        options: ['Encapsulation', 'Inheritance', 'Polymorphism', 'Abstraction'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which of the following is not a pillar of OOP?',
        options: ['Encapsulation', 'Inheritance', 'Compilation', 'Polymorphism'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_exceptions',
    title: '3. Exception Handling',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Exception Handling** is a mechanism to handle **runtime errors** in a program, preventing the program from crashing.
* An **exception** is an event that **disrupts normal program flow**.

**Key Points:**

* Java uses **try-catch-finally** blocks for handling exceptions.
* Exceptions improve **code reliability, readability, and robustness**.
* Java categorizes exceptions into **checked, unchecked, and errors**.

---

## **B. Types of Exceptions**

| Type                    | Description                                   | Examples                                      |
| ----------------------- | --------------------------------------------- | --------------------------------------------- |
| **Checked Exception**   | Must be **handled at compile time**           | `IOException`, `SQLException`                 |
| **Unchecked Exception** | Occur at runtime, not required to be declared | `ArithmeticException`, `NullPointerException` |
| **Error**               | Serious problems **not meant to be caught**   | `OutOfMemoryError`, `StackOverflowError`      |

---

## **C. Try-Catch Block**

```java
public class ExceptionDemo {
    public static void main(String[] args) {
        try {
            int a = 10, b = 0;
            int result = a / b; // may throw ArithmeticException
            System.out.println("Result: " + result);
        } catch (ArithmeticException e) {
            System.out.println("Cannot divide by zero: " + e.getMessage());
        }
        System.out.println("Program continues...");
    }
}
```

* **`try` block:** Code that may throw an exception.
* **`catch` block:** Handles the exception.

---

## **D. Finally Block**

* Executes **always**, regardless of exception occurrence.

```java
try {
    int[] arr = new int[3];
    System.out.println(arr[5]);
} catch (ArrayIndexOutOfBoundsException e) {
    System.out.println("Index out of bounds!");
} finally {
    System.out.println("Finally block executed.");
}
```

* Useful for **closing resources** like files, DB connections, or sockets.

---

## **E. Throw and Throws**

**1. `throw`** – Used to **explicitly throw an exception**.

```java
public void checkAge(int age) {
    if (age < 18) {
        throw new IllegalArgumentException("Age must be 18 or above");
    }
}
```

**2. `throws`** – Declares exceptions a method may throw.

```java
public void readFile(String filename) throws IOException {
    FileReader file = new FileReader(filename);
}
```

---

## **F. Custom Exception**

* Create your own exception by **extending Exception or RuntimeException**.

```java
class InvalidAgeException extends Exception {
    public InvalidAgeException(String message) {
        super(message);
    }
}

public class TestCustomException {
    public static void checkAge(int age) throws InvalidAgeException {
        if (age < 18) {
            throw new InvalidAgeException("Age must be 18+");
        }
    }

    public static void main(String[] args) {
        try {
            checkAge(15);
        } catch (InvalidAgeException e) {
            System.out.println("Caught: " + e.getMessage());
        }
    }
}
```

---

## **G. Best Practices / Exam Tips**

* Always **handle checked exceptions** using try-catch or throws.
* Use **specific exceptions**, not `Exception` or `Throwable`.
* Use **finally block** for **resource cleanup**.
* Prefer **unchecked exceptions** for **programming errors**.
* Be familiar with **common Java exceptions**: `NullPointerException`, `ArrayIndexOutOfBoundsException`, `IOException`, etc.
* **Custom exceptions** are useful to convey **specific business logic errors**.
''',
    codeSnippet: '''
// Try-Catch Block Example
public class ExceptionDemo {
    public static void main(String[] args) {
        try {
            int a = 10, b = 0;
            int result = a / b; // ArithmeticException
            System.out.println("Result: " + result);
        } catch (ArithmeticException e) {
            System.out.println("Cannot divide by zero: " + e.getMessage());
        }
        System.out.println("Program continues...");
    }
}

// Finally Block Example
try {
    int[] arr = new int[3];
    System.out.println(arr[5]);
} catch (ArrayIndexOutOfBoundsException e) {
    System.out.println("Index out of bounds!");
} finally {
    System.out.println("Finally block executed.");
}

// Throw Example
public void checkAge(int age) {
    if (age < 18) {
        throw new IllegalArgumentException("Age must be 18 or above");
    }
}

// Throws Example
public void readFile(String filename) throws IOException {
    FileReader file = new FileReader(filename);
}

// Custom Exception
class InvalidAgeException extends Exception {
    public InvalidAgeException(String message) {
        super(message);
    }
}

public class TestCustomException {
    public static void checkAge(int age) throws InvalidAgeException {
        if (age < 18) {
            throw new InvalidAgeException("Age must be 18+");
        }
    }

    public static void main(String[] args) {
        try {
            checkAge(15);
        } catch (InvalidAgeException e) {
            System.out.println("Caught: " + e.getMessage());
        }
    }
}
''',
    revisionPoints: [
      'Exception handling prevents programs from crashing due to runtime errors',
      'Java categorizes exceptions into checked, unchecked, and errors',
      'Checked exceptions must be handled at compile time (IOException, SQLException)',
      'Unchecked exceptions occur at runtime (ArithmeticException, NullPointerException)',
      'Errors are serious problems not meant to be caught (OutOfMemoryError, StackOverflowError)',
      'try block contains code that may throw an exception',
      'catch block handles the exception',
      'finally block executes always, regardless of exception occurrence',
      'finally block is useful for closing resources like files, DB connections, or sockets',
      'throw keyword is used to explicitly throw an exception',
      'throws keyword declares exceptions a method may throw',
      'Custom exceptions are created by extending Exception or RuntimeException',
      'Always handle checked exceptions using try-catch or throws',
      'Use specific exceptions, not Exception or Throwable',
      'Use finally block for resource cleanup',
      'Prefer unchecked exceptions for programming errors',
      'Common exceptions: NullPointerException, ArrayIndexOutOfBoundsException, IOException',
      'Custom exceptions convey specific business logic errors',
    ],
    quizQuestions: [
      Question(
        question: 'Which block executes regardless of whether an exception occurred?',
        options: ['try', 'catch', 'finally', 'throw'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which of the following is a checked exception?',
        options: ['NullPointerException', 'ArithmeticException', 'IOException', 'ArrayIndexOutOfBoundsException'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which keyword is used to manually throw an exception?',
        options: ['throws', 'throw', 'catch', 'try'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is an unchecked exception in Java?',
        options: ['Exception that must be declared', 'Exception that occurs at runtime', 'Exception that cannot be caught', 'Exception that is ignored by JVM'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is the parent class of all exceptions in Java?',
        options: ['Exception', 'Throwable', 'Error', 'RuntimeException'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which keyword declares exceptions a method may throw?',
        options: ['throw', 'throws', 'try', 'catch'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of the finally block?',
        options: ['To throw exceptions', 'To catch exceptions', 'To execute code always', 'To declare exceptions'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which exception is thrown when dividing by zero?',
        options: ['NullPointerException', 'ArithmeticException', 'IOException', 'NumberFormatException'],
        correctIndex: 1,
      ),
      Question(
        question: 'How do you create a custom exception?',
        options: ['Implement Exception interface', 'Extend Exception or RuntimeException class', 'Use @Exception annotation', 'Declare with exception keyword'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is an Error, not an Exception?',
        options: ['IOException', 'SQLException', 'OutOfMemoryError', 'NullPointerException'],
        correctIndex: 2,
      ),
      Question(
        question: 'What happens if no catch block matches the thrown exception?',
        options: ['The program crashes', 'The exception is ignored', 'The finally block handles it', 'The exception is automatically caught by JVM'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which exception is thrown when accessing an invalid array index?',
        options: ['IndexOutOfBoundsException', 'ArrayIndexOutOfBoundsException', 'InvalidIndexException', 'ArrayException'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_collections',
    title: '4. Java Collections Framework',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Java Collections Framework (JCF)** provides **predefined data structures and algorithms** to store, access, and manipulate groups of objects efficiently.
* Includes **interfaces, classes, and algorithms** for collections.

**Key Points:**

* Reduces **development time** by providing reusable data structures.
* Ensures **type safety** and supports **generics**.
* Includes **Lists, Sets, Maps, Queues, and Deques**.

---

## **B. Core Interfaces**

| Interface      | Description                | Common Implementations                |
| -------------- | -------------------------- | ------------------------------------- |
| **Collection** | Root of most collections   | `List`, `Set`, `Queue`                |
| **List**       | Ordered, allows duplicates | `ArrayList`, `LinkedList`, `Vector`   |
| **Set**        | No duplicates, unordered   | `HashSet`, `LinkedHashSet`, `TreeSet` |
| **Queue**      | FIFO structure             | `PriorityQueue`, `LinkedList`         |
| **Deque**      | Double-ended queue         | `ArrayDeque`, `LinkedList`            |
| **Map**        | Key-value pairs            | `HashMap`, `TreeMap`, `LinkedHashMap` |

---

## **C. List Examples**

**1. ArrayList**

```java
import java.util.ArrayList;
import java.util.List;

public class ListDemo {
    public static void main(String[] args) {
        List<String> fruits = new ArrayList<>();
        fruits.add("Apple");
        fruits.add("Banana");
        fruits.add("Mango");

        fruits.remove("Banana");  // remove element
        System.out.println(fruits);

        for (String fruit : fruits) {
            System.out.println(fruit);
        }
    }
}
```

**2. LinkedList**

* Supports **fast insertions/deletions**.
* Can act as **List, Queue, or Deque**.

```java
import java.util.LinkedList;

LinkedList<String> queue = new LinkedList<>();
queue.add("Task1");
queue.add("Task2");
System.out.println(queue.poll()); // removes Task1
```

---

## **D. Set Examples**

```java
import java.util.HashSet;
import java.util.Set;

public class SetDemo {
    public static void main(String[] args) {
        Set<Integer> numbers = new HashSet<>();
        numbers.add(1);
        numbers.add(2);
        numbers.add(1); // duplicate ignored

        System.out.println(numbers); // Output: [1, 2]
    }
}
```

* **HashSet:** Unordered, fast access.
* **LinkedHashSet:** Maintains insertion order.
* **TreeSet:** Sorted order (ascending).

---

## **E. Map Examples**

```java
import java.util.HashMap;
import java.util.Map;

public class MapDemo {
    public static void main(String[] args) {
        Map<String, Integer> scores = new HashMap<>();
        scores.put("Alice", 90);
        scores.put("Bob", 85);
        scores.put("Charlie", 95);

        System.out.println(scores.get("Alice")); // 90

        for (Map.Entry<String, Integer> entry : scores.entrySet()) {
            System.out.println(entry.getKey() + ": " + entry.getValue());
        }
    }
}
```

* **HashMap:** Unordered, allows null keys and values.
* **LinkedHashMap:** Maintains insertion order.
* **TreeMap:** Sorted by keys.

---

## **F. Queue & Deque Examples**

```java
import java.util.ArrayDeque;
import java.util.Deque;

public class DequeDemo {
    public static void main(String[] args) {
        Deque<String> deque = new ArrayDeque<>();
        deque.addFirst("Task1");
        deque.addLast("Task2");

        System.out.println(deque.pollFirst()); // Task1
        System.out.println(deque.pollLast());  // Task2
    }
}
```

* **PriorityQueue:** Elements sorted by natural ordering or comparator.

---

## **G. Algorithms and Utilities (Collections Class)**

```java
import java.util.*;

public class CollectionsDemo {
    public static void main(String[] args) {
        List<Integer> numbers = Arrays.asList(5, 2, 8, 3);
        Collections.sort(numbers);
        System.out.println(numbers); // [2, 3, 5, 8]

        Collections.reverse(numbers);
        System.out.println(numbers); // [8, 5, 3, 2]

        System.out.println(Collections.max(numbers)); // 8
        System.out.println(Collections.min(numbers)); // 2
    }
}
```

* Common utilities: `sort()`, `reverse()`, `shuffle()`, `max()`, `min()`.

---

## **H. Best Practices / Exam Tips**

* Prefer **ArrayList** for **random access**, **LinkedList** for **frequent insertion/deletion**.
* Use **HashSet** for **fast lookups**, **TreeSet** for **sorted data**.
* Always **program to interface**, e.g., `List<String> list = new ArrayList<>();`
* Use **Generics** to avoid `ClassCastException`.
* Understand **differences between List, Set, and Map** for interviews.
''',
    codeSnippet: '''
// ArrayList Example
import java.util.ArrayList;
import java.util.List;

List<String> fruits = new ArrayList<>();
fruits.add("Apple");
fruits.add("Banana");
fruits.add("Mango");
fruits.remove("Banana");
System.out.println(fruits);

// LinkedList Example
import java.util.LinkedList;

LinkedList<String> queue = new LinkedList<>();
queue.add("Task1");
queue.add("Task2");
System.out.println(queue.poll()); // Task1

// HashSet Example
import java.util.HashSet;
import java.util.Set;

Set<Integer> numbers = new HashSet<>();
numbers.add(1);
numbers.add(2);
numbers.add(1); // duplicate ignored
System.out.println(numbers); // [1, 2]

// HashMap Example
import java.util.HashMap;
import java.util.Map;

Map<String, Integer> scores = new HashMap<>();
scores.put("Alice", 90);
scores.put("Bob", 85);
scores.put("Charlie", 95);
System.out.println(scores.get("Alice")); // 90

for (Map.Entry<String, Integer> entry : scores.entrySet()) {
    System.out.println(entry.getKey() + ": " + entry.getValue());
}

// ArrayDeque Example
import java.util.ArrayDeque;
import java.util.Deque;

Deque<String> deque = new ArrayDeque<>();
deque.addFirst("Task1");
deque.addLast("Task2");
System.out.println(deque.pollFirst()); // Task1

// Collections Utilities
import java.util.*;

List<Integer> nums = Arrays.asList(5, 2, 8, 3);
Collections.sort(nums);
Collections.reverse(nums);
System.out.println(Collections.max(nums)); // 8
''',
    revisionPoints: [
      'Java Collections Framework provides predefined data structures and algorithms',
      'JCF reduces development time by providing reusable data structures',
      'Collections ensure type safety and support generics',
      'Core interfaces: Collection, List, Set, Queue, Deque, Map',
      'List is ordered and allows duplicates (ArrayList, LinkedList, Vector)',
      'Set does not allow duplicates (HashSet, LinkedHashSet, TreeSet)',
      'Map stores key-value pairs (HashMap, TreeMap, LinkedHashMap)',
      'Queue follows FIFO structure (PriorityQueue, LinkedList)',
      'Deque is a double-ended queue (ArrayDeque, LinkedList)',
      'ArrayList is best for random access',
      'LinkedList supports fast insertions and deletions',
      'HashSet provides unordered, fast access',
      'LinkedHashSet maintains insertion order',
      'TreeSet maintains sorted order (ascending)',
      'HashMap is unordered and allows null keys/values',
      'LinkedHashMap maintains insertion order',
      'TreeMap is sorted by keys',
      'Collections class provides utilities: sort(), reverse(), shuffle(), max(), min()',
      'Always program to interface (List, Set, Map) not implementation',
      'Use generics to avoid ClassCastException',
    ],
    quizQuestions: [
      Question(
        question: 'Which collection type maintains insertion order and allows duplicates?',
        options: ['Set', 'List', 'Map', 'Queue'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which collection implementation provides constant-time performance for basic operations?',
        options: ['ArrayList', 'LinkedList', 'HashMap', 'TreeMap'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which collection does not allow duplicate elements?',
        options: ['ArrayList', 'LinkedList', 'Vector', 'HashSet'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which interface maps keys to values?',
        options: ['List', 'Set', 'Map', 'Collection'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which collection implementation maintains elements in sorted order?',
        options: ['HashSet', 'ArrayList', 'TreeSet', 'LinkedHashSet'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which is best for random access operations?',
        options: ['LinkedList', 'ArrayList', 'HashSet', 'TreeSet'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which collection maintains insertion order in a Set?',
        options: ['HashSet', 'TreeSet', 'LinkedHashSet', 'ArrayList'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does the poll() method do in a Queue?',
        options: ['Adds an element', 'Removes and returns the first element', 'Returns the first element without removing', 'Clears the queue'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Map implementation maintains sorted keys?',
        options: ['HashMap', 'LinkedHashMap', 'TreeMap', 'HashTable'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the root interface of most collections?',
        options: ['List', 'Collection', 'Iterable', 'Object'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which collection is best for frequent insertions and deletions?',
        options: ['ArrayList', 'LinkedList', 'Vector', 'HashSet'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Collections class method sorts a list?',
        options: ['order()', 'sort()', 'arrange()', 'organize()'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_multithreading',
    title: '5. Multithreading in Java',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Multithreading** is the ability of a Java program to execute **multiple threads concurrently**.
* A **thread** is the **smallest unit of execution** within a process.

**Key Points:**

* Improves **performance, responsiveness, and resource utilization**.
* Java supports **multithreading** via the `Thread` class and `Runnable` interface.
* Threads share **heap memory** but have **individual stack memory**.

---

## **B. Creating Threads**

**1. Extending Thread Class**

```java
class MyThread extends Thread {
    public void run() {
        for(int i=1; i<=5; i++) {
            System.out.println(Thread.currentThread().getName() + ": " + i);
        }
    }
}

public class ThreadDemo {
    public static void main(String[] args) {
        MyThread t1 = new MyThread();
        MyThread t2 = new MyThread();
        t1.start(); // start thread
        t2.start();
    }
}
```

**2. Implementing Runnable Interface**

```java
class MyRunnable implements Runnable {
    public void run() {
        for(int i=1; i<=5; i++) {
            System.out.println(Thread.currentThread().getName() + ": " + i);
        }
    }
}

public class RunnableDemo {
    public static void main(String[] args) {
        Thread t1 = new Thread(new MyRunnable());
        Thread t2 = new Thread(new MyRunnable());
        t1.start();
        t2.start();
    }
}
```

---

## **C. Thread Lifecycle**

| State               | Description                              |
| ------------------- | ---------------------------------------- |
| **New**             | Thread created but not started           |
| **Runnable**        | Ready to run, waiting for CPU scheduling |
| **Running**         | Currently executing                      |
| **Blocked/Waiting** | Waiting for resource or notification     |
| **Terminated**      | Finished execution                       |

---

## **D. Thread Methods**

| Method                    | Description                                                |
| ------------------------- | ---------------------------------------------------------- |
| `start()`                 | Begins thread execution and calls `run()`                  |
| `run()`                   | Contains thread code (do not call directly for new thread) |
| `sleep(ms)`               | Pauses thread for specified milliseconds                   |
| `join()`                  | Waits for thread to finish execution                       |
| `setPriority(int)`        | Sets thread priority (1–10)                                |
| `yield()`                 | Suggests scheduler to switch thread                        |
| `getName()` / `setName()` | Get/set thread name                                        |

---

## **E. Synchronization (Thread Safety)**

* Used to **avoid race conditions** when multiple threads access shared resources.

```java
class Counter {
    private int count = 0;

    public synchronized void increment() {
        count++;
    }

    public int getCount() {
        return count;
    }
}

public class SyncDemo {
    public static void main(String[] args) throws InterruptedException {
        Counter counter = new Counter();
        Thread t1 = new Thread(() -> { for(int i=0;i<1000;i++) counter.increment(); });
        Thread t2 = new Thread(() -> { for(int i=0;i<1000;i++) counter.increment(); });

        t1.start(); t2.start();
        t1.join(); t2.join();
        System.out.println("Final Count: " + counter.getCount());
    }
}
```

* **`synchronized`** ensures **only one thread at a time** can access the method.

---

## **F. Thread Communication (wait, notify, notifyAll)**

```java
class Message {
    private String message;
    private boolean available = false;

    public synchronized void send(String msg) throws InterruptedException {
        while(available) wait();
        message = msg;
        available = true;
        notify();
    }

    public synchronized String receive() throws InterruptedException {
        while(!available) wait();
        available = false;
        notify();
        return message;
    }
}
```

* `wait()` – pauses thread until notified.
* `notify()` – wakes up one waiting thread.
* `notifyAll()` – wakes up all waiting threads.

---

## **G. Executor Framework (Java 5+)**

* Simplifies **thread management using thread pools**.

```java
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class ExecutorDemo {
    public static void main(String[] args) {
        ExecutorService executor = Executors.newFixedThreadPool(3);

        for(int i=1;i<=5;i++) {
            int taskId = i;
            executor.submit(() -> {
                System.out.println(Thread.currentThread().getName() + " executing task " + taskId);
            });
        }

        executor.shutdown();
    }
}
```

* **Advantages:**
  * Reuses threads
  * Reduces thread creation overhead
  * Manages concurrency efficiently

---

## **H. Best Practices / Exam Tips**

* Prefer **Runnable or ExecutorService** over extending Thread.
* Use **synchronized or concurrent collections** for shared resources.
* Avoid **deadlocks** by careful lock ordering.
* Use **thread pools** for better performance.
* Understand **thread lifecycle and methods** for interviews.
''',
    codeSnippet: '''
// Extending Thread Class
class MyThread extends Thread {
    public void run() {
        for(int i=1; i<=5; i++) {
            System.out.println(Thread.currentThread().getName() + ": " + i);
        }
    }
}

MyThread t1 = new MyThread();
MyThread t2 = new MyThread();
t1.start();
t2.start();

// Implementing Runnable Interface
class MyRunnable implements Runnable {
    public void run() {
        for(int i=1; i<=5; i++) {
            System.out.println(Thread.currentThread().getName() + ": " + i);
        }
    }
}

Thread t1 = new Thread(new MyRunnable());
Thread t2 = new Thread(new MyRunnable());
t1.start();
t2.start();

// Synchronization Example
class Counter {
    private int count = 0;

    public synchronized void increment() {
        count++;
    }

    public int getCount() {
        return count;
    }
}

// Thread Communication
class Message {
    private String message;
    private boolean available = false;

    public synchronized void send(String msg) throws InterruptedException {
        while(available) wait();
        message = msg;
        available = true;
        notify();
    }

    public synchronized String receive() throws InterruptedException {
        while(!available) wait();
        available = false;
        notify();
        return message;
    }
}

// Executor Framework
ExecutorService executor = Executors.newFixedThreadPool(3);
for(int i=1;i<=5;i++) {
    int taskId = i;
    executor.submit(() -> {
        System.out.println(Thread.currentThread().getName() + " task " + taskId);
    });
}
executor.shutdown();
''',
    revisionPoints: [
      'Multithreading allows concurrent execution of multiple threads',
      'A thread is the smallest unit of execution within a process',
      'Improves performance, responsiveness, and resource utilization',
      'Java supports multithreading via Thread class and Runnable interface',
      'Threads share heap memory but have individual stack memory',
      'Two ways to create threads: extend Thread or implement Runnable',
      'Thread lifecycle states: New, Runnable, Running, Blocked/Waiting, Terminated',
      'start() begins thread execution and calls run()',
      'run() contains thread code but should not be called directly',
      'sleep(ms) pauses thread for specified milliseconds',
      'join() waits for thread to finish execution',
      'setPriority(int) sets thread priority from 1 to 10',
      'synchronized keyword ensures only one thread accesses method at a time',
      'Synchronization prevents race conditions on shared resources',
      'wait() pauses thread until notified',
      'notify() wakes up one waiting thread',
      'notifyAll() wakes up all waiting threads',
      'ExecutorService simplifies thread management using thread pools',
      'Thread pools reuse threads and reduce creation overhead',
      'Prefer Runnable or ExecutorService over extending Thread',
    ],
    quizQuestions: [
      Question(
        question: 'Which method is used to start a thread\'s execution?',
        options: ['run()', 'start()', 'execute()', 'begin()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is a safe way to create thread-safe code?',
        options: ['Using static methods', 'Using synchronized keyword', 'Using final variables', 'Using private methods'],
        correctIndex: 1,
      ),
      Question(
        question: 'What happens when you call the run() method directly instead of start()?',
        options: ['The thread starts running in parallel', 'A new thread is not created; it runs in the same thread', 'It throws an exception', 'The thread starts but with lower priority'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is not a thread state?',
        options: ['Waiting', 'Running', 'Zombie', 'Blocked'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which interface should a class implement to use as a task in an ExecutorService?',
        options: ['Thread', 'Executor', 'Runnable', 'Callable'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the smallest unit of execution within a process?',
        options: ['Process', 'Thread', 'Task', 'Job'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method pauses a thread for specified milliseconds?',
        options: ['wait()', 'pause()', 'sleep()', 'delay()'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which method waits for a thread to finish execution?',
        options: ['wait()', 'join()', 'finish()', 'complete()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the synchronized keyword prevent?',
        options: ['Deadlocks', 'Race conditions', 'Thread creation', 'Memory leaks'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method wakes up all waiting threads?',
        options: ['notify()', 'notifyAll()', 'wakeAll()', 'resumeAll()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the range of thread priorities in Java?',
        options: ['0 to 9', '1 to 10', '0 to 10', '1 to 100'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main advantage of using ExecutorService?',
        options: ['Creates more threads', 'Reuses threads and reduces overhead', 'Increases memory usage', 'Slows down execution'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_generics_streams',
    title: '6. Generics and Streams in Java',
    explanation: '''## Generics and Streams in Java

### A. Generics

**Definition:**
Generics enable **type-safe code** by allowing classes, interfaces, and methods to operate on objects of **various types** while providing compile-time type checking. They eliminate the need for casting and prevent `ClassCastException`.

**Key Points:**

* Can be used in **classes, interfaces, and methods**.
* Denoted by `<T>`, `<E>`, `<K,V>` etc.
* Supports **bounded types** (`<T extends Number>`) and **wildcards** (`<?>`).

#### 1. Generic Class

A generic class allows you to create a class that can work with any type of data.

```java
class Box<T> {
    private T value;
    
    public void set(T value) { this.value = value; }
    public T get() { return value; }
}

public class Main {
    public static void main(String[] args) {
        Box<String> stringBox = new Box<>();
        stringBox.set("Java");
        System.out.println(stringBox.get()); // Java

        Box<Integer> intBox = new Box<>();
        intBox.set(100);
        System.out.println(intBox.get()); // 100
    }
}
```

#### 2. Generic Method

Generic methods can work with different types without making the entire class generic.

```java
public class Utils {
    public static <T> void printArray(T[] array) {
        for (T element : array) {
            System.out.println(element);
        }
    }
    
    public static void main(String[] args) {
        Integer[] numbers = {1, 2, 3};
        String[] words = {"Hello", "World"};
        
        printArray(numbers);
        printArray(words);
    }
}
```

#### 3. Bounded Generics

Bounded generics restrict the types that can be used as type arguments.

```java
class Calculator<T extends Number> {
    public double square(T number) {
        return number.doubleValue() * number.doubleValue();
    }
}

public class Main {
    public static void main(String[] args) {
        Calculator<Integer> intCalc = new Calculator<>();
        System.out.println(intCalc.square(5)); // 25.0
        
        Calculator<Double> doubleCalc = new Calculator<>();
        System.out.println(doubleCalc.square(3.5)); // 12.25
    }
}
```

#### 4. Wildcards

Wildcards provide flexibility for method parameters.

```java
public static void printNumbers(List<? extends Number> list) {
    for(Number n : list) System.out.println(n);
}
```

**Common Mistakes / Tips:**

* Cannot instantiate `new T()`.
* Generics work **at compile-time only** (type erasure at runtime).
* Use wildcards for **flexible method parameters**.

---

### B. Streams

**Definition:**
A **Stream** represents a **sequence of elements** supporting **functional-style operations** on collections (like `filter`, `map`, `reduce`) without modifying the source.

**Key Points:**

* **Lazy evaluation:** Operations are executed only when a terminal operation is called.
* Can be **parallelized** for multi-core processing (`parallelStream()`).
* Types of operations:
  * **Intermediate**: `filter()`, `map()`, `sorted()`, `distinct()`
  * **Terminal**: `collect()`, `forEach()`, `reduce()`, `count()`

#### Stream Examples:

**1. Filter and Print**

```java
List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "David");
names.stream()
     .filter(name -> name.length() <= 4)
     .forEach(System.out::println); 
// Output: Bob
```

**2. Map and Collect**

```java
List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
List<Integer> squares = numbers.stream()
                               .map(n -> n * n)
                               .collect(Collectors.toList());
System.out.println(squares); // [1, 4, 9, 16, 25]
```

**3. Reduce Example**

```java
int sum = numbers.stream()
                 .reduce(0, Integer::sum);
System.out.println(sum); // 15
```

**4. Sorting and Distinct**

```java
List<Integer> nums = Arrays.asList(5, 2, 2, 3, 1);
nums.stream()
    .distinct()
    .sorted()
    .forEach(System.out::println); 
// 1, 2, 3, 5
```

**5. Parallel Stream Example**

```java
numbers.parallelStream()
       .map(n -> n * 2)
       .forEach(System.out::println);
```

**Notes / Exam Tips:**

* Always differentiate **Stream vs Collection**: streams don't store data.
* Use `collect()` to transform a stream back to a list, set, or map.
* Prefer **method references** (`Class::method`) for cleaner code.
* Streams can help **replace loops**, making code concise and readable.
''',
    codeSnippet: '''
// ========================================
// GENERICS EXAMPLES
// ========================================

// 1. Generic Class Example
class Box<T> {
    private T value;
    
    public void set(T value) { 
        this.value = value; 
    }
    
    public T get() { 
        return value; 
    }
}

// Usage
Box<String> stringBox = new Box<>();
stringBox.set("Java Programming");
System.out.println(stringBox.get()); // Java Programming

Box<Integer> intBox = new Box<>();
intBox.set(100);
System.out.println(intBox.get()); // 100

// 2. Generic Method Example
class ArrayUtils {
    public static <T> void printArray(T[] array) {
        System.out.print("[ ");
        for (T element : array) {
            System.out.print(element + " ");
        }
        System.out.println("]");
    }
}

// Usage
Integer[] numbers = {1, 2, 3, 4, 5};
String[] words = {"Hello", "World", "Java"};

ArrayUtils.printArray(numbers); // [ 1 2 3 4 5 ]
ArrayUtils.printArray(words);   // [ Hello World Java ]

// 3. Bounded Generics Example
class Calculator<T extends Number> {
    public double square(T number) {
        return number.doubleValue() * number.doubleValue();
    }
    
    public double add(T a, T b) {
        return a.doubleValue() + b.doubleValue();
    }
}

// Usage
Calculator<Integer> intCalc = new Calculator<>();
System.out.println(intCalc.square(5));      // 25.0
System.out.println(intCalc.add(10, 20));    // 30.0

Calculator<Double> doubleCalc = new Calculator<>();
System.out.println(doubleCalc.square(3.5)); // 12.25

// 4. Wildcards Example
class WildcardExample {
    // Upper bounded wildcard
    public static void printNumbers(List<? extends Number> list) {
        for(Number n : list) {
            System.out.println(n);
        }
    }
    
    // Lower bounded wildcard
    public static void addNumbers(List<? super Integer> list) {
        list.add(10);
        list.add(20);
    }
}

// ========================================
// STREAMS EXAMPLES
// ========================================

import java.util.*;
import java.util.stream.*;

// 1. Filter Example
List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "David", "Eve");

// Filter names with length <= 4
List<String> shortNames = names.stream()
                               .filter(name -> name.length() <= 4)
                               .collect(Collectors.toList());
System.out.println(shortNames); // [Bob, Eve]

// 2. Map Example
List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);

// Square each number
List<Integer> squares = numbers.stream()
                               .map(n -> n * n)
                               .collect(Collectors.toList());
System.out.println(squares); // [1, 4, 9, 16, 25]

// 3. Reduce Example
int sum = numbers.stream()
                 .reduce(0, Integer::sum);
System.out.println("Sum: " + sum); // Sum: 15

// Find maximum
Optional<Integer> max = numbers.stream()
                               .reduce(Integer::max);
max.ifPresent(m -> System.out.println("Max: " + m)); // Max: 5

// 4. Filter, Map, and Collect
List<Integer> evenSquares = numbers.stream()
                                   .filter(n -> n % 2 == 0)
                                   .map(n -> n * n)
                                   .collect(Collectors.toList());
System.out.println(evenSquares); // [4, 16]

// 5. Distinct and Sorted
List<Integer> nums = Arrays.asList(5, 2, 8, 2, 3, 1, 8, 3);
List<Integer> uniqueSorted = nums.stream()
                                 .distinct()
                                 .sorted()
                                 .collect(Collectors.toList());
System.out.println(uniqueSorted); // [1, 2, 3, 5, 8]

// 6. Count Example
long count = names.stream()
                  .filter(name -> name.startsWith("A"))
                  .count();
System.out.println("Names starting with A: " + count); // 1

// 7. ForEach Example
names.stream()
     .map(String::toUpperCase)
     .forEach(System.out::println);

// 8. Parallel Stream Example
List<Integer> largeList = IntStream.rangeClosed(1, 100)
                                   .boxed()
                                   .collect(Collectors.toList());

int parallelSum = largeList.parallelStream()
                           .reduce(0, Integer::sum);
System.out.println("Parallel Sum: " + parallelSum); // 5050

// 9. Collecting to Map
Map<Integer, String> idNameMap = names.stream()
    .collect(Collectors.toMap(
        name -> name.length(),
        name -> name,
        (existing, replacement) -> existing
    ));

// 10. Grouping Example
Map<Integer, List<String>> groupedByLength = names.stream()
    .collect(Collectors.groupingBy(String::length));
System.out.println(groupedByLength);
// {3=[Bob, Eve], 5=[Alice, David], 7=[Charlie]}
''',
    revisionPoints: [
      'Generics provide compile-time type safety and eliminate need for casting',
      'Generic classes, methods, and interfaces use type parameters like <T>, <E>, <K,V>',
      'Bounded generics (<T extends Number>) restrict allowable types',
      'Wildcards (? extends, ? super) provide flexibility in method parameters',
      'Streams enable functional-style operations on collections without modifying source',
      'Stream operations are lazy - executed only when terminal operation is called',
      'Intermediate operations: filter(), map(), sorted(), distinct()',
      'Terminal operations: collect(), forEach(), reduce(), count()',
      'Parallel streams leverage multi-core processors for better performance',
      'Method references (Class::method) make stream code cleaner and more readable',
    ],
    quizQuestions: [
      Question(
        question: 'What is the main benefit of using Generics?',
        options: ['Better performance', 'Compile-time type safety', 'Easier syntax', 'Smaller code size'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which symbol is used for bounded generics that restricts to Number types?',
        options: ['<T implements Number>', '<T extends Number>', '<T super Number>', '<T of Number>'],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of operation is filter() in Streams?',
        options: ['Terminal operation', 'Intermediate operation', 'Source operation', 'Collector operation'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method converts a Stream back to a List?',
        options: ['toList()', 'collect(Collectors.toList())', 'asList()', 'convertToList()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the reduce() operation do in Streams?',
        options: ['Filters elements', 'Transforms elements', 'Combines elements into a single result', 'Sorts elements'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_file_handling',
    title: '7. File Handling and I/O in Java',
    explanation: '''## File Handling and I/O in Java

### A. Introduction

**Definition:**
Java provides **File Handling** and **I/O streams** to **read from and write to files**. This allows applications to **store persistent data**, process text or binary files, and handle user inputs efficiently.

**Key Points:**

* **I/O Streams:** Flow of data (input = read, output = write).
* Streams can be **byte-oriented** (`InputStream`, `OutputStream`) or **character-oriented** (`Reader`, `Writer`).
* Files can be **text files** or **binary files**.
* Introduced in **`java.io`** package.

---

### B. File Class

**Purpose:** Represent files and directories, check existence, create/delete files, list directories.

**Basic Example:**

```java
import java.io.File;

public class FileExample {
    public static void main(String[] args) {
        File file = new File("sample.txt");

        // Check if file exists
        if(file.exists()) {
            System.out.println("File exists");
        } else {
            System.out.println("File does not exist");
        }

        try {
            // Create file
            if(file.createNewFile()) {
                System.out.println("File created: " + file.getName());
            }
        } catch(Exception e) {
            System.out.println("Error creating file: " + e.getMessage());
        }

        // Delete file
        // file.delete();
    }
}
```

**Notes / Tips:**

* `file.exists()`, `file.isFile()`, `file.isDirectory()` are useful for validation.
* Always handle `IOException` with try-catch.

---

### C. Byte Streams (`InputStream` / `OutputStream`)

**Definition:** Used for reading/writing **binary data**, such as images, audio, or serialized objects.

**Example: Writing to a file (OutputStream)**

```java
import java.io.FileOutputStream;
import java.io.IOException;

public class ByteOutputExample {
    public static void main(String[] args) {
        try (FileOutputStream fos = new FileOutputStream("data.bin")) {
            String data = "Hello Binary World";
            byte[] bytes = data.getBytes();
            fos.write(bytes);
            System.out.println("Data written successfully!");
        } catch(IOException e) {
            e.printStackTrace();
        }
    }
}
```

**Example: Reading from a file (InputStream)**

```java
import java.io.FileInputStream;
import java.io.IOException;

public class ByteInputExample {
    public static void main(String[] args) {
        try (FileInputStream fis = new FileInputStream("data.bin")) {
            int content;
            while ((content = fis.read()) != -1) {
                System.out.print((char) content);
            }
        } catch(IOException e) {
            e.printStackTrace();
        }
    }
}
```

**Tips:**

* Always **close streams** or use **try-with-resources**.
* Byte streams are better for **non-text files**.

---

### D. Character Streams (`Reader` / `Writer`)

**Definition:** Used for reading/writing **text data**, respecting encoding.

**Example: Writing to a text file**

```java
import java.io.FileWriter;
import java.io.IOException;

public class CharWriterExample {
    public static void main(String[] args) {
        try (FileWriter writer = new FileWriter("example.txt")) {
            writer.write("Hello World in Java!");
        } catch(IOException e) {
            e.printStackTrace();
        }
    }
}
```

**Example: Reading from a text file**

```java
import java.io.FileReader;
import java.io.IOException;

public class CharReaderExample {
    public static void main(String[] args) {
        try (FileReader reader = new FileReader("example.txt")) {
            int ch;
            while ((ch = reader.read()) != -1) {
                System.out.print((char) ch);
            }
        } catch(IOException e) {
            e.printStackTrace();
        }
    }
}
```

**Buffered Reader / Writer (Efficient for large files)**

```java
import java.io.*;

public class BufferedExample {
    public static void main(String[] args) throws IOException {
        // Writing
        try (BufferedWriter bw = new BufferedWriter(new FileWriter("buffer.txt"))) {
            bw.write("Line 1\\nLine 2\\nLine 3");
        }

        // Reading
        try (BufferedReader br = new BufferedReader(new FileReader("buffer.txt"))) {
            String line;
            while ((line = br.readLine()) != null) {
                System.out.println(line);
            }
        }
    }
}
```

**Tips / Exam Notes:**

* `BufferedReader` is preferred for **line-by-line reading**.
* `PrintWriter` is good for formatted output.
* Always handle **exceptions** (`IOException`).

---

### E. Serialization and Deserialization

**Definition:** Convert **objects to byte streams** for storage (`serialization`) and read back (`deserialization`).

**Example:**

```java
import java.io.*;

class Student implements Serializable {
    String name;
    int age;
    
    Student(String name, int age) {
        this.name = name;
        this.age = age;
    }
}

public class SerializationExample {
    public static void main(String[] args) throws IOException, ClassNotFoundException {
        Student s = new Student("Alice", 20);

        // Serialize
        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("student.ser"))) {
            oos.writeObject(s);
        }

        // Deserialize
        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream("student.ser"))) {
            Student s2 = (Student) ois.readObject();
            System.out.println(s2.name + " - " + s2.age); // Alice - 20
        }
    }
}
```

**Tips:**

* Class must implement `Serializable`.
* Mark fields `transient` to avoid serialization.

---

### F. Exam Tips / Notes

* **Byte streams** → Binary files
* **Character streams** → Text files
* **Buffered streams** → Efficient I/O
* Always **close streams** or use **try-with-resources**
* Use **File class** to manage files and directories
* Understand **serialization vs I/O streams**
''',
    codeSnippet: '''
// ========================================
// FILE CLASS EXAMPLES
// ========================================

import java.io.*;

// 1. File Operations
File file = new File("example.txt");

// Check existence
if (file.exists()) {
    System.out.println("File exists!");
    System.out.println("Absolute path: " + file.getAbsolutePath());
    System.out.println("Size: " + file.length() + " bytes");
    System.out.println("Readable: " + file.canRead());
    System.out.println("Writable: " + file.canWrite());
}

// Create new file
try {
    if (file.createNewFile()) {
        System.out.println("File created: " + file.getName());
    }
} catch (IOException e) {
    e.printStackTrace();
}

// Delete file
// file.delete();

// Directory operations
File dir = new File("myFolder");
if (dir.mkdir()) {
    System.out.println("Directory created");
}

// List files in directory
File folder = new File(".");
String[] files = folder.list();
for (String f : files) {
    System.out.println(f);
}

// ========================================
// BYTE STREAMS (Binary Data)
// ========================================

// Writing binary data
try (FileOutputStream fos = new FileOutputStream("data.bin")) {
    String text = "Hello Binary World!";
    byte[] bytes = text.getBytes();
    fos.write(bytes);
    System.out.println("Binary data written");
} catch (IOException e) {
    e.printStackTrace();
}

// Reading binary data
try (FileInputStream fis = new FileInputStream("data.bin")) {
    int content;
    System.out.print("Reading: ");
    while ((content = fis.read()) != -1) {
        System.out.print((char) content);
    }
    System.out.println();
} catch (IOException e) {
    e.printStackTrace();
}

// ========================================
// CHARACTER STREAMS (Text Data)
// ========================================

// Writing text with FileWriter
try (FileWriter writer = new FileWriter("output.txt")) {
    writer.write("Hello Java File I/O!\\n");
    writer.write("This is line 2.\\n");
    writer.write("Writing text files is easy!");
    System.out.println("Text written successfully");
} catch (IOException e) {
    e.printStackTrace();
}

// Reading text with FileReader
try (FileReader reader = new FileReader("output.txt")) {
    int ch;
    System.out.print("Content: ");
    while ((ch = reader.read()) != -1) {
        System.out.print((char) ch);
    }
} catch (IOException e) {
    e.printStackTrace();
}

// ========================================
// BUFFERED STREAMS (Efficient I/O)
// ========================================

// Writing with BufferedWriter
try (BufferedWriter bw = new BufferedWriter(new FileWriter("buffer.txt"))) {
    bw.write("First line");
    bw.newLine(); // Platform-independent newline
    bw.write("Second line");
    bw.newLine();
    bw.write("Third line");
} catch (IOException e) {
    e.printStackTrace();
}

// Reading with BufferedReader (line by line)
try (BufferedReader br = new BufferedReader(new FileReader("buffer.txt"))) {
    String line;
    System.out.println("Reading line by line:");
    while ((line = br.readLine()) != null) {
        System.out.println(line);
    }
} catch (IOException e) {
    e.printStackTrace();
}

// ========================================
// PRINTWRITER (Formatted Output)
// ========================================

try (PrintWriter pw = new PrintWriter("formatted.txt")) {
    pw.println("Name: John Doe");
    pw.println("Age: 25");
    pw.printf("Score: %.2f%%\\n", 95.5);
    pw.println("Status: Active");
} catch (IOException e) {
    e.printStackTrace();
}

// ========================================
// SERIALIZATION EXAMPLE
// ========================================

import java.io.Serializable;

class Student implements Serializable {
    private static final long serialVersionUID = 1L;
    String name;
    int rollNo;
    transient String password; // Won't be serialized
    
    Student(String name, int rollNo, String password) {
        this.name = name;
        this.rollNo = rollNo;
        this.password = password;
    }
    
    public String toString() {
        return "Student[name=" + name + ", rollNo=" + rollNo + 
               ", password=" + password + "]";
    }
}

// Serialize object
Student student = new Student("Alice", 101, "secret123");
try (ObjectOutputStream oos = new ObjectOutputStream(
        new FileOutputStream("student.ser"))) {
    oos.writeObject(student);
    System.out.println("Object serialized");
} catch (IOException e) {
    e.printStackTrace();
}

// Deserialize object
try (ObjectInputStream ois = new ObjectInputStream(
        new FileInputStream("student.ser"))) {
    Student s = (Student) ois.readObject();
    System.out.println("Deserialized: " + s);
    // Note: password will be null (transient)
} catch (IOException | ClassNotFoundException e) {
    e.printStackTrace();
}

// ========================================
// TRY-WITH-RESOURCES (Auto-close)
// ========================================

// Automatically closes resources
try (BufferedReader br = new BufferedReader(new FileReader("file.txt"));
     BufferedWriter bw = new BufferedWriter(new FileWriter("copy.txt"))) {
    
    String line;
    while ((line = br.readLine()) != null) {
        bw.write(line);
        bw.newLine();
    }
} catch (IOException e) {
    e.printStackTrace();
}

// ========================================
// COPY FILE EXAMPLE
// ========================================

public static void copyFile(String source, String dest) {
    try (FileInputStream fis = new FileInputStream(source);
         FileOutputStream fos = new FileOutputStream(dest)) {
        
        byte[] buffer = new byte[1024];
        int length;
        
        while ((length = fis.read(buffer)) > 0) {
            fos.write(buffer, 0, length);
        }
        
        System.out.println("File copied successfully");
    } catch (IOException e) {
        e.printStackTrace();
    }
}
''',
    revisionPoints: [
      'File class represents files and directories in the file system',
      'Byte streams (InputStream/OutputStream) handle binary data',
      'Character streams (Reader/Writer) handle text data with encoding',
      'BufferedReader/Writer provide efficient I/O through buffering',
      'Try-with-resources ensures automatic resource closure',
      'FileInputStream/FileOutputStream for byte-level file operations',
      'FileReader/FileWriter for character-level file operations',
      'Serialization converts objects to byte streams for storage',
      'Classes must implement Serializable interface for serialization',
      'transient keyword prevents fields from being serialized',
      'Always handle IOException when working with files',
      'PrintWriter provides convenient formatted text output',
    ],
    quizQuestions: [
      Question(
        question: 'Which package contains file I/O classes?',
        options: ['java.util', 'java.io', 'java.lang', 'java.file'],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of streams are used for reading binary data?',
        options: ['Character streams', 'Byte streams', 'Text streams', 'Object streams'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is best for reading text files line by line?',
        options: ['FileReader', 'BufferedReader', 'Scanner', 'FileInputStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does try-with-resources ensure?',
        options: ['Faster execution', 'Automatic resource closure', 'Exception handling', 'Memory optimization'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which interface must a class implement to be serialized?',
        options: ['Cloneable', 'Comparable', 'Serializable', 'Iterable'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which keyword prevents a field from being serialized?',
        options: ['static', 'final', 'transient', 'volatile'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which class is used to represent files and directories?',
        options: ['FileSystem', 'File', 'Path', 'Directory'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the parent class of FileInputStream and FileOutputStream?',
        options: ['Stream', 'InputStream and OutputStream', 'ByteStream', 'FileStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method is used to check if a file exists?',
        options: ['checkExists()', 'exists()', 'isPresent()', 'fileExists()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class provides formatted text output similar to System.out.println?',
        options: ['FileWriter', 'BufferedWriter', 'PrintWriter', 'TextWriter'],
        correctIndex: 2,
      ),
      Question(
        question: 'What exception must be handled when performing file I/O operations?',
        options: ['FileException', 'IOException', 'IOError', 'FileNotFoundException only'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which stream class is used to write objects to a file?',
        options: ['FileOutputStream', 'ObjectOutputStream', 'DataOutputStream', 'BufferedOutputStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the readLine() method of BufferedReader return at end of file?',
        options: ['Empty string', 'null', '-1', 'EOF'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is more efficient for reading large text files?',
        options: ['FileReader alone', 'BufferedReader', 'Scanner', 'FileInputStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of serialVersionUID in Serializable classes?',
        options: ['Track object version', 'Improve performance', 'Security feature', 'Required by compiler'],
        correctIndex: 0,
      ),
    ],
  ),
  Topic(
    id: 'java_jdbc',
    title: '8. Java Database Connectivity (JDBC)',
    explanation: '''## Java Database Connectivity (JDBC)

### A. Introduction

**Definition:**
JDBC is a **Java API** that allows Java programs to **connect, interact, and manipulate relational databases** like MySQL, Oracle, or PostgreSQL.

**Key Points:**

* JDBC is **part of `java.sql` package**.
* Provides methods for **connecting to databases, executing SQL queries, and processing results**.
* Works with **drivers** that implement the JDBC interface (JDBC-ODBC bridge, Type 4 driver).

---

### B. JDBC Architecture

1. **JDBC API** – Java application calls methods to interact with the database.
2. **JDBC Driver Manager** – Manages database drivers.
3. **Database** – Actual RDBMS that stores the data.

**Steps to use JDBC:**

1. Load the JDBC driver
2. Establish connection
3. Create a statement
4. Execute query
5. Process results
6. Close connection

---

### C. Load JDBC Driver

```java
try {
    Class.forName("com.mysql.cj.jdbc.Driver");
    System.out.println("Driver Loaded Successfully!");
} catch(ClassNotFoundException e) {
    e.printStackTrace();
}
```

* From **JDBC 4.0**, **automatic driver loading** is supported, so explicitly loading class is optional.

---

### D. Establish Connection

```java
import java.sql.*;

public class JDBCConnectionExample {
    public static void main(String[] args) {
        String url = "jdbc:mysql://localhost:3306/testdb";
        String user = "root";
        String password = "password";

        try (Connection conn = DriverManager.getConnection(url, user, password)) {
            if (conn != null) {
                System.out.println("Connected to the database!");
            }
        } catch(SQLException e) {
            e.printStackTrace();
        }
    }
}
```

* `Connection` object represents the **database session**.
* Always use **try-with-resources** to automatically close the connection.

---

### E. Create Statement and Execute Queries

**1. Using Statement (for static SQL)**

```java
try (Connection conn = DriverManager.getConnection(url, user, password);
     Statement stmt = conn.createStatement()) {

    String sql = "CREATE TABLE students (id INT PRIMARY KEY, name VARCHAR(50), age INT)";
    stmt.executeUpdate(sql);
    System.out.println("Table created successfully!");

} catch(SQLException e) {
    e.printStackTrace();
}
```

**2. Insert Data**

```java
String insertSQL = "INSERT INTO students (id, name, age) VALUES (1, 'Alice', 20)";
stmt.executeUpdate(insertSQL);
```

**3. Query Data**

```java
String selectSQL = "SELECT * FROM students";
ResultSet rs = stmt.executeQuery(selectSQL);

while(rs.next()) {
    int id = rs.getInt("id");
    String name = rs.getString("name");
    int age = rs.getInt("age");
    System.out.println(id + " | " + name + " | " + age);
}
```

---

### F. PreparedStatement (for dynamic SQL and security)

**Why use it:**

* Avoid **SQL injection**
* Precompiled query → faster execution

```java
String sql = "INSERT INTO students (id, name, age) VALUES (?, ?, ?)";
try (PreparedStatement ps = conn.prepareStatement(sql)) {
    ps.setInt(1, 2);
    ps.setString(2, "Bob");
    ps.setInt(3, 22);
    ps.executeUpdate();
}
```

**Query Example with PreparedStatement**

```java
String selectSQL = "SELECT * FROM students WHERE age > ?";
PreparedStatement ps = conn.prepareStatement(selectSQL);
ps.setInt(1, 20);
ResultSet rs = ps.executeQuery();
while(rs.next()) {
    System.out.println(rs.getString("name") + " - " + rs.getInt("age"));
}
```

---

### G. ResultSet Types and Navigation

* **ResultSet Types:**

  * `TYPE_FORWARD_ONLY` → Default, move forward only
  * `TYPE_SCROLL_INSENSITIVE` → Can scroll, insensitive to DB changes
  * `TYPE_SCROLL_SENSITIVE` → Can scroll, reflects DB changes

* **Cursor Methods:**

  * `next()`, `previous()`, `first()`, `last()`, `absolute(int row)`

```java
ResultSet rs = stmt.executeQuery("SELECT * FROM students");
rs.last();
System.out.println("Last student: " + rs.getString("name"));
```

---

### H. Close Resources

Always close in reverse order of opening to **prevent resource leaks**:

```java
rs.close();
stmt.close();
conn.close();
```

Or use **try-with-resources** as shown above.

---

### I. Transaction Management

```java
try {
    conn.setAutoCommit(false); // start transaction

    stmt.executeUpdate("INSERT INTO students VALUES (3, 'Charlie', 25)");
    stmt.executeUpdate("UPDATE students SET age=21 WHERE id=1");

    conn.commit(); // commit transaction
} catch(SQLException e) {
    conn.rollback(); // rollback on error
}
```

**Tips / Exam Notes:**

* **Statement** → Simple SQL execution
* **PreparedStatement** → Dynamic SQL, prevents injection
* **ResultSet** → Holds query results
* **Transactions** → `commit()` / `rollback()` for data consistency
* Always handle **SQLException**
''',
    codeSnippet: '''
// ========================================
// JDBC DRIVER LOADING
// ========================================

import java.sql.*;

// Explicit driver loading (optional from JDBC 4.0)
try {
    Class.forName("com.mysql.cj.jdbc.Driver");
    System.out.println("Driver Loaded Successfully!");
} catch(ClassNotFoundException e) {
    e.printStackTrace();
}

// ========================================
// ESTABLISH CONNECTION
// ========================================

String url = "jdbc:mysql://localhost:3306/testdb";
String user = "root";
String password = "password";

try (Connection conn = DriverManager.getConnection(url, user, password)) {
    if (conn != null) {
        System.out.println("Connected to the database!");
    }
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// STATEMENT - CREATE TABLE
// ========================================

try (Connection conn = DriverManager.getConnection(url, user, password);
     Statement stmt = conn.createStatement()) {

    String sql = "CREATE TABLE students (id INT PRIMARY KEY, name VARCHAR(50), age INT)";
    stmt.executeUpdate(sql);
    System.out.println("Table created successfully!");

} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// STATEMENT - INSERT DATA
// ========================================

try (Statement stmt = conn.createStatement()) {
    String insertSQL = "INSERT INTO students (id, name, age) VALUES (1, 'Alice', 20)";
    int rows = stmt.executeUpdate(insertSQL);
    System.out.println(rows + " row(s) inserted");
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// STATEMENT - QUERY DATA
// ========================================

try (Statement stmt = conn.createStatement()) {
    String selectSQL = "SELECT * FROM students";
    ResultSet rs = stmt.executeQuery(selectSQL);

    while(rs.next()) {
        int id = rs.getInt("id");
        String name = rs.getString("name");
        int age = rs.getInt("age");
        System.out.println(id + " | " + name + " | " + age);
    }
    rs.close();
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// PREPAREDSTATEMENT - INSERT (Prevents SQL Injection)
// ========================================

String sql = "INSERT INTO students (id, name, age) VALUES (?, ?, ?)";
try (PreparedStatement ps = conn.prepareStatement(sql)) {
    ps.setInt(1, 2);
    ps.setString(2, "Bob");
    ps.setInt(3, 22);
    int rows = ps.executeUpdate();
    System.out.println(rows + " row(s) inserted using PreparedStatement");
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// PREPAREDSTATEMENT - QUERY WITH PARAMETERS
// ========================================

String selectSQL = "SELECT * FROM students WHERE age > ?";
try (PreparedStatement ps = conn.prepareStatement(selectSQL)) {
    ps.setInt(1, 20);
    ResultSet rs = ps.executeQuery();
    
    while(rs.next()) {
        System.out.println(rs.getString("name") + " - " + rs.getInt("age"));
    }
    rs.close();
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// PREPAREDSTATEMENT - UPDATE
// ========================================

String updateSQL = "UPDATE students SET age = ? WHERE id = ?";
try (PreparedStatement ps = conn.prepareStatement(updateSQL)) {
    ps.setInt(1, 21);
    ps.setInt(2, 1);
    int rows = ps.executeUpdate();
    System.out.println(rows + " row(s) updated");
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// PREPAREDSTATEMENT - DELETE
// ========================================

String deleteSQL = "DELETE FROM students WHERE id = ?";
try (PreparedStatement ps = conn.prepareStatement(deleteSQL)) {
    ps.setInt(1, 2);
    int rows = ps.executeUpdate();
    System.out.println(rows + " row(s) deleted");
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// RESULTSET NAVIGATION
// ========================================

Statement stmt = conn.createStatement(
    ResultSet.TYPE_SCROLL_INSENSITIVE,
    ResultSet.CONCUR_READ_ONLY
);

ResultSet rs = stmt.executeQuery("SELECT * FROM students");

// Move to last row
rs.last();
System.out.println("Last student: " + rs.getString("name"));

// Move to first row
rs.first();
System.out.println("First student: " + rs.getString("name"));

// Move to specific row
rs.absolute(2);
System.out.println("Second student: " + rs.getString("name"));

// Move backward
rs.previous();
System.out.println("Previous student: " + rs.getString("name"));

// ========================================
// TRANSACTION MANAGEMENT
// ========================================

try {
    conn.setAutoCommit(false); // Start transaction

    Statement stmt = conn.createStatement();
    
    // Multiple operations
    stmt.executeUpdate("INSERT INTO students VALUES (3, 'Charlie', 25)");
    stmt.executeUpdate("UPDATE students SET age=21 WHERE id=1");
    stmt.executeUpdate("DELETE FROM students WHERE id=2");

    conn.commit(); // Commit transaction
    System.out.println("Transaction committed successfully");
    
} catch(SQLException e) {
    try {
        conn.rollback(); // Rollback on error
        System.out.println("Transaction rolled back");
    } catch(SQLException ex) {
        ex.printStackTrace();
    }
    e.printStackTrace();
} finally {
    try {
        conn.setAutoCommit(true); // Restore auto-commit
    } catch(SQLException e) {
        e.printStackTrace();
    }
}

// ========================================
// BATCH PROCESSING (Multiple operations)
// ========================================

try (PreparedStatement ps = conn.prepareStatement(
        "INSERT INTO students (id, name, age) VALUES (?, ?, ?)")) {
    
    // Add batch operations
    ps.setInt(1, 4); ps.setString(2, "David"); ps.setInt(3, 23);
    ps.addBatch();
    
    ps.setInt(1, 5); ps.setString(2, "Eve"); ps.setInt(3, 24);
    ps.addBatch();
    
    ps.setInt(1, 6); ps.setString(2, "Frank"); ps.setInt(3, 22);
    ps.addBatch();
    
    // Execute all batches
    int[] results = ps.executeBatch();
    System.out.println("Batch executed: " + results.length + " operations");
    
} catch(SQLException e) {
    e.printStackTrace();
}

// ========================================
// COMPLETE EXAMPLE
// ========================================

public class JDBCCompleteExample {
    public static void main(String[] args) {
        String url = "jdbc:mysql://localhost:3306/testdb";
        String user = "root";
        String password = "password";
        
        try (Connection conn = DriverManager.getConnection(url, user, password)) {
            
            // Create table
            String createTable = "CREATE TABLE IF NOT EXISTS students " +
                               "(id INT PRIMARY KEY, name VARCHAR(50), age INT)";
            try (Statement stmt = conn.createStatement()) {
                stmt.executeUpdate(createTable);
            }
            
            // Insert data
            String insert = "INSERT INTO students VALUES (?, ?, ?)";
            try (PreparedStatement ps = conn.prepareStatement(insert)) {
                ps.setInt(1, 1);
                ps.setString(2, "Alice");
                ps.setInt(3, 20);
                ps.executeUpdate();
            }
            
            // Query data
            String select = "SELECT * FROM students";
            try (Statement stmt = conn.createStatement();
                 ResultSet rs = stmt.executeQuery(select)) {
                
                while(rs.next()) {
                    System.out.println(rs.getInt("id") + " | " + 
                                     rs.getString("name") + " | " + 
                                     rs.getInt("age"));
                }
            }
            
        } catch(SQLException e) {
            e.printStackTrace();
        }
    }
}
''',
    revisionPoints: [
      'JDBC is part of java.sql package for database connectivity',
      'DriverManager.getConnection() establishes database connection',
      'Statement is used for static SQL queries',
      'PreparedStatement prevents SQL injection and improves performance',
      'ResultSet holds the results of SQL queries',
      'executeQuery() returns ResultSet for SELECT statements',
      'executeUpdate() returns int for INSERT/UPDATE/DELETE statements',
      'Try-with-resources ensures automatic closing of database resources',
      'Transaction management uses commit() and rollback()',
      'setAutoCommit(false) starts a transaction',
      'ResultSet navigation methods: next(), previous(), first(), last()',
      'Batch processing improves performance for multiple operations',
      'Always handle SQLException when working with JDBC',
      'Connection URL format: jdbc:database://host:port/dbname',
    ],
    quizQuestions: [
      Question(
        question: 'Which package contains JDBC classes?',
        options: ['java.util', 'java.sql', 'java.db', 'java.jdbc'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used to execute SQL queries?',
        options: ['Connection', 'Statement', 'ResultSet', 'Driver'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the advantage of PreparedStatement over Statement?',
        options: ['Faster compilation', 'Prevents SQL injection', 'Better syntax', 'Automatic commits'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method is used to execute SELECT queries?',
        options: ['execute()', 'executeQuery()', 'executeUpdate()', 'runQuery()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does executeUpdate() return?',
        options: ['ResultSet', 'Boolean', 'Number of affected rows', 'Connection object'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which method starts a transaction in JDBC?',
        options: ['beginTransaction()', 'startTransaction()', 'setAutoCommit(false)', 'transaction()'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the default ResultSet type?',
        options: ['TYPE_SCROLL_SENSITIVE', 'TYPE_FORWARD_ONLY', 'TYPE_SCROLL_INSENSITIVE', 'TYPE_BIDIRECTIONAL'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which exception must be handled in JDBC operations?',
        options: ['IOException', 'SQLException', 'DatabaseException', 'JDBCException'],
        correctIndex: 1,
      ),
      Question(
        question: 'What method is used to commit a transaction?',
        options: ['save()', 'commit()', 'finalize()', 'complete()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which interface represents a database connection?',
        options: ['Statement', 'ResultSet', 'Connection', 'Driver'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does the question mark (?) represent in PreparedStatement?',
        options: ['Comment', 'Parameter placeholder', 'Wildcard', 'Optional field'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method retrieves data from ResultSet by column name?',
        options: ['get()', 'fetch()', 'getString()', 'retrieve()'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_memory_gc',
    title: '9. Java Memory Management & Garbage Collection',
    explanation: '''## Java Memory Management & Garbage Collection

### A. Introduction

**Definition:**
Java Memory Management is the **process by which Java allocates, uses, and frees memory** during program execution. The **JVM automatically manages memory**, mainly through **Garbage Collection (GC)**, reducing programmer errors like memory leaks.

**Key Points:**

* Memory is divided into **Heap** and **Non-Heap (Stack, Method Area, PC Register, etc.)**.
* **Heap** → Stores **objects**.
* **Stack** → Stores **primitive local variables and method call frames**.
* **Garbage Collector (GC)** → Automatically reclaims memory of objects no longer referenced.

---

### B. JVM Memory Structure

| Memory Area         | Purpose                                             |
| ------------------- | --------------------------------------------------- |
| Heap                | Stores objects and arrays                           |
| Stack               | Stores method call frames, local variables          |
| Method Area         | Stores class structures, bytecode, static variables |
| PC Register         | Program counter for JVM instructions                |
| Native Method Stack | For executing native (C/C++) code                   |

---

### C. Garbage Collection (GC)

**Definition:**
GC is the **automatic process** of identifying and deleting objects **no longer in use** to free heap memory.

**Types of References:**

* **Strong Reference** → Object is never GC'ed if referenced.
* **Soft Reference** → GC collects only if memory is low.
* **Weak Reference** → GC collects as soon as object is weakly reachable.
* **Phantom Reference** → Used for post-mortem cleanup.

---

### D. Generational Heap

Java divides heap into **Young Generation** and **Old Generation**:

| Generation              | Purpose               | GC Type       |
| ----------------------- | --------------------- | ------------- |
| Young (Eden + Survivor) | Newly created objects | Minor GC      |
| Old (Tenured)           | Long-lived objects    | Major/Full GC |

**Flow:**

1. Objects are created in **Eden**.
2. Survived objects → **Survivor space**.
3. Long-lived → **Old Generation**.

---

### E. How GC Works

1. **Mark:** Identify objects that are reachable.
2. **Sweep:** Remove unreferenced objects.
3. **Compact (optional):** Reduce heap fragmentation.

**Example: Basic Garbage Collection**

```java
public class GCDemo {
    public static void main(String[] args) {
        GCDemo obj1 = new GCDemo();
        GCDemo obj2 = new GCDemo();

        obj1 = null; // obj1 eligible for GC
        System.gc(); // Suggest JVM to run GC
        System.out.println("Requesting Garbage Collection");
    }

    @Override
    protected void finalize() throws Throwable {
        System.out.println("Garbage Collected");
    }
}
```

**Notes:**

* `System.gc()` is only a **request**, JVM decides when to run GC.
* `finalize()` method is **called before object is garbage collected** (deprecated in latest Java versions).

---

### F. Common Garbage Collectors in Java

| Collector                   | Description                                                      |
| --------------------------- | ---------------------------------------------------------------- |
| Serial GC                   | Single-threaded, simple, good for small apps                     |
| Parallel GC                 | Multi-threaded, reduces pause time                               |
| CMS (Concurrent Mark-Sweep) | Low pause, concurrent collection for Old Gen                     |
| G1 GC                       | Modern collector, splits heap into regions, good for large heaps |
| ZGC / Shenandoah            | Ultra-low pause, scalable for huge applications                  |

**Switching Collector Example:**

```bash
java -XX:+UseG1GC MyApp
```

---

### G. Best Practices / Exam Tips

* Avoid **creating unnecessary objects**.
* Set **null references** when objects are no longer needed.
* Use **StringBuilder** for string concatenation inside loops to reduce garbage.
* Understand **Minor GC vs Major GC** for performance tuning.
* **Memory leaks** can occur if references are unintentionally retained.

---

### H. Visual Example of Memory Management

```text
Heap:
+----------------+
| Eden           |  <- New Objects
| Survivor       |  <- Objects surviving Minor GC
| Old Generation |  <- Long-lived objects
+----------------+

Stack:
Method1 -> locals
Method2 -> locals
```

* Objects created in **Eden**, after Minor GC surviving objects move to Survivor, then Old Generation.
''',
    codeSnippet: '''
// ========================================
// JVM MEMORY AREAS
// ========================================

/*
JVM Memory Structure:
1. Heap - Stores objects and arrays
2. Stack - Method call frames, local variables
3. Method Area - Class structures, static variables
4. PC Register - Program counter
5. Native Method Stack - Native code execution
*/

// ========================================
// GARBAGE COLLECTION DEMO
// ========================================

public class GCDemo {
    public static void main(String[] args) {
        GCDemo obj1 = new GCDemo();
        GCDemo obj2 = new GCDemo();

        // obj1 becomes eligible for GC
        obj1 = null;
        
        // Request GC (not guaranteed to run)
        System.gc();
        System.out.println("Requesting Garbage Collection");
    }

    @Override
    protected void finalize() throws Throwable {
        System.out.println("Garbage Collected: " + this);
    }
}

// ========================================
// REFERENCE TYPES
// ========================================

import java.lang.ref.*;

public class ReferenceDemo {
    public static void main(String[] args) {
        // 1. Strong Reference (never GC'ed while referenced)
        Object strongRef = new Object();
        
        // 2. Soft Reference (GC only if memory is low)
        SoftReference<Object> softRef = new SoftReference<>(new Object());
        Object obj1 = softRef.get(); // May return null if GC'ed
        
        // 3. Weak Reference (GC immediately when not strongly referenced)
        WeakReference<Object> weakRef = new WeakReference<>(new Object());
        Object obj2 = weakRef.get(); // May return null
        
        // 4. Phantom Reference (for cleanup after finalization)
        ReferenceQueue<Object> queue = new ReferenceQueue<>();
        PhantomReference<Object> phantomRef = new PhantomReference<>(new Object(), queue);
    }
}

// ========================================
// MEMORY LEAK EXAMPLE
// ========================================

import java.util.*;

public class MemoryLeakExample {
    // Static collection retains references
    private static List<Object> cache = new ArrayList<>();
    
    public static void main(String[] args) {
        // This causes memory leak - objects never removed
        for (int i = 0; i < 1000000; i++) {
            cache.add(new Object());
        }
        // Objects stay in memory even if not needed
    }
    
    // FIX: Clear cache when done
    public static void clearCache() {
        cache.clear(); // Allow GC
    }
}

// ========================================
// GENERATIONAL HEAP EXAMPLE
// ========================================

public class GenerationalHeapDemo {
    public static void main(String[] args) {
        // New objects created in Eden (Young Generation)
        for (int i = 0; i < 100; i++) {
            String temp = new String("Object " + i);
            // Most short-lived objects die in Young Gen
        }
        
        // Long-lived objects
        List<String> longLived = new ArrayList<>();
        for (int i = 0; i < 1000; i++) {
            longLived.add("Item " + i);
        }
        // Eventually moves to Old Generation after surviving Minor GCs
    }
}

// ========================================
// MONITORING MEMORY
// ========================================

public class MemoryMonitor {
    public static void main(String[] args) {
        Runtime runtime = Runtime.getRuntime();
        
        long totalMemory = runtime.totalMemory();
        long freeMemory = runtime.freeMemory();
        long maxMemory = runtime.maxMemory();
        long usedMemory = totalMemory - freeMemory;
        
        System.out.println("Total Memory: " + totalMemory / (1024*1024) + " MB");
        System.out.println("Free Memory: " + freeMemory / (1024*1024) + " MB");
        System.out.println("Used Memory: " + usedMemory / (1024*1024) + " MB");
        System.out.println("Max Memory: " + maxMemory / (1024*1024) + " MB");
    }
}

// ========================================
// BEST PRACTICES
// ========================================

public class GCBestPractices {
    public static void main(String[] args) {
        // BAD: Creates many String objects (garbage)
        String result = "";
        for (int i = 0; i < 1000; i++) {
            result += i; // Creates new String each time
        }
        
        // GOOD: Use StringBuilder to reduce garbage
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < 1000; i++) {
            sb.append(i); // Reuses same object
        }
        String betterResult = sb.toString();
        
        // Set to null when done to help GC
        sb = null;
    }
}

// ========================================
// OBJECT LIFECYCLE
// ========================================

public class ObjectLifecycle {
    public static void main(String[] args) {
        // 1. Object created in Heap (Eden space)
        MyObject obj = new MyObject();
        
        // 2. Object is in use (referenced)
        obj.doSomething();
        
        // 3. Object becomes eligible for GC (no references)
        obj = null;
        
        // 4. GC may collect it during next cycle
        System.gc(); // Suggestion only
        
        // 5. finalize() called before destruction (deprecated)
    }
}

class MyObject {
    void doSomething() {
        System.out.println("Doing work...");
    }
    
    @Override
    protected void finalize() throws Throwable {
        System.out.println("Object being finalized");
    }
}

// ========================================
// GC TUNING FLAGS (Command Line)
// ========================================

/*
// Use G1 Garbage Collector
java -XX:+UseG1GC MyApp

// Use Parallel GC
java -XX:+UseParallelGC MyApp

// Set heap size
java -Xms512m -Xmx2g MyApp

// Print GC details
java -XX:+PrintGCDetails -XX:+PrintGCTimeStamps MyApp

// Enable verbose GC logging
java -verbose:gc MyApp
*/
''',
    revisionPoints: [
      'JVM memory is divided into Heap, Stack, Method Area, PC Register, and Native Method Stack',
      'Heap stores objects and arrays',
      'Stack stores method call frames and local variables',
      'Garbage Collection automatically reclaims memory of unreferenced objects',
      'Heap is divided into Young Generation (Eden + Survivor) and Old Generation',
      'Minor GC occurs in Young Generation, Major GC in Old Generation',
      'Strong references prevent GC, weak/soft references allow conditional GC',
      'System.gc() is only a request, JVM decides when to run GC',
      'finalize() method is called before object destruction (deprecated)',
      'Memory leaks occur when references are unintentionally retained',
      'StringBuilder reduces garbage compared to String concatenation in loops',
      'Common GC algorithms: Serial, Parallel, CMS, G1, ZGC, Shenandoah',
      'Mark, Sweep, and Compact are the three phases of GC',
      'Setting references to null helps make objects eligible for GC',
    ],
    quizQuestions: [
      Question(
        question: 'Which memory area stores objects in Java?',
        options: ['Stack', 'Heap', 'Method Area', 'PC Register'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does GC stand for in Java?',
        options: ['Global Compiler', 'Garbage Collection', 'Generic Class', 'Garbage Collector'],
        correctIndex: 1,
      ),
      Question(
        question: 'Where are method call frames stored?',
        options: ['Heap', 'Stack', 'Method Area', 'Eden'],
        correctIndex: 1,
      ),
      Question(
        question: 'What are the two main divisions of the Java Heap?',
        options: ['New and Old', 'Young and Old', 'Eden and Stack', 'Primary and Secondary'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which type of GC occurs in the Young Generation?',
        options: ['Major GC', 'Minor GC', 'Full GC', 'Complete GC'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does System.gc() do?',
        options: ['Forces garbage collection', 'Requests garbage collection', 'Stops garbage collection', 'Disables garbage collection'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which reference type is never garbage collected while referenced?',
        options: ['Weak Reference', 'Soft Reference', 'Strong Reference', 'Phantom Reference'],
        correctIndex: 2,
      ),
      Question(
        question: 'Where are newly created objects initially stored?',
        options: ['Old Generation', 'Eden space', 'Survivor space', 'Method Area'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method is called before an object is garbage collected?',
        options: ['destroy()', 'finalize()', 'dispose()', 'cleanup()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the first phase of garbage collection?',
        options: ['Sweep', 'Compact', 'Mark', 'Delete'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which GC is recommended for large heap applications?',
        options: ['Serial GC', 'G1 GC', 'Parallel GC', 'CMS GC'],
        correctIndex: 1,
      ),
      Question(
        question: 'What can cause memory leaks in Java?',
        options: ['Too many objects', 'Unintentionally retained references', 'Using primitives', 'Stack overflow'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_lambda',
    title: '10. Lambda Expressions and Functional Interfaces',
    explanation: '''## Lambda Expressions and Functional Interfaces in Java

### A. Introduction

**Definition:**

* **Lambda Expression:** A concise way to represent **anonymous functions** in Java. Introduced in **Java 8**, it allows writing **functional-style code**.
* **Functional Interface:** An interface that contains **exactly one abstract method**. Lambda expressions are used primarily to provide **implementation for functional interfaces**.

**Key Points:**

* Lambda expressions **reduce boilerplate code** (no need for anonymous classes).
* Functional interfaces are annotated with `@FunctionalInterface` (optional but recommended).
* Syntax:

  ```java
  (parameters) -> expression/body
  ```

---

### B. Functional Interface

**Definition:** Interface with **exactly one abstract method**. Can have default and static methods.

**Example:**

```java
@FunctionalInterface
interface Calculator {
    int operation(int a, int b);

    // Default method allowed
    default void hello() {
        System.out.println("Hello from Calculator");
    }

    // Static method allowed
    static void info() {
        System.out.println("Static info method");
    }
}
```

---

### C. Lambda Expression Syntax

| Syntax Type         | Example                                      |
| ------------------- | -------------------------------------------- |
| No parameter        | `() -> System.out.println("Hello")`          |
| Single parameter    | `x -> x * x`                                 |
| Multiple parameters | `(a, b) -> a + b`                            |
| Multiple statements | `(a, b) -> { int sum = a + b; return sum; }` |

---

### D. Lambda with Functional Interface

```java
public class LambdaDemo {
    public static void main(String[] args) {
        Calculator add = (a, b) -> a + b;
        Calculator multiply = (a, b) -> a * b;

        System.out.println(add.operation(5, 3));      // 8
        System.out.println(multiply.operation(5, 3)); // 15
    }
}
```

**Notes:**

* Lambda implements the **single abstract method** of the functional interface.
* No need to write a **separate class** for implementation.

---

### E. Common Functional Interfaces in Java 8

| Interface           | Method              | Description                   |
| ------------------- | ------------------- | ----------------------------- |
| `Predicate<T>`      | `test(T t)`         | Returns boolean               |
| `Function<T,R>`     | `apply(T t)`        | Converts T → R                |
| `Consumer<T>`       | `accept(T t)`       | Performs action, returns void |
| `Supplier<T>`       | `get()`             | Supplies result of type T     |
| `UnaryOperator<T>`  | `apply(T t)`        | Takes T, returns T            |
| `BinaryOperator<T>` | `apply(T t1, T t2)` | Takes two Ts, returns T       |

**Examples:**

```java
import java.util.function.*;

public class FunctionalInterfaceDemo {
    public static void main(String[] args) {
        // Predicate example
        Predicate<Integer> isEven = n -> n % 2 == 0;
        System.out.println(isEven.test(10)); // true

        // Function example
        Function<String, Integer> stringLength = s -> s.length();
        System.out.println(stringLength.apply("Java")); // 4

        // Consumer example
        Consumer<String> printer = s -> System.out.println(s);
        printer.accept("Hello Lambda!"); // Hello Lambda!

        // Supplier example
        Supplier<Double> randomValue = () -> Math.random();
        System.out.println(randomValue.get()); // e.g., 0.7263
    }
}
```

---

### F. Lambda Expressions with Collections & Streams

```java
import java.util.*;

public class LambdaCollections {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("Alice", "Bob", "Charlie");

        // Using forEach with lambda
        names.forEach(name -> System.out.println(name));

        // Using filter with Streams
        names.stream()
             .filter(name -> name.startsWith("A"))
             .forEach(System.out::println); // Alice
    }
}
```

**Notes:**

* Lambda expressions integrate seamlessly with **Streams API**.
* `::` method reference is shorthand for lambda calling a method.

---

### G. Best Practices / Exam Tips

* Use lambda **only with functional interfaces**.
* Prefer **method references** for cleaner code:

  ```java
  names.forEach(System.out::println);
  ```
* Avoid complex multi-line lambdas; better to use **regular method**.
* Functional interfaces are widely used in **Streams, Collections, and Event handling**.
''',
    codeSnippet: '''
// ========================================
// FUNCTIONAL INTERFACE DEFINITION
// ========================================

@FunctionalInterface
interface Calculator {
    int operation(int a, int b);

    // Default method allowed
    default void hello() {
        System.out.println("Hello from Calculator");
    }

    // Static method allowed
    static void info() {
        System.out.println("Static info method");
    }
}

// ========================================
// LAMBDA SYNTAX EXAMPLES
// ========================================

// 1. No parameters
Runnable r1 = () -> System.out.println("Hello");
r1.run();

// 2. Single parameter (no parentheses needed)
Function<Integer, Integer> square = x -> x * x;
System.out.println(square.apply(5)); // 25

// 3. Multiple parameters
Calculator add = (a, b) -> a + b;
System.out.println(add.operation(10, 20)); // 30

// 4. Multiple statements (needs braces and return)
Calculator subtract = (a, b) -> {
    int result = a - b;
    System.out.println("Subtracting: " + a + " - " + b);
    return result;
};
System.out.println(subtract.operation(20, 10)); // 10

// ========================================
// LAMBDA WITH FUNCTIONAL INTERFACE
// ========================================

public class LambdaDemo {
    public static void main(String[] args) {
        // Addition using lambda
        Calculator add = (a, b) -> a + b;
        
        // Multiplication using lambda
        Calculator multiply = (a, b) -> a * b;
        
        // Division using lambda
        Calculator divide = (a, b) -> {
            if (b == 0) {
                System.out.println("Cannot divide by zero");
                return 0;
            }
            return a / b;
        };

        System.out.println("Addition: " + add.operation(5, 3));      // 8
        System.out.println("Multiplication: " + multiply.operation(5, 3)); // 15
        System.out.println("Division: " + divide.operation(10, 2));   // 5
    }
}

// ========================================
// PREDICATE FUNCTIONAL INTERFACE
// ========================================

import java.util.function.Predicate;

public class PredicateDemo {
    public static void main(String[] args) {
        // Check if number is even
        Predicate<Integer> isEven = n -> n % 2 == 0;
        System.out.println(isEven.test(10)); // true
        System.out.println(isEven.test(7));  // false

        // Check if string is empty
        Predicate<String> isEmpty = s -> s.isEmpty();
        System.out.println(isEmpty.test("")); // true
        
        // Check if age is adult
        Predicate<Integer> isAdult = age -> age >= 18;
        System.out.println(isAdult.test(20)); // true
    }
}

// ========================================
// FUNCTION FUNCTIONAL INTERFACE
// ========================================

import java.util.function.Function;

public class FunctionDemo {
    public static void main(String[] args) {
        // Convert String to Integer (length)
        Function<String, Integer> stringLength = s -> s.length();
        System.out.println(stringLength.apply("Java")); // 4

        // Convert Integer to String
        Function<Integer, String> intToString = n -> "Number: " + n;
        System.out.println(intToString.apply(42)); // Number: 42

        // Square a number
        Function<Integer, Integer> square = n -> n * n;
        System.out.println(square.apply(5)); // 25
    }
}

// ========================================
// CONSUMER FUNCTIONAL INTERFACE
// ========================================

import java.util.function.Consumer;

public class ConsumerDemo {
    public static void main(String[] args) {
        // Print string
        Consumer<String> printer = s -> System.out.println(s);
        printer.accept("Hello Lambda!"); // Hello Lambda!

        // Print with formatting
        Consumer<Integer> printSquare = n -> System.out.println("Square: " + (n * n));
        printSquare.accept(5); // Square: 25

        // Multiple operations
        Consumer<String> upperCase = s -> System.out.println(s.toUpperCase());
        upperCase.accept("java"); // JAVA
    }
}

// ========================================
// SUPPLIER FUNCTIONAL INTERFACE
// ========================================

import java.util.function.Supplier;

public class SupplierDemo {
    public static void main(String[] args) {
        // Supply random number
        Supplier<Double> randomValue = () -> Math.random();
        System.out.println(randomValue.get()); // e.g., 0.7263

        // Supply current time
        Supplier<Long> currentTime = () -> System.currentTimeMillis();
        System.out.println(currentTime.get()); // Current timestamp

        // Supply constant
        Supplier<String> greeting = () -> "Hello World";
        System.out.println(greeting.get()); // Hello World
    }
}

// ========================================
// UNARY OPERATOR & BINARY OPERATOR
// ========================================

import java.util.function.*;

public class OperatorDemo {
    public static void main(String[] args) {
        // UnaryOperator (same type input and output)
        UnaryOperator<Integer> square = n -> n * n;
        System.out.println(square.apply(5)); // 25

        // BinaryOperator (two inputs of same type, one output)
        BinaryOperator<Integer> max = (a, b) -> a > b ? a : b;
        System.out.println(max.apply(10, 20)); // 20

        BinaryOperator<String> concat = (s1, s2) -> s1 + s2;
        System.out.println(concat.apply("Hello ", "World")); // Hello World
    }
}

// ========================================
// LAMBDA WITH COLLECTIONS
// ========================================

import java.util.*;

public class LambdaCollections {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "David");

        // 1. forEach with lambda
        System.out.println("All names:");
        names.forEach(name -> System.out.println(name));

        // 2. forEach with method reference
        System.out.println("\\nUsing method reference:");
        names.forEach(System.out::println);

        // 3. Filter with streams
        System.out.println("\\nNames starting with 'A':");
        names.stream()
             .filter(name -> name.startsWith("A"))
             .forEach(System.out::println); // Alice

        // 4. Map with streams
        System.out.println("\\nName lengths:");
        names.stream()
             .map(name -> name.length())
             .forEach(System.out::println); // 5, 3, 7, 5

        // 5. Sorting with lambda
        System.out.println("\\nSorted names:");
        names.stream()
             .sorted((a, b) -> a.compareTo(b))
             .forEach(System.out::println);
    }
}

// ========================================
// LAMBDA WITH THREADS
// ========================================

public class LambdaThreads {
    public static void main(String[] args) {
        // Old way (anonymous class)
        Thread t1 = new Thread(new Runnable() {
            @Override
            public void run() {
                System.out.println("Old way");
            }
        });

        // New way (lambda)
        Thread t2 = new Thread(() -> System.out.println("Lambda way"));

        t1.start();
        t2.start();
    }
}

// ========================================
// CUSTOM FUNCTIONAL INTERFACE EXAMPLES
// ========================================

@FunctionalInterface
interface StringOperation {
    String process(String s);
}

@FunctionalInterface
interface NumberChecker {
    boolean check(int num);
}

public class CustomFunctionalDemo {
    public static void main(String[] args) {
        // String operations
        StringOperation toUpper = s -> s.toUpperCase();
        StringOperation reverse = s -> new StringBuilder(s).reverse().toString();
        
        System.out.println(toUpper.process("java")); // JAVA
        System.out.println(reverse.process("java")); // avaj

        // Number checkers
        NumberChecker isPrime = num -> {
            if (num < 2) return false;
            for (int i = 2; i <= Math.sqrt(num); i++) {
                if (num % i == 0) return false;
            }
            return true;
        };
        
        System.out.println(isPrime.check(17)); // true
        System.out.println(isPrime.check(10)); // false
    }
}

// ========================================
// METHOD REFERENCES
// ========================================

import java.util.*;

public class MethodReferenceDemo {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("Alice", "Bob", "Charlie");

        // Lambda
        names.forEach(name -> System.out.println(name));

        // Method reference (cleaner)
        names.forEach(System.out::println);

        // Static method reference
        List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
        numbers.forEach(MethodReferenceDemo::printSquare);
    }

    static void printSquare(int n) {
        System.out.println(n * n);
    }
}
''',
    revisionPoints: [
      'Lambda expressions represent anonymous functions introduced in Java 8',
      'Functional interfaces have exactly one abstract method',
      '@FunctionalInterface annotation marks functional interfaces',
      'Lambda syntax: (parameters) -> expression or body',
      'Predicate<T> tests conditions and returns boolean',
      'Function<T,R> converts input type T to output type R',
      'Consumer<T> performs actions without returning a value',
      'Supplier<T> supplies values without taking parameters',
      'UnaryOperator<T> takes and returns same type',
      'BinaryOperator<T> takes two parameters of same type and returns same type',
      'Lambda expressions work seamlessly with Streams API',
      'Method references (::) provide shorthand for lambdas',
      'Functional interfaces can have default and static methods',
      'Lambda expressions reduce boilerplate code compared to anonymous classes',
    ],
    quizQuestions: [
      Question(
        question: 'What annotation marks a functional interface?',
        options: ['@Functional', '@FunctionalInterface', '@Lambda', '@Interface'],
        correctIndex: 1,
      ),
      Question(
        question: 'How many abstract methods does a functional interface have?',
        options: ['Zero', 'Exactly one', 'One or more', 'Any number'],
        correctIndex: 1,
      ),
      Question(
        question: 'In which Java version were lambda expressions introduced?',
        options: ['Java 6', 'Java 7', 'Java 8', 'Java 9'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which functional interface is used to test conditions?',
        options: ['Function', 'Predicate', 'Consumer', 'Supplier'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which functional interface returns a value without taking parameters?',
        options: ['Consumer', 'Function', 'Supplier', 'Predicate'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does the :: operator represent in Java?',
        options: ['Lambda expression', 'Method reference', 'Scope resolution', 'Type casting'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which functional interface performs an action without returning a value?',
        options: ['Supplier', 'Function', 'Consumer', 'Predicate'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the basic lambda syntax for no parameters?',
        options: ['-> expression', '() -> expression', '[] -> expression', '{} -> expression'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which functional interface converts one type to another?',
        options: ['Converter', 'Function', 'Mapper', 'Transformer'],
        correctIndex: 1,
      ),
      Question(
        question: 'Can functional interfaces have default methods?',
        options: ['No, never', 'Yes, any number', 'Only one', 'Only static methods'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does BinaryOperator take as parameters?',
        options: ['Two different types', 'Two same types', 'One parameter', 'No parameters'],
        correctIndex: 1,
      ),
      Question(
        question: 'Lambda expressions are primarily used with which API?',
        options: ['JDBC API', 'Streams API', 'Reflection API', 'Collections only'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_annotations_enums',
    title: '11. Java Annotations and Enums',
    explanation: '''## Java Annotations and Enums

### A. Annotations

**Definition:**

* **Annotations** are **metadata** that provide information about the code but do not directly affect program execution.
* Introduced in Java 5, they are widely used for **documentation, compilation checks, and runtime processing**.

**Key Points:**

* Declared with `@` symbol.
* Can be **built-in** (like `@Override`) or **custom**.
* Can be applied to **classes, methods, fields, parameters, packages**.

---

### B. Built-in Annotations

| Annotation             | Purpose                                     |
| ---------------------- | ------------------------------------------- |
| `@Override`            | Checks method overrides parent class method |
| `@Deprecated`          | Marks method/class as obsolete              |
| `@SuppressWarnings`    | Suppresses compiler warnings                |
| `@FunctionalInterface` | Marks interface as functional interface     |
| `@SafeVarargs`         | Suppresses unsafe varargs warnings          |

**Example:**

```java
class Parent {
    void display() {
        System.out.println("Parent display");
    }
}

class Child extends Parent {
    @Override
    void display() {
        System.out.println("Child display");
    }

    @Deprecated
    void oldMethod() {
        System.out.println("Old method, do not use");
    }
}

public class AnnotationDemo {
    @SuppressWarnings("deprecation")
    public static void main(String[] args) {
        Child c = new Child();
        c.display();
        c.oldMethod(); // No compiler warning
    }
}
```

---

### C. Custom Annotations

**Definition:** User-defined annotations for special purposes.

**Example:**

```java
import java.lang.annotation.*;

@Retention(RetentionPolicy.RUNTIME) // Available at runtime
@Target(ElementType.METHOD)         // Can be applied to methods
@interface MyAnnotation {
    String author();
    String date();
    int version() default 1;
}

class Demo {
    @MyAnnotation(author="Alice", date="2025-10-17")
    public void myMethod() {
        System.out.println("Method with custom annotation");
    }
}

public class CustomAnnotationDemo {
    public static void main(String[] args) throws Exception {
        Demo obj = new Demo();
        obj.myMethod();

        // Access annotation using reflection
        MyAnnotation ann = obj.getClass().getMethod("myMethod").getAnnotation(MyAnnotation.class);
        System.out.println("Author: " + ann.author());
        System.out.println("Date: " + ann.date());
        System.out.println("Version: " + ann.version());
    }
}
```

**Notes / Exam Tips:**

* Use `@Retention` to specify **lifetime** (`SOURCE`, `CLASS`, `RUNTIME`).
* Use `@Target` to specify **applicable elements**.
* Annotations are heavily used in **Spring, JPA, and testing frameworks**.

---

### D. Enums

**Definition:**

* **Enums** are special classes representing a **fixed set of constants**.
* Introduced in Java 5, they improve **type safety** over integer constants.

**Syntax:**

```java
enum Day {
    MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY
}
```

**Example: Using Enums**

```java
public class EnumDemo {
    enum Color { RED, GREEN, BLUE }

    public static void main(String[] args) {
        Color c1 = Color.RED;
        System.out.println(c1); // RED

        for (Color c : Color.values()) {
            System.out.println(c);
        }
    }
}
```

---

### E. Enums with Fields and Methods

```java
enum Planet {
    MERCURY(3.3e+23, 2.4e6),
    VENUS(4.87e+24, 6.1e6),
    EARTH(5.97e+24, 6.37e6);

    private double mass;
    private double radius;

    Planet(double mass, double radius) {
        this.mass = mass;
        this.radius = radius;
    }

    public double getMass() { return mass; }
    public double getRadius() { return radius; }

    public double surfaceGravity() {
        final double G = 6.67430e-11;
        return G * mass / (radius * radius);
    }
}

public class EnumWithMethods {
    public static void main(String[] args) {
        for(Planet p : Planet.values()) {
            System.out.println(p + ": " + p.surfaceGravity());
        }
    }
}
```

**Notes / Exam Tips:**

* Enums can implement **interfaces** but **cannot extend other classes**.
* Useful in **switch-case statements** for type safety:

```java
Day today = Day.MONDAY;
switch(today) {
    case MONDAY: System.out.println("Start of week"); break;
    case FRIDAY: System.out.println("End of week"); break;
    default: System.out.println("Midweek"); break;
}
```

* Enums + Annotations are widely used in **Spring, JPA, and configuration constants**.
''',
    codeSnippet: '''
// ========================================
// BUILT-IN ANNOTATIONS
// ========================================

class Parent {
    void display() {
        System.out.println("Parent display");
    }
    
    void show() {
        System.out.println("Parent show");
    }
}

class Child extends Parent {
    // @Override - Checks method overrides parent
    @Override
    void display() {
        System.out.println("Child display");
    }

    // @Deprecated - Marks method as obsolete
    @Deprecated
    void oldMethod() {
        System.out.println("Old method, do not use");
    }
    
    // Compile error if method doesn't override
    // @Override
    // void wrongMethod() {} // Error: doesn't override anything
}

public class AnnotationDemo {
    // @SuppressWarnings - Suppresses compiler warnings
    @SuppressWarnings("deprecation")
    public static void main(String[] args) {
        Child c = new Child();
        c.display();
        c.oldMethod(); // No compiler warning
    }
}

// ========================================
// @FunctionalInterface ANNOTATION
// ========================================

@FunctionalInterface
interface Calculator {
    int calculate(int a, int b);
    
    // Can have default methods
    default void info() {
        System.out.println("Calculator interface");
    }
    
    // Compile error if we add another abstract method
    // int anotherMethod(); // Error: not a functional interface
}

// ========================================
// CUSTOM ANNOTATIONS
// ========================================

import java.lang.annotation.*;

// Define custom annotation
@Retention(RetentionPolicy.RUNTIME) // Available at runtime
@Target(ElementType.METHOD)         // Can be applied to methods
@interface MyAnnotation {
    String author();
    String date();
    int version() default 1;
}

// Another custom annotation
@Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.TYPE, ElementType.METHOD})
@interface TestInfo {
    String testName();
    String tester();
    String[] tags() default {};
}

class Demo {
    @MyAnnotation(author="Alice", date="2025-10-17")
    public void myMethod() {
        System.out.println("Method with custom annotation");
    }
    
    @MyAnnotation(author="Bob", date="2025-10-18", version=2)
    public void anotherMethod() {
        System.out.println("Another annotated method");
    }
    
    @TestInfo(testName="LoginTest", tester="Charlie", tags={"smoke", "critical"})
    public void testMethod() {
        System.out.println("Test method");
    }
}

// ========================================
// ACCESSING ANNOTATIONS VIA REFLECTION
// ========================================

import java.lang.reflect.Method;

public class CustomAnnotationDemo {
    public static void main(String[] args) throws Exception {
        Demo obj = new Demo();
        obj.myMethod();

        // Access annotation using reflection
        Method method = obj.getClass().getMethod("myMethod");
        MyAnnotation ann = method.getAnnotation(MyAnnotation.class);
        
        if (ann != null) {
            System.out.println("Author: " + ann.author());
            System.out.println("Date: " + ann.date());
            System.out.println("Version: " + ann.version());
        }
        
        // Access TestInfo annotation
        Method testMethod = obj.getClass().getMethod("testMethod");
        TestInfo testInfo = testMethod.getAnnotation(TestInfo.class);
        
        if (testInfo != null) {
            System.out.println("Test Name: " + testInfo.testName());
            System.out.println("Tester: " + testInfo.tester());
            System.out.print("Tags: ");
            for (String tag : testInfo.tags()) {
                System.out.print(tag + " ");
            }
        }
    }
}

// ========================================
// RETENTION POLICIES
// ========================================

// SOURCE - Discarded by compiler
@Retention(RetentionPolicy.SOURCE)
@interface SourceAnnotation {
    String value();
}

// CLASS - Retained in .class file but not at runtime
@Retention(RetentionPolicy.CLASS)
@interface ClassAnnotation {
    String value();
}

// RUNTIME - Available at runtime via reflection
@Retention(RetentionPolicy.RUNTIME)
@interface RuntimeAnnotation {
    String value();
}

// ========================================
// BASIC ENUM
// ========================================

enum Day {
    MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY
}

public class EnumDemo {
    enum Color { RED, GREEN, BLUE }

    public static void main(String[] args) {
        // Using enum
        Color c1 = Color.RED;
        System.out.println(c1); // RED

        // Iterating enum values
        System.out.println("All colors:");
        for (Color c : Color.values()) {
            System.out.println(c);
        }
        
        // Get enum by name
        Color c2 = Color.valueOf("BLUE");
        System.out.println(c2); // BLUE
        
        // Get ordinal (position)
        System.out.println(Color.RED.ordinal()); // 0
    }
}

// ========================================
// ENUM WITH SWITCH STATEMENT
// ========================================

public class EnumSwitchDemo {
    public static void main(String[] args) {
        Day today = Day.MONDAY;
        
        switch(today) {
            case MONDAY:
                System.out.println("Start of week");
                break;
            case FRIDAY:
                System.out.println("End of work week");
                break;
            case SATURDAY:
            case SUNDAY:
                System.out.println("Weekend!");
                break;
            default:
                System.out.println("Midweek");
                break;
        }
    }
}

// ========================================
// ENUM WITH FIELDS AND METHODS
// ========================================

enum Planet {
    MERCURY(3.3e+23, 2.4e6),
    VENUS(4.87e+24, 6.1e6),
    EARTH(5.97e+24, 6.37e6),
    MARS(6.42e+23, 3.4e6);

    private double mass;   // in kilograms
    private double radius; // in meters

    // Constructor
    Planet(double mass, double radius) {
        this.mass = mass;
        this.radius = radius;
    }

    // Getters
    public double getMass() { return mass; }
    public double getRadius() { return radius; }

    // Method
    public double surfaceGravity() {
        final double G = 6.67430e-11; // gravitational constant
        return G * mass / (radius * radius);
    }
}

public class EnumWithMethods {
    public static void main(String[] args) {
        System.out.println("Planet surface gravities:");
        for(Planet p : Planet.values()) {
            System.out.printf("%s: %.2f m/s²%n", p, p.surfaceGravity());
        }
        
        // Access specific planet
        Planet earth = Planet.EARTH;
        System.out.println("\\nEarth mass: " + earth.getMass());
        System.out.println("Earth radius: " + earth.getRadius());
    }
}

// ========================================
// ENUM IMPLEMENTING INTERFACE
// ========================================

interface Describable {
    String getDescription();
}

enum Status implements Describable {
    PENDING {
        public String getDescription() {
            return "Task is pending";
        }
    },
    IN_PROGRESS {
        public String getDescription() {
            return "Task is in progress";
        }
    },
    COMPLETED {
        public String getDescription() {
            return "Task is completed";
        }
    };
    
    // Abstract method that all constants must implement
    public abstract String getDescription();
}

public class EnumInterfaceDemo {
    public static void main(String[] args) {
        for (Status s : Status.values()) {
            System.out.println(s + ": " + s.getDescription());
        }
    }
}

// ========================================
// ENUM WITH ABSTRACT METHODS
// ========================================

enum Operation {
    PLUS {
        public double apply(double x, double y) { return x + y; }
    },
    MINUS {
        public double apply(double x, double y) { return x - y; }
    },
    TIMES {
        public double apply(double x, double y) { return x * y; }
    },
    DIVIDE {
        public double apply(double x, double y) { return x / y; }
    };
    
    public abstract double apply(double x, double y);
}

public class EnumOperationDemo {
    public static void main(String[] args) {
        double x = 10.0;
        double y = 5.0;
        
        for (Operation op : Operation.values()) {
            System.out.printf("%.1f %s %.1f = %.1f%n",
                x, op, y, op.apply(x, y));
        }
    }
}

// ========================================
// COMPLETE EXAMPLE: ANNOTATIONS + ENUMS
// ========================================

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@interface Priority {
    Level level();
}

enum Level {
    LOW, MEDIUM, HIGH, CRITICAL
}

class Task {
    @Priority(level = Level.HIGH)
    public void urgentTask() {
        System.out.println("Urgent task");
    }
    
    @Priority(level = Level.LOW)
    public void normalTask() {
        System.out.println("Normal task");
    }
}

public class AnnotationEnumDemo {
    public static void main(String[] args) throws Exception {
        Task task = new Task();
        
        for (Method m : task.getClass().getDeclaredMethods()) {
            if (m.isAnnotationPresent(Priority.class)) {
                Priority p = m.getAnnotation(Priority.class);
                System.out.println(m.getName() + " - Priority: " + p.level());
            }
        }
    }
}
''',
    revisionPoints: [
      'Annotations are metadata that provide information about code',
      'Annotations are declared with @ symbol',
      '@Override checks method overrides parent class method',
      '@Deprecated marks method or class as obsolete',
      '@SuppressWarnings suppresses compiler warnings',
      '@FunctionalInterface marks interface with one abstract method',
      'Custom annotations use @interface keyword',
      '@Retention specifies annotation lifetime (SOURCE, CLASS, RUNTIME)',
      '@Target specifies where annotation can be applied',
      'Enums represent fixed set of constants introduced in Java 5',
      'Enums provide type safety over integer constants',
      'Enums can have constructors, fields, and methods',
      'Enums can implement interfaces but cannot extend classes',
      'Enums are useful in switch-case statements for type safety',
      'Enums.values() returns array of all enum constants',
    ],
    quizQuestions: [
      Question(
        question: 'Which annotation indicates a method overrides a superclass method?',
        options: ['@Overload', '@Override', '@Inherited', '@Super'],
        correctIndex: 1,
      ),
      Question(
        question: 'In which Java version were annotations introduced?',
        options: ['Java 4', 'Java 5', 'Java 6', 'Java 8'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation marks a method or class as obsolete?',
        options: ['@Old', '@Deprecated', '@Obsolete', '@Remove'],
        correctIndex: 1,
      ),
      Question(
        question: 'What symbol is used to declare annotations?',
        options: ['#', '@', '&', r'$'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which retention policy makes annotation available at runtime?',
        options: ['SOURCE', 'CLASS', 'RUNTIME', 'COMPILE'],
        correctIndex: 2,
      ),
      Question(
        question: 'What keyword is used to define custom annotations?',
        options: ['@annotation', '@interface', 'annotation', 'interface'],
        correctIndex: 1,
      ),
      Question(
        question: 'In which Java version were enums introduced?',
        options: ['Java 4', 'Java 5', 'Java 6', 'Java 7'],
        correctIndex: 1,
      ),
      Question(
        question: 'Can enums extend other classes in Java?',
        options: ['Yes, any class', 'Yes, only Enum class', 'No', 'Only abstract classes'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which method returns all enum constants as an array?',
        options: ['getAll()', 'values()', 'constants()', 'all()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Can enums have constructors?',
        options: ['No, never', 'Yes, public only', 'Yes, private only', 'Yes, any access modifier'],
        correctIndex: 2,
      ),
      Question(
        question: 'Can enums implement interfaces?',
        options: ['No', 'Yes', 'Only functional interfaces', 'Only marker interfaces'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation suppresses compiler warnings?',
        options: ['@Ignore', '@SuppressWarnings', '@NoWarn', '@Suppress'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_design_patterns',
    title: '12. Java Design Patterns',
    explanation: '''## Java Design Patterns

### A. Introduction

**Definition:**

* **Design Patterns** are **reusable solutions** to common software design problems.
* They are **templates** or **best practices** to solve issues related to object creation, structure, and behavior in software design.
* They **do not provide code**, but a framework or approach to solve recurring problems.

**Key Points:**

* Divided into **three main categories**:

  1. **Creational Patterns** → Object creation mechanisms
  2. **Structural Patterns** → Object composition
  3. **Behavioral Patterns** → Object interaction and responsibility

---

### B. Creational Design Patterns

**1. Singleton Pattern**

* Ensures a **class has only one instance** and provides a global point of access.

```java
class Singleton {
    private static Singleton instance;

    private Singleton() {} // private constructor

    public static Singleton getInstance() {
        if (instance == null) {
            instance = new Singleton();
        }
        return instance;
    }
}

public class SingletonDemo {
    public static void main(String[] args) {
        Singleton s1 = Singleton.getInstance();
        Singleton s2 = Singleton.getInstance();
        System.out.println(s1 == s2); // true
    }
}
```

**Notes:**

* Thread-safe version uses `synchronized` block or **Bill Pugh Singleton** with static inner class.

---

**2. Factory Pattern**

* Provides **interface for creating objects**, but lets subclasses decide which class to instantiate.

```java
interface Shape {
    void draw();
}

class Circle implements Shape {
    public void draw() { System.out.println("Drawing Circle"); }
}

class Rectangle implements Shape {
    public void draw() { System.out.println("Drawing Rectangle"); }
}

class ShapeFactory {
    public Shape getShape(String shapeType) {
        if (shapeType.equalsIgnoreCase("CIRCLE")) return new Circle();
        if (shapeType.equalsIgnoreCase("RECTANGLE")) return new Rectangle();
        return null;
    }
}

public class FactoryDemo {
    public static void main(String[] args) {
        ShapeFactory factory = new ShapeFactory();
        Shape s1 = factory.getShape("CIRCLE");
        s1.draw(); // Drawing Circle
    }
}
```

---

### C. Structural Design Patterns

**1. Adapter Pattern**

* Allows incompatible interfaces to work together.

```java
interface MediaPlayer {
    void play(String audioType, String fileName);
}

class AudioPlayer implements MediaPlayer {
    MediaAdapter adapter;

    public void play(String audioType, String fileName) {
        if(audioType.equalsIgnoreCase("mp3")) {
            System.out.println("Playing mp3 file: " + fileName);
        } else {
            adapter = new MediaAdapter(audioType);
            adapter.play(audioType, fileName);
        }
    }
}

interface AdvancedMediaPlayer {
    void playVlc(String fileName);
    void playMp4(String fileName);
}

class VlcPlayer implements AdvancedMediaPlayer {
    public void playVlc(String fileName) { System.out.println("Playing vlc: " + fileName); }
    public void playMp4(String fileName) {}
}

class MediaAdapter implements MediaPlayer {
    AdvancedMediaPlayer advancedMusicPlayer;

    public MediaAdapter(String audioType) {
        if(audioType.equalsIgnoreCase("vlc")) advancedMusicPlayer = new VlcPlayer();
    }

    public void play(String audioType, String fileName) {
        if(audioType.equalsIgnoreCase("vlc")) advancedMusicPlayer.playVlc(fileName);
    }
}

public class AdapterDemo {
    public static void main(String[] args) {
        AudioPlayer player = new AudioPlayer();
        player.play("mp3", "song.mp3");
        player.play("vlc", "video.vlc");
    }
}
```

---

**2. Decorator Pattern**

* Dynamically adds **behavior to objects** without modifying their class.

```java
interface Coffee {
    double cost();
}

class SimpleCoffee implements Coffee {
    public double cost() { return 5; }
}

class MilkDecorator implements Coffee {
    private Coffee coffee;
    MilkDecorator(Coffee coffee) { this.coffee = coffee; }
    public double cost() { return coffee.cost() + 2; }
}

public class DecoratorDemo {
    public static void main(String[] args) {
        Coffee c = new MilkDecorator(new SimpleCoffee());
        System.out.println("Cost: " + c.cost()); // 7.0
    }
}
```

---

### D. Behavioral Design Patterns

**1. Observer Pattern**

* **One-to-many dependency** between objects; when one object changes, all observers are notified.

```java
import java.util.*;

interface Observer {
    void update(String message);
}

class Subject {
    private List<Observer> observers = new ArrayList<>();
    public void attach(Observer o) { observers.add(o); }
    public void notifyObservers(String message) {
        for(Observer o : observers) o.update(message);
    }
}

class ConcreteObserver implements Observer {
    private String name;
    ConcreteObserver(String name) { this.name = name; }
    public void update(String message) {
        System.out.println(name + " received: " + message);
    }
}

public class ObserverDemo {
    public static void main(String[] args) {
        Subject subject = new Subject();
        Observer o1 = new ConcreteObserver("Observer1");
        Observer o2 = new ConcreteObserver("Observer2");
        subject.attach(o1);
        subject.attach(o2);
        subject.notifyObservers("Update Available!");
    }
}
```

**2. Strategy Pattern**

* Defines a **family of algorithms**, encapsulates each, and makes them interchangeable.

```java
interface PaymentStrategy {
    void pay(int amount);
}

class CreditCardPayment implements PaymentStrategy {
    public void pay(int amount) { System.out.println("Paid " + amount + " using Credit Card"); }
}

class PayPalPayment implements PaymentStrategy {
    public void pay(int amount) { System.out.println("Paid " + amount + " using PayPal"); }
}

public class StrategyDemo {
    public static void main(String[] args) {
        PaymentStrategy strategy = new CreditCardPayment();
        strategy.pay(500);
        strategy = new PayPalPayment();
        strategy.pay(1000);
    }
}
```

---

### E. Best Practices / Exam Tips

* **Singleton** → Only one instance
* **Factory / Builder** → Flexible object creation
* **Adapter / Decorator** → Modify/extend behavior without changing class
* **Observer / Strategy** → Encapsulate behavior, loose coupling
* Understand **UML diagrams** of patterns for exams.
* Most common interview patterns: **Singleton, Factory, Observer, Strategy, Decorator**
''',
    codeSnippet: '''
// ========================================
// SINGLETON PATTERN (Creational)
// ========================================

// Basic Singleton
class Singleton {
    private static Singleton instance;

    private Singleton() {} // Private constructor

    public static Singleton getInstance() {
        if (instance == null) {
            instance = new Singleton();
        }
        return instance;
    }
    
    public void showMessage() {
        System.out.println("Singleton instance");
    }
}

// Thread-Safe Singleton (using synchronized)
class ThreadSafeSingleton {
    private static ThreadSafeSingleton instance;
    
    private ThreadSafeSingleton() {}
    
    public static synchronized ThreadSafeSingleton getInstance() {
        if (instance == null) {
            instance = new ThreadSafeSingleton();
        }
        return instance;
    }
}

// Bill Pugh Singleton (Best approach)
class BillPughSingleton {
    private BillPughSingleton() {}
    
    private static class SingletonHelper {
        private static final BillPughSingleton INSTANCE = new BillPughSingleton();
    }
    
    public static BillPughSingleton getInstance() {
        return SingletonHelper.INSTANCE;
    }
}

public class SingletonDemo {
    public static void main(String[] args) {
        Singleton s1 = Singleton.getInstance();
        Singleton s2 = Singleton.getInstance();
        System.out.println(s1 == s2); // true - same instance
    }
}

// ========================================
// FACTORY PATTERN (Creational)
// ========================================

interface Shape {
    void draw();
}

class Circle implements Shape {
    public void draw() {
        System.out.println("Drawing Circle");
    }
}

class Rectangle implements Shape {
    public void draw() {
        System.out.println("Drawing Rectangle");
    }
}

class Triangle implements Shape {
    public void draw() {
        System.out.println("Drawing Triangle");
    }
}

class ShapeFactory {
    public Shape getShape(String shapeType) {
        if (shapeType == null) return null;
        if (shapeType.equalsIgnoreCase("CIRCLE")) return new Circle();
        if (shapeType.equalsIgnoreCase("RECTANGLE")) return new Rectangle();
        if (shapeType.equalsIgnoreCase("TRIANGLE")) return new Triangle();
        return null;
    }
}

public class FactoryDemo {
    public static void main(String[] args) {
        ShapeFactory factory = new ShapeFactory();
        
        Shape shape1 = factory.getShape("CIRCLE");
        shape1.draw(); // Drawing Circle
        
        Shape shape2 = factory.getShape("RECTANGLE");
        shape2.draw(); // Drawing Rectangle
    }
}

// ========================================
// ADAPTER PATTERN (Structural)
// ========================================

interface MediaPlayer {
    void play(String audioType, String fileName);
}

interface AdvancedMediaPlayer {
    void playVlc(String fileName);
    void playMp4(String fileName);
}

class VlcPlayer implements AdvancedMediaPlayer {
    public void playVlc(String fileName) {
        System.out.println("Playing vlc file: " + fileName);
    }
    public void playMp4(String fileName) {}
}

class Mp4Player implements AdvancedMediaPlayer {
    public void playVlc(String fileName) {}
    public void playMp4(String fileName) {
        System.out.println("Playing mp4 file: " + fileName);
    }
}

class MediaAdapter implements MediaPlayer {
    AdvancedMediaPlayer advancedMusicPlayer;

    public MediaAdapter(String audioType) {
        if(audioType.equalsIgnoreCase("vlc")) {
            advancedMusicPlayer = new VlcPlayer();
        } else if (audioType.equalsIgnoreCase("mp4")) {
            advancedMusicPlayer = new Mp4Player();
        }
    }

    public void play(String audioType, String fileName) {
        if(audioType.equalsIgnoreCase("vlc")) {
            advancedMusicPlayer.playVlc(fileName);
        } else if(audioType.equalsIgnoreCase("mp4")) {
            advancedMusicPlayer.playMp4(fileName);
        }
    }
}

class AudioPlayer implements MediaPlayer {
    MediaAdapter adapter;

    public void play(String audioType, String fileName) {
        // Built-in support for mp3
        if(audioType.equalsIgnoreCase("mp3")) {
            System.out.println("Playing mp3 file: " + fileName);
        }
        // Use adapter for other formats
        else if(audioType.equalsIgnoreCase("vlc") || audioType.equalsIgnoreCase("mp4")) {
            adapter = new MediaAdapter(audioType);
            adapter.play(audioType, fileName);
        } else {
            System.out.println("Invalid format: " + audioType);
        }
    }
}

public class AdapterDemo {
    public static void main(String[] args) {
        AudioPlayer player = new AudioPlayer();
        player.play("mp3", "song.mp3");
        player.play("vlc", "video.vlc");
        player.play("mp4", "movie.mp4");
    }
}

// ========================================
// DECORATOR PATTERN (Structural)
// ========================================

interface Coffee {
    double cost();
    String description();
}

class SimpleCoffee implements Coffee {
    public double cost() { return 5; }
    public String description() { return "Simple Coffee"; }
}

// Base Decorator
abstract class CoffeeDecorator implements Coffee {
    protected Coffee coffee;
    
    public CoffeeDecorator(Coffee coffee) {
        this.coffee = coffee;
    }
}

class MilkDecorator extends CoffeeDecorator {
    public MilkDecorator(Coffee coffee) { super(coffee); }
    
    public double cost() {
        return coffee.cost() + 2;
    }
    
    public String description() {
        return coffee.description() + ", Milk";
    }
}

class SugarDecorator extends CoffeeDecorator {
    public SugarDecorator(Coffee coffee) { super(coffee); }
    
    public double cost() {
        return coffee.cost() + 1;
    }
    
    public String description() {
        return coffee.description() + ", Sugar";
    }
}

public class DecoratorDemo {
    public static void main(String[] args) {
        Coffee coffee = new SimpleCoffee();
        System.out.println(coffee.description() + " - Cost: " + coffee.cost());
        
        coffee = new MilkDecorator(coffee);
        System.out.println(coffee.description() + " - Cost: " + coffee.cost());
        
        coffee = new SugarDecorator(coffee);
        System.out.println(coffee.description() + " - Cost: " + coffee.cost());
    }
}

// ========================================
// OBSERVER PATTERN (Behavioral)
// ========================================

import java.util.*;

interface Observer {
    void update(String message);
}

class Subject {
    private List<Observer> observers = new ArrayList<>();
    private String state;
    
    public void attach(Observer observer) {
        observers.add(observer);
    }
    
    public void detach(Observer observer) {
        observers.remove(observer);
    }
    
    public void notifyObservers(String message) {
        for(Observer observer : observers) {
            observer.update(message);
        }
    }
    
    public void setState(String state) {
        this.state = state;
        notifyObservers(state);
    }
}

class ConcreteObserver implements Observer {
    private String name;
    
    public ConcreteObserver(String name) {
        this.name = name;
    }
    
    public void update(String message) {
        System.out.println(name + " received: " + message);
    }
}

public class ObserverDemo {
    public static void main(String[] args) {
        Subject subject = new Subject();
        
        Observer o1 = new ConcreteObserver("Observer1");
        Observer o2 = new ConcreteObserver("Observer2");
        Observer o3 = new ConcreteObserver("Observer3");
        
        subject.attach(o1);
        subject.attach(o2);
        subject.attach(o3);
        
        subject.setState("Update Available!");
    }
}

// ========================================
// STRATEGY PATTERN (Behavioral)
// ========================================

interface PaymentStrategy {
    void pay(int amount);
}

class CreditCardPayment implements PaymentStrategy {
    private String cardNumber;
    
    public CreditCardPayment(String cardNumber) {
        this.cardNumber = cardNumber;
    }
    
    public void pay(int amount) {
        System.out.println("Paid " + amount + " using Credit Card: " + cardNumber);
    }
}

class PayPalPayment implements PaymentStrategy {
    private String email;
    
    public PayPalPayment(String email) {
        this.email = email;
    }
    
    public void pay(int amount) {
        System.out.println("Paid " + amount + " using PayPal: " + email);
    }
}

class BitcoinPayment implements PaymentStrategy {
    private String walletAddress;
    
    public BitcoinPayment(String walletAddress) {
        this.walletAddress = walletAddress;
    }
    
    public void pay(int amount) {
        System.out.println("Paid " + amount + " using Bitcoin: " + walletAddress);
    }
}

class ShoppingCart {
    private PaymentStrategy paymentStrategy;
    
    public void setPaymentStrategy(PaymentStrategy strategy) {
        this.paymentStrategy = strategy;
    }
    
    public void checkout(int amount) {
        paymentStrategy.pay(amount);
    }
}

public class StrategyDemo {
    public static void main(String[] args) {
        ShoppingCart cart = new ShoppingCart();
        
        // Pay with credit card
        cart.setPaymentStrategy(new CreditCardPayment("1234-5678-9012"));
        cart.checkout(500);
        
        // Pay with PayPal
        cart.setPaymentStrategy(new PayPalPayment("user@example.com"));
        cart.checkout(1000);
        
        // Pay with Bitcoin
        cart.setPaymentStrategy(new BitcoinPayment("1A2B3C4D5E"));
        cart.checkout(250);
    }
}

// ========================================
// BUILDER PATTERN (Creational)
// ========================================

class Computer {
    private String CPU;
    private String RAM;
    private String storage;
    private String GPU;
    
    private Computer(Builder builder) {
        this.CPU = builder.CPU;
        this.RAM = builder.RAM;
        this.storage = builder.storage;
        this.GPU = builder.GPU;
    }
    
    public static class Builder {
        private String CPU;
        private String RAM;
        private String storage;
        private String GPU;
        
        public Builder setCPU(String CPU) {
            this.CPU = CPU;
            return this;
        }
        
        public Builder setRAM(String RAM) {
            this.RAM = RAM;
            return this;
        }
        
        public Builder setStorage(String storage) {
            this.storage = storage;
            return this;
        }
        
        public Builder setGPU(String GPU) {
            this.GPU = GPU;
            return this;
        }
        
        public Computer build() {
            return new Computer(this);
        }
    }
    
    @Override
    public String toString() {
        return "Computer[CPU=" + CPU + ", RAM=" + RAM + 
               ", Storage=" + storage + ", GPU=" + GPU + "]";
    }
}

public class BuilderDemo {
    public static void main(String[] args) {
        Computer computer = new Computer.Builder()
            .setCPU("Intel i7")
            .setRAM("16GB")
            .setStorage("512GB SSD")
            .setGPU("NVIDIA RTX 3060")
            .build();
        
        System.out.println(computer);
    }
}
''',
    revisionPoints: [
      'Design patterns are reusable solutions to common software problems',
      'Three main categories: Creational, Structural, and Behavioral',
      'Singleton ensures only one instance of a class exists',
      'Factory pattern creates objects without specifying exact class',
      'Adapter pattern allows incompatible interfaces to work together',
      'Decorator pattern adds behavior to objects dynamically',
      'Observer pattern implements one-to-many dependency',
      'Strategy pattern defines family of interchangeable algorithms',
      'Builder pattern constructs complex objects step by step',
      'Singleton uses private constructor and static getInstance() method',
      'Bill Pugh Singleton is the best thread-safe approach',
      'Factory pattern promotes loose coupling',
      'Decorator wraps objects to add new functionality',
      'Observer pattern is used in event handling systems',
      'Strategy pattern enables runtime algorithm selection',
    ],
    quizQuestions: [
      Question(
        question: 'Which pattern ensures a class has only one instance?',
        options: ['Factory', 'Singleton', 'Builder', 'Prototype'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which category does the Singleton pattern belong to?',
        options: ['Creational', 'Structural', 'Behavioral', 'Architectural'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which pattern creates objects without specifying their exact class?',
        options: ['Singleton', 'Factory', 'Adapter', 'Observer'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which pattern allows incompatible interfaces to work together?',
        options: ['Adapter', 'Decorator', 'Bridge', 'Composite'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which pattern adds behavior to objects dynamically?',
        options: ['Proxy', 'Decorator', 'Adapter', 'Facade'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which pattern implements one-to-many dependency?',
        options: ['Strategy', 'Command', 'Observer', 'Mediator'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which pattern encapsulates a family of algorithms?',
        options: ['Template Method', 'Strategy', 'State', 'Chain of Responsibility'],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of constructor does Singleton pattern use?',
        options: ['Public', 'Private', 'Protected', 'Default'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is the best thread-safe Singleton approach?',
        options: ['Synchronized method', 'Double-checked locking', 'Bill Pugh Singleton', 'Eager initialization'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which pattern is commonly used in event handling?',
        options: ['Factory', 'Singleton', 'Observer', 'Strategy'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which category does the Decorator pattern belong to?',
        options: ['Creational', 'Structural', 'Behavioral', 'Functional'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which category does the Strategy pattern belong to?',
        options: ['Creational', 'Structural', 'Behavioral', 'Concurrent'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java8_features',
    title: '13. Java 8+ Features (Streams, Optional, DateTime API)',
    explanation: '''## Java 8+ Features (Streams, Optional, DateTime API)

### A. Introduction

**Definition:**
Java 8 introduced **several powerful features** to improve **code readability, performance, and functional programming style**. Key features include:

1. **Streams API** – Functional-style operations on collections.
2. **Optional** – Handle null values safely, avoiding `NullPointerException`.
3. **DateTime API** – Modern classes for date and time handling (`java.time` package).

**Key Points:**

* Java 8 emphasizes **functional programming**.
* Streams allow **lazy and parallel processing**.
* Optional avoids **null checks** clutter.
* DateTime API replaces old `Date` and `Calendar` classes with **immutable, thread-safe objects**.

---

### B. Streams API

**1. Basics of Streams**

* **Stream** = sequence of elements from a **Collection** or array.
* Supports **intermediate operations** (`filter`, `map`, `sorted`) and **terminal operations** (`collect`, `forEach`, `reduce`).

**Example: Basic Stream Operations**

```java
import java.util.*;
import java.util.stream.*;

public class StreamExample {
    public static void main(String[] args) {
        List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);

        // Filter even numbers and print
        numbers.stream()
               .filter(n -> n % 2 == 0)
               .forEach(System.out::println); // 2, 4

        // Map to squares
        List<Integer> squares = numbers.stream()
                                       .map(n -> n * n)
                                       .collect(Collectors.toList());
        System.out.println(squares); // [1, 4, 9, 16, 25]

        // Reduce to sum
        int sum = numbers.stream()
                         .reduce(0, Integer::sum);
        System.out.println(sum); // 15
    }
}
```

**2. Parallel Streams**

* Run operations in parallel using multiple threads.

```java
List<Integer> nums = Arrays.asList(1,2,3,4,5);
int sum = nums.parallelStream().reduce(0, Integer::sum);
System.out.println(sum); // 15
```

**Tips / Exam Notes:**

* Streams are **non-modifying** and **lazy**.
* Use **method references** for clean code: `System.out::println`.

---

### C. Optional

**Definition:**

* **Optional** is a container object that may or may not contain a **non-null value**.
* Helps avoid **null checks** and `NullPointerException`.

**Example: Basic Optional Usage**

```java
import java.util.Optional;

public class OptionalDemo {
    public static void main(String[] args) {
        Optional<String> name = Optional.of("Alice");
        System.out.println(name.isPresent()); // true
        System.out.println(name.get());       // Alice

        Optional<String> empty = Optional.empty();
        System.out.println(empty.isPresent()); // false

        // Default value
        System.out.println(empty.orElse("Default")); // Default
    }
}
```

**Common Methods:**

* `isPresent()` – check if value exists
* `get()` – retrieve value
* `orElse(value)` – return default if empty
* `ifPresent(Consumer)` – execute action if value exists

**Example: Optional with Stream**

```java
Optional<String> result = Arrays.asList("Bob", "Alice").stream()
                                  .filter(s -> s.startsWith("A"))
                                  .findFirst();

result.ifPresent(System.out::println); // Alice
```

---

### D. DateTime API (`java.time`)

**Problems with old Date API:**

* `java.util.Date` mutable → thread-unsafe
* Zero-based months → confusing
* Poor formatting support

**Java 8 Solution:** `java.time` package

**1. LocalDate / LocalTime / LocalDateTime**

```java
import java.time.*;

public class DateTimeDemo {
    public static void main(String[] args) {
        LocalDate date = LocalDate.now();
        LocalTime time = LocalTime.now();
        LocalDateTime dateTime = LocalDateTime.now();

        System.out.println(date);     // 2025-10-17
        System.out.println(time);     // 20:30:15.123
        System.out.println(dateTime); // 2025-10-17T20:30:15.123
    }
}
```

**2. Creating Specific Date/Time**

```java
LocalDate dob = LocalDate.of(2000, Month.JANUARY, 1);
LocalTime meeting = LocalTime.of(14, 30); // 2:30 PM
```

**3. Date/Time Manipulation**

```java
LocalDate today = LocalDate.now();
LocalDate nextWeek = today.plusWeeks(1);
LocalDate lastMonth = today.minusMonths(1);

System.out.println(nextWeek);
System.out.println(lastMonth);
```

**4. Formatting and Parsing**

```java
import java.time.format.DateTimeFormatter;

LocalDateTime now = LocalDateTime.now();
DateTimeFormatter formatter = DateTimeFormatter.ofPattern("dd-MM-yyyy HH:mm");
String formatted = now.format(formatter);
System.out.println(formatted); // 17-10-2025 20:30

LocalDateTime parsed = LocalDateTime.parse(formatted, formatter);
System.out.println(parsed);
```

**5. Duration and Period**

```java
LocalDate start = LocalDate.of(2025, 1, 1);
LocalDate end = LocalDate.of(2025, 10, 17);
Period p = Period.between(start, end);
System.out.println("Months: " + p.getMonths() + ", Days: " + p.getDays());

LocalTime t1 = LocalTime.of(10, 0);
LocalTime t2 = LocalTime.of(12, 30);
Duration d = Duration.between(t1, t2);
System.out.println(d.toHours() + " hours"); // 2 hours
```

---

### E. Best Practices / Exam Tips

* Use **Streams** for collection processing instead of loops.
* Use **Optional** to handle nullable values safely.
* Prefer **`LocalDate` / `LocalTime` / `LocalDateTime`** over old `Date`/`Calendar`.
* Always **format and parse dates using `DateTimeFormatter`**.
* Streams + Optional + DateTime are **widely asked in Java 8+ interviews**.
''',
    codeSnippet: '''
// ========================================
// STREAMS API - BASIC OPERATIONS
// ========================================

import java.util.*;
import java.util.stream.*;

public class StreamExample {
    public static void main(String[] args) {
        List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);

        // 1. Filter - Select even numbers
        System.out.println("Even numbers:");
        numbers.stream()
               .filter(n -> n % 2 == 0)
               .forEach(System.out::println); // 2, 4, 6, 8, 10

        // 2. Map - Square each number
        List<Integer> squares = numbers.stream()
                                       .map(n -> n * n)
                                       .collect(Collectors.toList());
        System.out.println("Squares: " + squares);

        // 3. Reduce - Sum all numbers
        int sum = numbers.stream()
                         .reduce(0, Integer::sum);
        System.out.println("Sum: " + sum); // 55

        // 4. Sorted - Sort in reverse order
        List<Integer> sorted = numbers.stream()
                                      .sorted(Comparator.reverseOrder())
                                      .collect(Collectors.toList());
        System.out.println("Sorted: " + sorted);

        // 5. Distinct - Remove duplicates
        List<Integer> withDuplicates = Arrays.asList(1, 2, 2, 3, 3, 4);
        List<Integer> distinct = withDuplicates.stream()
                                               .distinct()
                                               .collect(Collectors.toList());
        System.out.println("Distinct: " + distinct);

        // 6. Limit - Take first 3 elements
        List<Integer> limited = numbers.stream()
                                       .limit(3)
                                       .collect(Collectors.toList());
        System.out.println("Limited: " + limited);

        // 7. Skip - Skip first 5 elements
        List<Integer> skipped = numbers.stream()
                                       .skip(5)
                                       .collect(Collectors.toList());
        System.out.println("Skipped: " + skipped);
    }
}

// ========================================
// STREAMS API - STRING OPERATIONS
// ========================================

public class StreamStringDemo {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "David", "Eve");

        // Filter names starting with 'A'
        names.stream()
             .filter(name -> name.startsWith("A"))
             .forEach(System.out::println); // Alice

        // Convert to uppercase
        List<String> upperNames = names.stream()
                                       .map(String::toUpperCase)
                                       .collect(Collectors.toList());
        System.out.println(upperNames);

        // Count names with length > 3
        long count = names.stream()
                          .filter(name -> name.length() > 3)
                          .count();
        System.out.println("Count: " + count); // 4

        // Find first name starting with 'C'
        Optional<String> first = names.stream()
                                      .filter(name -> name.startsWith("C"))
                                      .findFirst();
        first.ifPresent(System.out::println); // Charlie

        // Check if any name starts with 'Z'
        boolean anyMatch = names.stream()
                                .anyMatch(name -> name.startsWith("Z"));
        System.out.println("Any Z? " + anyMatch); // false
    }
}

// ========================================
// PARALLEL STREAMS
// ========================================

public class ParallelStreamDemo {
    public static void main(String[] args) {
        List<Integer> nums = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);

        // Parallel stream for faster processing
        int sum = nums.parallelStream()
                      .reduce(0, Integer::sum);
        System.out.println("Parallel sum: " + sum); // 55

        // Sequential vs Parallel
        long start = System.currentTimeMillis();
        int seqSum = nums.stream().reduce(0, Integer::sum);
        long seqTime = System.currentTimeMillis() - start;

        start = System.currentTimeMillis();
        int parSum = nums.parallelStream().reduce(0, Integer::sum);
        long parTime = System.currentTimeMillis() - start;

        System.out.println("Sequential time: " + seqTime + "ms");
        System.out.println("Parallel time: " + parTime + "ms");
    }
}

// ========================================
// OPTIONAL - BASIC USAGE
// ========================================

import java.util.Optional;

public class OptionalDemo {
    public static void main(String[] args) {
        // Create Optional with value
        Optional<String> name = Optional.of("Alice");
        System.out.println(name.isPresent()); // true
        System.out.println(name.get());       // Alice

        // Create empty Optional
        Optional<String> empty = Optional.empty();
        System.out.println(empty.isPresent()); // false

        // orElse - provide default value
        String value1 = empty.orElse("Default");
        System.out.println(value1); // Default

        // orElseGet - lazy default value
        String value2 = empty.orElseGet(() -> "Lazy Default");
        System.out.println(value2); // Lazy Default

        // ifPresent - execute action if value exists
        name.ifPresent(n -> System.out.println("Hello " + n)); // Hello Alice

        // ofNullable - handle potential null
        String nullableString = null;
        Optional<String> optional = Optional.ofNullable(nullableString);
        System.out.println(optional.isPresent()); // false

        // map - transform value
        Optional<Integer> length = name.map(String::length);
        System.out.println(length.get()); // 5

        // filter - apply condition
        Optional<String> filtered = name.filter(n -> n.startsWith("A"));
        System.out.println(filtered.isPresent()); // true
    }
}

// ========================================
// OPTIONAL WITH STREAMS
// ========================================

public class OptionalStreamDemo {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("Bob", "Alice", "Charlie");

        // Find first name starting with 'A'
        Optional<String> result = names.stream()
                                       .filter(s -> s.startsWith("A"))
                                       .findFirst();

        result.ifPresent(System.out::println); // Alice

        // Find any name starting with 'C'
        Optional<String> anyResult = names.stream()
                                          .filter(s -> s.startsWith("C"))
                                          .findAny();

        System.out.println(anyResult.orElse("Not found")); // Charlie
    }
}

// ========================================
// DATETIME API - LocalDate, LocalTime, LocalDateTime
// ========================================

import java.time.*;

public class DateTimeDemo {
    public static void main(String[] args) {
        // Current date and time
        LocalDate date = LocalDate.now();
        LocalTime time = LocalTime.now();
        LocalDateTime dateTime = LocalDateTime.now();

        System.out.println("Date: " + date);     // 2025-10-17
        System.out.println("Time: " + time);     // 20:30:15.123
        System.out.println("DateTime: " + dateTime); // 2025-10-17T20:30:15.123

        // Create specific date/time
        LocalDate dob = LocalDate.of(2000, Month.JANUARY, 1);
        LocalTime meeting = LocalTime.of(14, 30); // 2:30 PM
        LocalDateTime appointment = LocalDateTime.of(2025, 10, 20, 15, 0);

        System.out.println("DOB: " + dob);
        System.out.println("Meeting: " + meeting);
        System.out.println("Appointment: " + appointment);

        // Get components
        System.out.println("Year: " + date.getYear());
        System.out.println("Month: " + date.getMonth());
        System.out.println("Day: " + date.getDayOfMonth());
        System.out.println("Hour: " + time.getHour());
        System.out.println("Minute: " + time.getMinute());
    }
}

// ========================================
// DATETIME API - MANIPULATION
// ========================================

public class DateManipulationDemo {
    public static void main(String[] args) {
        LocalDate today = LocalDate.now();
        System.out.println("Today: " + today);

        // Add/Subtract days, weeks, months, years
        LocalDate nextWeek = today.plusWeeks(1);
        LocalDate lastMonth = today.minusMonths(1);
        LocalDate nextYear = today.plusYears(1);

        System.out.println("Next week: " + nextWeek);
        System.out.println("Last month: " + lastMonth);
        System.out.println("Next year: " + nextYear);

        // With methods - replace components
        LocalDate modified = today.withYear(2030).withMonth(12).withDayOfMonth(25);
        System.out.println("Modified: " + modified); // 2030-12-25

        // Check if leap year
        System.out.println("Is leap year? " + today.isLeapYear());

        // Compare dates
        LocalDate date1 = LocalDate.of(2025, 1, 1);
        LocalDate date2 = LocalDate.of(2025, 12, 31);
        System.out.println("date1 before date2? " + date1.isBefore(date2)); // true
    }
}

// ========================================
// DATETIME API - FORMATTING AND PARSING
// ========================================

import java.time.format.DateTimeFormatter;

public class DateFormattingDemo {
    public static void main(String[] args) {
        LocalDateTime now = LocalDateTime.now();

        // Predefined formatters
        String iso = now.format(DateTimeFormatter.ISO_DATE_TIME);
        System.out.println("ISO: " + iso);

        // Custom formatter
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("dd-MM-yyyy HH:mm:ss");
        String formatted = now.format(formatter);
        System.out.println("Formatted: " + formatted); // 17-10-2025 20:30:15

        // Parse string to LocalDateTime
        String dateString = "25-12-2025 10:30:00";
        LocalDateTime parsed = LocalDateTime.parse(dateString, formatter);
        System.out.println("Parsed: " + parsed);

        // Different patterns
        DateTimeFormatter f1 = DateTimeFormatter.ofPattern("yyyy/MM/dd");
        DateTimeFormatter f2 = DateTimeFormatter.ofPattern("MMM dd, yyyy");
        DateTimeFormatter f3 = DateTimeFormatter.ofPattern("EEEE, dd MMMM yyyy");

        LocalDate date = LocalDate.now();
        System.out.println(date.format(f1)); // 2025/10/17
        System.out.println(date.format(f2)); // Oct 17, 2025
        System.out.println(date.format(f3)); // Friday, 17 October 2025
    }
}

// ========================================
// DATETIME API - DURATION AND PERIOD
// ========================================

public class DurationPeriodDemo {
    public static void main(String[] args) {
        // Period - Date-based (years, months, days)
        LocalDate start = LocalDate.of(2025, 1, 1);
        LocalDate end = LocalDate.of(2025, 10, 17);
        Period period = Period.between(start, end);

        System.out.println("Period: " + period);
        System.out.println("Months: " + period.getMonths());
        System.out.println("Days: " + period.getDays());

        // Duration - Time-based (hours, minutes, seconds)
        LocalTime t1 = LocalTime.of(10, 0);
        LocalTime t2 = LocalTime.of(12, 30);
        Duration duration = Duration.between(t1, t2);

        System.out.println("Duration: " + duration);
        System.out.println("Hours: " + duration.toHours()); // 2
        System.out.println("Minutes: " + duration.toMinutes()); // 150

        // Duration with DateTime
        LocalDateTime dt1 = LocalDateTime.of(2025, 10, 17, 10, 0);
        LocalDateTime dt2 = LocalDateTime.of(2025, 10, 17, 15, 30);
        Duration d = Duration.between(dt1, dt2);
        System.out.println("Duration: " + d.toHours() + " hours"); // 5 hours
    }
}
''',
    revisionPoints: [
      'Java 8 introduced Streams, Optional, and DateTime API',
      'Streams provide functional-style operations on collections',
      'Streams are lazy and non-modifying',
      'Intermediate operations: filter, map, sorted, distinct',
      'Terminal operations: collect, forEach, reduce, count',
      'Parallel streams enable multi-threaded processing',
      'Optional helps avoid NullPointerException',
      'Optional methods: isPresent(), get(), orElse(), ifPresent()',
      'LocalDate represents date without time',
      'LocalTime represents time without date',
      'LocalDateTime combines date and time',
      'DateTime API is immutable and thread-safe',
      'DateTimeFormatter used for formatting and parsing',
      'Period measures date-based duration (years, months, days)',
      'Duration measures time-based duration (hours, minutes, seconds)',
    ],
    quizQuestions: [
      Question(
        question: 'Which class helps prevent NullPointerException?',
        options: ['Stream', 'Optional', 'Lambda', 'Future'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which operation is a terminal operation in Streams?',
        options: ['filter()', 'map()', 'collect()', 'sorted()'],
        correctIndex: 2,
      ),
      Question(
        question: 'What package contains the new DateTime API?',
        options: ['java.util', 'java.time', 'java.date', 'java.calendar'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class represents date without time?',
        options: ['LocalTime', 'LocalDate', 'LocalDateTime', 'Date'],
        correctIndex: 1,
      ),
      Question(
        question: 'Are streams modifying or non-modifying?',
        options: ['Modifying', 'Non-modifying', 'Depends on operation', 'Both'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method executes an action if Optional has a value?',
        options: ['execute()', 'ifPresent()', 'doIfPresent()', 'apply()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the advantage of parallel streams?',
        options: ['Simplicity', 'Multi-threaded processing', 'Less code', 'Better syntax'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used for formatting dates?',
        options: ['DateFormat', 'SimpleDateFormat', 'DateTimeFormatter', 'Formatter'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does Period measure?',
        options: ['Time-based duration', 'Date-based duration', 'Timestamps', 'Milliseconds'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does Duration measure?',
        options: ['Date-based duration', 'Time-based duration', 'Years and months', 'Days only'],
        correctIndex: 1,
      ),
      Question(
        question: 'Is the DateTime API thread-safe?',
        options: ['No', 'Yes', 'Depends on usage', 'Only with synchronization'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method provides a default value in Optional?',
        options: ['getOrDefault()', 'orElse()', 'defaultValue()', 'getDefault()'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_modules_packages',
    title: '14. Java Modules and Packages',
    explanation: '''## Java Modules and Packages

### A. Introduction

**Definition:**

* **Packages**: A mechanism to **organize Java classes and interfaces** into namespaces, avoiding naming conflicts and improving code maintainability.
* **Modules**: Introduced in **Java 9**, a module is a **higher-level grouping of packages** that specifies dependencies and encapsulation.

**Key Points:**

* Packages are **compile-time organizational units**.
* Modules are **runtime units** that **control visibility and dependencies**.
* `module-info.java` defines modules and their relationships.

---

### B. Java Packages

**1. Creating and Using Packages**

```java
// File: com/example/util/Calculator.java
package com.example.util;

public class Calculator {
    public static int add(int a, int b) {
        return a + b;
    }
}
```

**Using the package in another class:**

```java
import com.example.util.Calculator;

public class PackageDemo {
    public static void main(String[] args) {
        int result = Calculator.add(5, 3);
        System.out.println("Result: " + result); // 8
    }
}
```

**Notes:**

* Package names are usually **reverse domain names** (`com.company.project`).
* Use `javac -d . Calculator.java` to compile into a package directory.
* Organizes classes for **reuse and modularity**.

**2. Access Modifiers in Packages**

| Modifier     | Same Class | Same Package | Subclass | World |
| ------------ | ---------- | ------------ | -------- | ----- |
| `public`     | Yes        | Yes          | Yes      | Yes   |
| `protected`  | Yes        | Yes          | Yes      | No    |
| default (no) | Yes        | Yes          | No       | No    |
| `private`    | Yes        | No           | No       | No    |

---

### C. Java Modules (Java 9+)

**1. Introduction**

* Modules allow **strong encapsulation**, controlling **which packages are exposed**.
* A module can **require other modules** and **export its own packages**.
* Helps build **large-scale applications** with clear dependencies.

---

**2. Creating a Module**

**Step 1: Define module**

```java
// File: module-info.java
module math.module {
    exports com.example.util;   // Packages exposed to other modules
}
```

**Step 2: Use module in another module**

```java
// File: module-info.java
module app.module {
    requires math.module;  // Module dependency
}
```

**Step 3: Compile and Run Modules**

```bash
javac -d mods/math.module com/example/util/Calculator.java module-info.java
javac -d mods/app.module --module-path mods app/MainApp.java module-info.java
java --module-path mods -m app.module/app.MainApp
```

---

### D. Example: Using Packages and Modules Together

**Calculator Module (`math.module`)**

```java
package com.example.util;

public class Calculator {
    public static int multiply(int a, int b) {
        return a * b;
    }
}
```

**App Module (`app.module`)**

```java
import com.example.util.Calculator;

public class MainApp {
    public static void main(String[] args) {
        System.out.println(Calculator.multiply(4, 5)); // 20
    }
}
```

**Module Definitions:**

```java
// math.module/module-info.java
module math.module {
    exports com.example.util;
}

// app.module/module-info.java
module app.module {
    requires math.module;
}
```

---

### E. Best Practices / Exam Tips

* **Packages:** Organize classes, avoid naming conflicts.
* **Modules:** Encapsulate packages, control dependencies, enable scalable applications.
* **Access Modifiers:** Understand visibility within packages and modules.
* Always use **`module-info.java`** in modular projects.
* Modules are **widely used in modern Java applications** for large-scale systems.
''',
    codeSnippet: '''
// ========================================
// PACKAGES - BASIC USAGE
// ========================================

// File: com/example/util/Calculator.java
package com.example.util;

public class Calculator {
    public static int add(int a, int b) {
        return a + b;
    }
    
    public static int subtract(int a, int b) {
        return a - b;
    }
    
    public static int multiply(int a, int b) {
        return a * b;
    }
    
    public static int divide(int a, int b) {
        if (b == 0) {
            throw new ArithmeticException("Division by zero");
        }
        return a / b;
    }
}

// File: com/example/util/StringHelper.java
package com.example.util;

public class StringHelper {
    public static String reverse(String str) {
        return new StringBuilder(str).reverse().toString();
    }
    
    public static boolean isPalindrome(String str) {
        String reversed = reverse(str);
        return str.equalsIgnoreCase(reversed);
    }
}

// ========================================
// USING PACKAGES
// ========================================

// Import specific class
import com.example.util.Calculator;

// Import all classes from package
import com.example.util.*;

public class PackageDemo {
    public static void main(String[] args) {
        // Using Calculator from package
        int sum = Calculator.add(5, 3);
        int product = Calculator.multiply(4, 5);
        
        System.out.println("Sum: " + sum);         // 8
        System.out.println("Product: " + product); // 20
        
        // Using StringHelper
        String reversed = StringHelper.reverse("Java");
        boolean isPalin = StringHelper.isPalindrome("level");
        
        System.out.println("Reversed: " + reversed); // avaJ
        System.out.println("Is palindrome: " + isPalin); // true
    }
}

// ========================================
// ACCESS MODIFIERS IN PACKAGES
// ========================================

package com.example.access;

public class AccessDemo {
    public int publicVar = 10;      // Accessible everywhere
    protected int protectedVar = 20; // Accessible in same package & subclasses
    int defaultVar = 30;             // Accessible in same package only
    private int privateVar = 40;     // Accessible in same class only
    
    public void publicMethod() {
        System.out.println("Public method");
    }
    
    protected void protectedMethod() {
        System.out.println("Protected method");
    }
    
    void defaultMethod() {
        System.out.println("Default method");
    }
    
    private void privateMethod() {
        System.out.println("Private method");
    }
}

// Different package
package com.example.test;

import com.example.access.AccessDemo;

public class TestAccess {
    public static void main(String[] args) {
        AccessDemo obj = new AccessDemo();
        
        obj.publicMethod();      // OK - public accessible everywhere
        // obj.protectedMethod(); // Error - not a subclass
        // obj.defaultMethod();   // Error - different package
        // obj.privateMethod();   // Error - private
    }
}

// ========================================
// STATIC IMPORTS
// ========================================

// Import static members directly
import static java.lang.Math.PI;
import static java.lang.Math.pow;
import static java.lang.Math.sqrt;

public class StaticImportDemo {
    public static void main(String[] args) {
        // Use without class name
        System.out.println("PI: " + PI);
        System.out.println("Power: " + pow(2, 3));    // 8.0
        System.out.println("Square root: " + sqrt(16)); // 4.0
    }
}

// ========================================
// MODULES - MODULE-INFO.JAVA
// ========================================

// File: math.module/module-info.java
module math.module {
    // Export packages to other modules
    exports com.example.util;
    
    // Can export to specific modules only
    // exports com.example.internal to app.module;
}

// File: app.module/module-info.java
module app.module {
    // Declare dependency on other module
    requires math.module;
    
    // Require standard Java modules
    requires java.sql;
    requires java.logging;
}

// ========================================
// MODULE EXAMPLE - MATH MODULE
// ========================================

// File: math.module/com/example/util/MathUtil.java
package com.example.util;

public class MathUtil {
    public static int factorial(int n) {
        if (n <= 1) return 1;
        return n * factorial(n - 1);
    }
    
    public static boolean isPrime(int n) {
        if (n < 2) return false;
        for (int i = 2; i <= Math.sqrt(n); i++) {
            if (n % i == 0) return false;
        }
        return true;
    }
}

// File: math.module/module-info.java
module math.module {
    exports com.example.util;
}

// ========================================
// MODULE EXAMPLE - APP MODULE
// ========================================

// File: app.module/com/example/app/MainApp.java
package com.example.app;

import com.example.util.MathUtil;

public class MainApp {
    public static void main(String[] args) {
        System.out.println("Factorial of 5: " + MathUtil.factorial(5)); // 120
        System.out.println("Is 17 prime? " + MathUtil.isPrime(17));     // true
        System.out.println("Is 20 prime? " + MathUtil.isPrime(20));     // false
    }
}

// File: app.module/module-info.java
module app.module {
    requires math.module;
}

// ========================================
// COMPILING AND RUNNING MODULES
// ========================================

/*
Compile math.module:
javac -d mods/math.module \\
    math.module/module-info.java \\
    math.module/com/example/util/MathUtil.java

Compile app.module:
javac -d mods/app.module \\
    --module-path mods \\
    app.module/module-info.java \\
    app.module/com/example/app/MainApp.java

Run app.module:
java --module-path mods \\
     -m app.module/com.example.app.MainApp
*/

// ========================================
// ADVANCED MODULE FEATURES
// ========================================

// 1. TRANSITIVE DEPENDENCIES
module app.module {
    // Modules depending on app.module also get access to math.module
    requires transitive math.module;
}

// 2. OPENS (for reflection)
module my.module {
    // Allow reflection access to package
    opens com.example.internal;
    
    // Open to specific module
    opens com.example.data to hibernate.core;
}

// 3. PROVIDES/USES (for services)
module service.provider {
    provides com.example.Service with com.example.ServiceImpl;
}

module service.consumer {
    uses com.example.Service;
}

// ========================================
// PACKAGE NAMING CONVENTIONS
// ========================================

/*
Standard naming convention: reverse domain name

Examples:
com.company.project.module
org.opensource.library.core
edu.university.department.app

Benefits:
1. Avoids naming conflicts
2. Shows ownership
3. Organized hierarchy
4. Easy to identify source
*/

// ========================================
// COMPLETE EXAMPLE WITH MULTIPLE PACKAGES
// ========================================

// File: com/example/model/Student.java
package com.example.model;

public class Student {
    private String name;
    private int age;
    
    public Student(String name, int age) {
        this.name = name;
        this.age = age;
    }
    
    public String getName() { return name; }
    public int getAge() { return age; }
}

// File: com/example/service/StudentService.java
package com.example.service;

import com.example.model.Student;
import java.util.*;

public class StudentService {
    private List<Student> students = new ArrayList<>();
    
    public void addStudent(Student student) {
        students.add(student);
    }
    
    public List<Student> getAllStudents() {
        return students;
    }
}

// File: com/example/app/MainApp.java
package com.example.app;

import com.example.model.Student;
import com.example.service.StudentService;

public class MainApp {
    public static void main(String[] args) {
        StudentService service = new StudentService();
        
        service.addStudent(new Student("Alice", 20));
        service.addStudent(new Student("Bob", 22));
        
        for (Student s : service.getAllStudents()) {
            System.out.println(s.getName() + " - " + s.getAge());
        }
    }
}
''',
    revisionPoints: [
      'Packages organize classes into namespaces and avoid naming conflicts',
      'Package names follow reverse domain name convention',
      'Modules were introduced in Java 9',
      'Modules provide higher-level grouping of packages',
      'module-info.java defines module structure and dependencies',
      'exports keyword makes packages accessible to other modules',
      'requires keyword declares module dependencies',
      'Packages are compile-time organizational units',
      'Modules are runtime units that control visibility',
      'Access modifiers control visibility: public, protected, default, private',
      'public modifier allows access from anywhere',
      'private modifier restricts access to same class only',
      'Default (no modifier) allows access within same package',
      'protected allows access in same package and subclasses',
      'Static imports allow direct use of static members',
    ],
    quizQuestions: [
      Question(
        question: 'What keyword is used to make module contents accessible?',
        options: ['exports', 'public', 'open', 'accessible'],
        correctIndex: 0,
      ),
      Question(
        question: 'In which Java version were modules introduced?',
        options: ['Java 7', 'Java 8', 'Java 9', 'Java 10'],
        correctIndex: 2,
      ),
      Question(
        question: 'What file defines module structure and dependencies?',
        options: ['pom.xml', 'build.gradle', 'module-info.java', 'manifest.mf'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which keyword declares a module dependency?',
        options: ['imports', 'requires', 'depends', 'uses'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the standard naming convention for packages?',
        options: ['CamelCase', 'snake_case', 'Reverse domain name', 'kebab-case'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which access modifier allows access only within the same class?',
        options: ['public', 'protected', 'default', 'private'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which access modifier allows access everywhere?',
        options: ['public', 'protected', 'default', 'private'],
        correctIndex: 0,
      ),
      Question(
        question: 'Are packages compile-time or runtime units?',
        options: ['Compile-time', 'Runtime', 'Both', 'Neither'],
        correctIndex: 0,
      ),
      Question(
        question: 'Are modules compile-time or runtime units?',
        options: ['Compile-time', 'Runtime', 'Both', 'Neither'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which modifier allows access in same package and subclasses?',
        options: ['public', 'protected', 'default', 'private'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the transitive keyword do in module dependencies?',
        options: ['Makes module private', 'Propagates dependency', 'Removes dependency', 'Hides module'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of packages?',
        options: ['Speed up code', 'Organize classes', 'Improve security', 'Enable threading'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_javafx_gui',
    title: '15. JavaFX and GUI Development',
    explanation: '''## JavaFX and GUI Development

### A. Introduction

**Definition:**

* **JavaFX** is a **modern Java library** used to build **rich GUI applications** with **desktop and embedded interfaces**.
* It replaces Swing in modern Java applications.

**Key Points:**

* Supports **2D/3D graphics, animations, and multimedia**.
* FXML allows **declarative UI design** (like HTML for JavaFX).
* Uses **Scene Graph**: a hierarchical tree of UI components.

---

### B. JavaFX Architecture

| Component   | Purpose                                                |
| ----------- | ------------------------------------------------------ |
| Stage       | Main window of the application                         |
| Scene       | Container for all UI elements                          |
| Node        | Base class for all UI controls (buttons, labels, etc.) |
| Layout Pane | Organizes nodes (VBox, HBox, BorderPane, GridPane)     |
| Controls    | UI elements like Button, TextField, Label, TableView   |

---

### C. Setting Up JavaFX

1. **Add JavaFX library** to your project (via **Maven, Gradle, or SDK**).
2. **Extend `Application` class**.
3. Override **`start(Stage primaryStage)`** method.

**Basic Template:**

```java
import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;

public class HelloJavaFX extends Application {
    @Override
    public void start(Stage primaryStage) {
        Label label = new Label("Hello, JavaFX!");
        StackPane root = new StackPane(label);
        Scene scene = new Scene(root, 400, 300);

        primaryStage.setTitle("JavaFX Demo");
        primaryStage.setScene(scene);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}
```

---

### D. Common Layout Panes

| Layout Pane    | Description                              |
| -------------- | ---------------------------------------- |
| **VBox**       | Vertical arrangement of nodes            |
| **HBox**       | Horizontal arrangement                   |
| **BorderPane** | Top, bottom, left, right, center regions |
| **GridPane**   | Row and column grid layout               |
| **StackPane**  | Stack nodes on top of each other         |

**Example: VBox Layout**

```java
import javafx.scene.layout.VBox;
import javafx.scene.control.Button;

VBox vbox = new VBox(10); // spacing 10 px
Button b1 = new Button("Button 1");
Button b2 = new Button("Button 2");
vbox.getChildren().addAll(b1, b2);
```

---

### E. Handling Events

**Event Handling Example:**

```java
Button btn = new Button("Click Me");
btn.setOnAction(e -> System.out.println("Button Clicked!"));
```

* JavaFX supports **lambda expressions** for event handling.
* Events can be **mouse events, keyboard events, or UI control events**.

---

### F. Using FXML for Declarative UI

1. **FXML File (hello.fxml)**

```xml
<VBox xmlns="http://javafx.com/javafx" xmlns:fx="http://javafx.com/fxml">
    <Label text="Hello FXML!"/>
    <Button text="Click Me" onAction="#handleClick"/>
</VBox>
```

2. **Controller Class**

```java
import javafx.fxml.FXML;

public class Controller {
    @FXML
    private void handleClick() {
        System.out.println("Button clicked via FXML!");
    }
}
```

3. **Load FXML in Application**

```java
import javafx.fxml.FXMLLoader;
import javafx.scene.Parent;

Parent root = FXMLLoader.load(getClass().getResource("hello.fxml"));
Scene scene = new Scene(root, 400, 300);
primaryStage.setScene(scene);
primaryStage.show();
```

---

### G. Styling with CSS

```css
/* style.css */
.label {
    -fx-font-size: 18px;
    -fx-text-fill: darkblue;
}
.button {
    -fx-background-color: lightgreen;
    -fx-font-weight: bold;
}
```

* Apply CSS:

```java
scene.getStylesheets().add(getClass().getResource("style.css").toExternalForm());
```

---

### H. Common JavaFX Controls

| Control                | Description                |
| ---------------------- | -------------------------- |
| Button                 | Clickable button           |
| Label                  | Display text               |
| TextField              | Single-line input          |
| TextArea               | Multi-line input           |
| CheckBox / RadioButton | Selection controls         |
| TableView / ListView   | Display list or table data |
| MenuBar / MenuItem     | Menu system                |

---

### I. Best Practices / Exam Tips

* Use **Scene Graph** properly for hierarchical UI.
* Prefer **FXML + Controller** for large projects.
* Use **lambda expressions** for clean event handling.
* Use **CSS** for styling instead of inline properties.
* Understand **layout panes** for responsive design.
''',
    codeSnippet: '''
// ========================================
// BASIC JAVAFX APPLICATION
// ========================================

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;

public class HelloJavaFX extends Application {
    @Override
    public void start(Stage primaryStage) {
        // Create a label
        Label label = new Label("Hello, JavaFX!");
        
        // Create a layout pane
        StackPane root = new StackPane(label);
        
        // Create a scene
        Scene scene = new Scene(root, 400, 300);
        
        // Set up the stage (window)
        primaryStage.setTitle("JavaFX Demo");
        primaryStage.setScene(scene);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);  // Launch the JavaFX application
    }
}

// ========================================
// JAVAFX WITH BUTTON AND EVENT HANDLING
// ========================================

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Button;
import javafx.scene.control.Label;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

public class ButtonDemo extends Application {
    @Override
    public void start(Stage primaryStage) {
        Label label = new Label("Click the button!");
        Button btn = new Button("Click Me");
        
        // Event handling with lambda
        btn.setOnAction(e -> {
            label.setText("Button was clicked!");
            System.out.println("Button Clicked!");
        });
        
        VBox vbox = new VBox(10);  // 10px spacing
        vbox.getChildren().addAll(label, btn);
        
        Scene scene = new Scene(vbox, 300, 200);
        primaryStage.setTitle("Button Demo");
        primaryStage.setScene(scene);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

// ========================================
// LAYOUT PANES - VBOX (VERTICAL)
// ========================================

import javafx.scene.layout.VBox;
import javafx.scene.control.Button;
import javafx.scene.control.Label;

VBox vbox = new VBox(10); // 10px spacing between children
vbox.setPadding(new Insets(15, 20, 15, 20)); // Padding

Label label = new Label("Choose an option:");
Button b1 = new Button("Button 1");
Button b2 = new Button("Button 2");
Button b3 = new Button("Button 3");

vbox.getChildren().addAll(label, b1, b2, b3);

// ========================================
// LAYOUT PANES - HBOX (HORIZONTAL)
// ========================================

import javafx.scene.layout.HBox;
import javafx.scene.control.Button;

HBox hbox = new HBox(15); // 15px spacing
Button btn1 = new Button("Save");
Button btn2 = new Button("Cancel");
Button btn3 = new Button("Delete");

hbox.getChildren().addAll(btn1, btn2, btn3);

// ========================================
// LAYOUT PANES - BORDERPANE
// ========================================

import javafx.scene.layout.BorderPane;
import javafx.scene.control.*;

BorderPane borderPane = new BorderPane();

// Set regions
borderPane.setTop(new MenuBar());
borderPane.setLeft(new ListView<String>());
borderPane.setCenter(new TextArea());
borderPane.setRight(new VBox());
borderPane.setBottom(new Label("Status: Ready"));

// ========================================
// LAYOUT PANES - GRIDPANE
// ========================================

import javafx.scene.layout.GridPane;
import javafx.scene.control.*;

GridPane grid = new GridPane();
grid.setHgap(10);  // Horizontal gap
grid.setVgap(10);  // Vertical gap
grid.setPadding(new Insets(10, 10, 10, 10));

// Add elements at row, column positions
grid.add(new Label("Username:"), 0, 0);
grid.add(new TextField(), 1, 0);
grid.add(new Label("Password:"), 0, 1);
grid.add(new PasswordField(), 1, 1);
grid.add(new Button("Login"), 1, 2);

// ========================================
// TEXT INPUT CONTROLS
// ========================================

import javafx.scene.control.*;

// Single-line text input
TextField textField = new TextField();
textField.setPromptText("Enter your name");

// Multi-line text input
TextArea textArea = new TextArea();
textArea.setPromptText("Enter your message");
textArea.setPrefRowCount(5);

// Password field (masked input)
PasswordField passwordField = new PasswordField();
passwordField.setPromptText("Enter password");

// Get text from controls
String name = textField.getText();
String message = textArea.getText();
String password = passwordField.getText();

// ========================================
// SELECTION CONTROLS
// ========================================

// CheckBox
CheckBox checkBox1 = new CheckBox("Option 1");
CheckBox checkBox2 = new CheckBox("Option 2");

checkBox1.setOnAction(e -> {
    if (checkBox1.isSelected()) {
        System.out.println("Option 1 selected");
    }
});

// RadioButton (grouped)
ToggleGroup group = new ToggleGroup();
RadioButton rb1 = new RadioButton("Male");
RadioButton rb2 = new RadioButton("Female");
rb1.setToggleGroup(group);
rb2.setToggleGroup(group);

// ComboBox (dropdown)
ComboBox<String> comboBox = new ComboBox<>();
comboBox.getItems().addAll("Option 1", "Option 2", "Option 3");
comboBox.setValue("Option 1"); // Set default

// ========================================
// FXML - DECLARATIVE UI
// ========================================

// File: sample.fxml
/*
<?xml version="1.0" encoding="UTF-8"?>
<?import javafx.scene.control.*?>
<?import javafx.scene.layout.*?>

<VBox xmlns="http://javafx.com/javafx" 
      xmlns:fx="http://javafx.com/fxml"
      fx:controller="Controller" spacing="10" padding="20">
    
    <Label text="Hello FXML!" style="-fx-font-size: 18px;"/>
    <TextField fx:id="nameField" promptText="Enter name"/>
    <Button text="Submit" onAction="#handleSubmit"/>
    <Label fx:id="resultLabel"/>
</VBox>
*/

// Controller.java
import javafx.fxml.FXML;
import javafx.scene.control.*;

public class Controller {
    @FXML
    private TextField nameField;
    
    @FXML
    private Label resultLabel;
    
    @FXML
    private void handleSubmit() {
        String name = nameField.getText();
        resultLabel.setText("Hello, " + name + "!");
    }
}

// Loading FXML in Application
import javafx.fxml.FXMLLoader;
import javafx.scene.Parent;

Parent root = FXMLLoader.load(getClass().getResource("sample.fxml"));
Scene scene = new Scene(root, 400, 300);
primaryStage.setScene(scene);
primaryStage.show();

// ========================================
// CSS STYLING
// ========================================

// File: style.css
/*
.root {
    -fx-background-color: #f4f4f4;
}

.label {
    -fx-font-size: 16px;
    -fx-text-fill: darkblue;
    -fx-font-weight: bold;
}

.button {
    -fx-background-color: lightgreen;
    -fx-text-fill: white;
    -fx-font-size: 14px;
    -fx-padding: 10px 20px;
    -fx-background-radius: 5px;
}

.button:hover {
    -fx-background-color: green;
    -fx-cursor: hand;
}

.text-field {
    -fx-font-size: 14px;
    -fx-border-color: lightgray;
    -fx-border-radius: 3px;
}
*/

// Apply CSS to scene
scene.getStylesheets().add(getClass().getResource("style.css").toExternalForm());

// Inline CSS
button.setStyle("-fx-background-color: blue; -fx-text-fill: white;");

// ========================================
// TABLEVIEW - DISPLAY TABULAR DATA
// ========================================

import javafx.scene.control.*;
import javafx.scene.control.cell.PropertyValueFactory;
import javafx.collections.*;

// Model class
public class Person {
    private String name;
    private int age;
    
    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }
    
    public String getName() { return name; }
    public void setName(String name) { this.name = name; }
    public int getAge() { return age; }
    public void setAge(int age) { this.age = age; }
}

// TableView setup
TableView<Person> table = new TableView<>();

// Define columns
TableColumn<Person, String> nameCol = new TableColumn<>("Name");
nameCol.setCellValueFactory(new PropertyValueFactory<>("name"));

TableColumn<Person, Integer> ageCol = new TableColumn<>("Age");
ageCol.setCellValueFactory(new PropertyValueFactory<>("age"));

table.getColumns().addAll(nameCol, ageCol);

// Add data
ObservableList<Person> data = FXCollections.observableArrayList(
    new Person("Alice", 25),
    new Person("Bob", 30),
    new Person("Charlie", 28)
);
table.setItems(data);

// ========================================
// LISTVIEW
// ========================================

import javafx.scene.control.ListView;
import javafx.collections.*;

ListView<String> listView = new ListView<>();
ObservableList<String> items = FXCollections.observableArrayList(
    "Item 1", "Item 2", "Item 3", "Item 4"
);
listView.setItems(items);

// Handle selection
listView.getSelectionModel().selectedItemProperty().addListener(
    (observable, oldValue, newValue) -> {
        System.out.println("Selected: " + newValue);
    }
);

// ========================================
// MENUBAR AND MENU
// ========================================

import javafx.scene.control.*;

MenuBar menuBar = new MenuBar();

// File menu
Menu fileMenu = new Menu("File");
MenuItem newItem = new MenuItem("New");
MenuItem openItem = new MenuItem("Open");
MenuItem exitItem = new MenuItem("Exit");

exitItem.setOnAction(e -> System.exit(0));

fileMenu.getItems().addAll(newItem, openItem, new SeparatorMenuItem(), exitItem);

// Edit menu
Menu editMenu = new Menu("Edit");
MenuItem cutItem = new MenuItem("Cut");
MenuItem copyItem = new MenuItem("Copy");
MenuItem pasteItem = new MenuItem("Paste");

editMenu.getItems().addAll(cutItem, copyItem, pasteItem);

menuBar.getMenus().addAll(fileMenu, editMenu);

// ========================================
// DIALOG BOXES
// ========================================

import javafx.scene.control.Alert;
import javafx.scene.control.Alert.AlertType;
import javafx.scene.control.ButtonType;
import java.util.Optional;

// Information Alert
Alert infoAlert = new Alert(AlertType.INFORMATION);
infoAlert.setTitle("Information");
infoAlert.setHeaderText(null);
infoAlert.setContentText("This is an information message!");
infoAlert.showAndWait();

// Confirmation Dialog
Alert confirmAlert = new Alert(AlertType.CONFIRMATION);
confirmAlert.setTitle("Confirmation");
confirmAlert.setContentText("Are you sure you want to delete?");
Optional<ButtonType> result = confirmAlert.showAndWait();

if (result.isPresent() && result.get() == ButtonType.OK) {
    System.out.println("User confirmed");
}

// Error Alert
Alert errorAlert = new Alert(AlertType.ERROR);
errorAlert.setTitle("Error");
errorAlert.setContentText("An error occurred!");
errorAlert.showAndWait();

// ========================================
// COMPLETE JAVAFX APPLICATION EXAMPLE
// ========================================

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.*;
import javafx.scene.layout.*;
import javafx.stage.Stage;
import javafx.geometry.Insets;

public class CompleteJavaFXApp extends Application {
    
    private TextField nameField;
    private TextArea outputArea;
    
    @Override
    public void start(Stage primaryStage) {
        // Create UI components
        Label nameLabel = new Label("Enter your name:");
        nameField = new TextField();
        nameField.setPromptText("Your name");
        
        Button submitBtn = new Button("Submit");
        Button clearBtn = new Button("Clear");
        
        outputArea = new TextArea();
        outputArea.setEditable(false);
        outputArea.setPrefRowCount(5);
        
        // Event handlers
        submitBtn.setOnAction(e -> handleSubmit());
        clearBtn.setOnAction(e -> handleClear());
        
        // Layout
        VBox vbox = new VBox(10);
        vbox.setPadding(new Insets(20));
        
        HBox buttonBox = new HBox(10);
        buttonBox.getChildren().addAll(submitBtn, clearBtn);
        
        vbox.getChildren().addAll(nameLabel, nameField, buttonBox, outputArea);
        
        // Scene and Stage
        Scene scene = new Scene(vbox, 400, 300);
        primaryStage.setTitle("Complete JavaFX App");
        primaryStage.setScene(scene);
        primaryStage.show();
    }
    
    private void handleSubmit() {
        String name = nameField.getText();
        if (!name.isEmpty()) {
            outputArea.appendText("Hello, " + name + "!\\n");
        } else {
            Alert alert = new Alert(Alert.AlertType.WARNING);
            alert.setContentText("Please enter a name!");
            alert.showAndWait();
        }
    }
    
    private void handleClear() {
        nameField.clear();
        outputArea.clear();
    }
    
    public static void main(String[] args) {
        launch(args);
    }
}
''',
    revisionPoints: [
      'JavaFX is a modern library for building rich GUI applications',
      'JavaFX replaces Swing in modern Java applications',
      'JavaFX supports 2D/3D graphics, animations, and multimedia',
      'FXML allows declarative UI design like HTML for JavaFX',
      'Scene Graph is a hierarchical tree of UI components',
      'Stage is the main window of the application',
      'Scene is the container for all UI elements',
      'Node is the base class for all UI controls',
      'VBox arranges nodes vertically',
      'HBox arranges nodes horizontally',
      'BorderPane has top, bottom, left, right, and center regions',
      'GridPane uses row and column grid layout',
      'StackPane stacks nodes on top of each other',
      'Lambda expressions are used for event handling in JavaFX',
      'CSS can be used to style JavaFX applications',
      'FXMLLoader is used to load FXML files',
      'TableView displays tabular data',
      'ListView displays list of items',
    ],
    quizQuestions: [
      Question(
        question: 'What is the container class for JavaFX UI elements?',
        options: ['Frame', 'Panel', 'Scene', 'Window'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which layout pane arranges nodes vertically?',
        options: ['HBox', 'VBox', 'GridPane', 'StackPane'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does FXML stand for in JavaFX?',
        options: ['Fast XML', 'FX Markup Language', 'Form XML', 'Framework XML'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class must be extended to create a JavaFX application?',
        options: ['JFrame', 'Application', 'Stage', 'Scene'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main window of a JavaFX application called?',
        options: ['Frame', 'Window', 'Stage', 'Scene'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which layout pane has top, bottom, left, right, and center regions?',
        options: ['VBox', 'HBox', 'BorderPane', 'GridPane'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is used to style JavaFX applications?',
        options: ['HTML', 'CSS', 'XML', 'JSON'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which control is used for multi-line text input?',
        options: ['TextField', 'TextArea', 'Label', 'TextPane'],
        correctIndex: 1,
      ),
      Question(
        question: 'What method is overridden to start a JavaFX application?',
        options: ['main()', 'run()', 'start()', 'init()'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which class is used to load FXML files?',
        options: ['FXMLReader', 'FXMLLoader', 'XMLLoader', 'FXLoader'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the base class for all UI controls in JavaFX?',
        options: ['Control', 'Node', 'Component', 'Widget'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which layout pane uses row and column grid layout?',
        options: ['VBox', 'HBox', 'GridPane', 'FlowPane'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_networking',
    title: '16. Networking and Socket Programming in Java',
    explanation: '''## Networking and Socket Programming in Java

### A. Introduction

**Definition:**

* **Networking in Java** allows communication between computers over **TCP/IP networks** using **sockets, URLs, and datagrams**.
* **Socket Programming** provides a **way to send and receive data** between a client and server application.

**Key Points:**

* Java provides networking support via **`java.net` package**.
* Supports both **TCP (reliable, connection-oriented)** and **UDP (unreliable, connectionless)** protocols.
* Sockets are endpoints for communication.

---

### B. Java Networking Basics

**1. Key Classes in `java.net` Package**

| Class            | Purpose                               |
| ---------------- | ------------------------------------- |
| `InetAddress`    | Represents IP address / host name     |
| `URL`            | Represents a Uniform Resource Locator |
| `URLConnection`  | Connect to a URL and read/write data  |
| `Socket`         | TCP client socket                     |
| `ServerSocket`   | TCP server socket                     |
| `DatagramSocket` | UDP socket                            |
| `DatagramPacket` | UDP data packet                       |

---

### C. TCP Socket Programming

**1. Server Side Example**

```java
import java.io.*;
import java.net.*;

public class TCPServer {
    public static void main(String[] args) throws IOException {
        ServerSocket server = new ServerSocket(5000);
        System.out.println("Server started, waiting for client...");

        Socket socket = server.accept(); // wait for client
        System.out.println("Client connected!");

        BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream()));
        PrintWriter out = new PrintWriter(socket.getOutputStream(), true);

        String message = in.readLine();
        System.out.println("Client says: " + message);
        out.println("Hello Client!");

        socket.close();
        server.close();
    }
}
```

**2. Client Side Example**

```java
import java.io.*;
import java.net.*;

public class TCPClient {
    public static void main(String[] args) throws IOException {
        Socket socket = new Socket("localhost", 5000);
        PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
        BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream()));

        out.println("Hello Server!");
        String response = in.readLine();
        System.out.println("Server says: " + response);

        socket.close();
    }
}
```

**Notes:**

* TCP is **connection-oriented**, reliable, and guarantees **order of delivery**.
* Server must run before client connects.

---

### D. UDP Socket Programming

**1. Server Side Example**

```java
import java.net.*;

public class UDPServer {
    public static void main(String[] args) throws Exception {
        DatagramSocket serverSocket = new DatagramSocket(9876);
        byte[] receiveData = new byte[1024];
        byte[] sendData;

        System.out.println("UDP Server waiting for client...");
        DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
        serverSocket.receive(receivePacket);

        String message = new String(receivePacket.getData(), 0, receivePacket.getLength());
        System.out.println("Client says: " + message);

        String response = "Hello UDP Client!";
        sendData = response.getBytes();
        InetAddress clientAddress = receivePacket.getAddress();
        int clientPort = receivePacket.getPort();
        DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length, clientAddress, clientPort);
        serverSocket.send(sendPacket);
        serverSocket.close();
    }
}
```

**2. Client Side Example**

```java
import java.net.*;

public class UDPClient {
    public static void main(String[] args) throws Exception {
        DatagramSocket clientSocket = new DatagramSocket();
        InetAddress IPAddress = InetAddress.getByName("localhost");

        byte[] sendData = "Hello UDP Server!".getBytes();
        DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length, IPAddress, 9876);
        clientSocket.send(sendPacket);

        byte[] receiveData = new byte[1024];
        DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
        clientSocket.receive(receivePacket);

        String response = new String(receivePacket.getData(), 0, receivePacket.getLength());
        System.out.println("Server says: " + response);
        clientSocket.close();
    }
}
```

**Notes:**

* UDP is **connectionless**, fast, but does **not guarantee delivery or order**.
* Useful for **video streaming, gaming, or real-time applications**.

---

### E. URL and HTTP Connection

**Example: Reading content from a URL**

```java
import java.io.*;
import java.net.*;

public class URLDemo {
    public static void main(String[] args) throws Exception {
        URL url = new URL("https://www.example.com");
        BufferedReader in = new BufferedReader(new InputStreamReader(url.openStream()));

        String inputLine;
        while ((inputLine = in.readLine()) != null) {
            System.out.println(inputLine);
        }
        in.close();
    }
}
```

**Notes:**

* `URLConnection` can be used to **read/write data**, **set headers**, and **handle HTTP methods**.

---

### F. Best Practices / Exam Tips

* **TCP** → Reliable, ordered, connection-oriented; use `Socket`/`ServerSocket`.
* **UDP** → Unreliable, fast, connectionless; use `DatagramSocket`/`DatagramPacket`.
* Close **streams and sockets** to prevent resource leaks.
* Use **try-with-resources** for automatic closing of resources.
* Be familiar with **InetAddress**, `URL`, and `HttpURLConnection`.
''',
    codeSnippet: '''
// ========================================
// TCP SERVER - SOCKET PROGRAMMING
// ========================================

import java.io.*;
import java.net.*;

public class TCPServer {
    public static void main(String[] args) {
        try {
            // Create server socket on port 5000
            ServerSocket serverSocket = new ServerSocket(5000);
            System.out.println("Server started on port 5000");
            System.out.println("Waiting for client connection...");
            
            // Accept client connection (blocking call)
            Socket clientSocket = serverSocket.accept();
            System.out.println("Client connected: " + clientSocket.getInetAddress());
            
            // Get input and output streams
            BufferedReader in = new BufferedReader(
                new InputStreamReader(clientSocket.getInputStream()));
            PrintWriter out = new PrintWriter(
                clientSocket.getOutputStream(), true);
            
            // Read message from client
            String clientMessage = in.readLine();
            System.out.println("Client says: " + clientMessage);
            
            // Send response to client
            out.println("Hello Client! Message received.");
            
            // Close connections
            clientSocket.close();
            serverSocket.close();
            System.out.println("Connection closed");
            
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// TCP CLIENT - SOCKET PROGRAMMING
// ========================================

import java.io.*;
import java.net.*;

public class TCPClient {
    public static void main(String[] args) {
        try {
            // Connect to server on localhost:5000
            Socket socket = new Socket("localhost", 5000);
            System.out.println("Connected to server");
            
            // Get input and output streams
            PrintWriter out = new PrintWriter(
                socket.getOutputStream(), true);
            BufferedReader in = new BufferedReader(
                new InputStreamReader(socket.getInputStream()));
            
            // Send message to server
            out.println("Hello Server!");
            
            // Read response from server
            String serverResponse = in.readLine();
            System.out.println("Server says: " + serverResponse);
            
            // Close connection
            socket.close();
            System.out.println("Connection closed");
            
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// MULTI-THREADED TCP SERVER
// ========================================

import java.io.*;
import java.net.*;

public class MultiThreadedServer {
    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(5000);
        System.out.println("Multi-threaded server started");
        
        while (true) {
            Socket clientSocket = serverSocket.accept();
            System.out.println("New client connected");
            
            // Create new thread for each client
            new ClientHandler(clientSocket).start();
        }
    }
}

class ClientHandler extends Thread {
    private Socket socket;
    
    public ClientHandler(Socket socket) {
        this.socket = socket;
    }
    
    @Override
    public void run() {
        try {
            BufferedReader in = new BufferedReader(
                new InputStreamReader(socket.getInputStream()));
            PrintWriter out = new PrintWriter(
                socket.getOutputStream(), true);
            
            String message;
            while ((message = in.readLine()) != null) {
                System.out.println("Received: " + message);
                out.println("Echo: " + message);
                
                if (message.equalsIgnoreCase("exit")) {
                    break;
                }
            }
            
            socket.close();
            System.out.println("Client disconnected");
            
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// UDP SERVER - DATAGRAM SOCKET
// ========================================

import java.net.*;

public class UDPServer {
    public static void main(String[] args) {
        try {
            // Create datagram socket on port 9876
            DatagramSocket serverSocket = new DatagramSocket(9876);
            System.out.println("UDP Server started on port 9876");
            
            byte[] receiveData = new byte[1024];
            byte[] sendData;
            
            while (true) {
                // Receive packet from client
                DatagramPacket receivePacket = new DatagramPacket(
                    receiveData, receiveData.length);
                serverSocket.receive(receivePacket);
                
                String clientMessage = new String(
                    receivePacket.getData(), 0, receivePacket.getLength());
                System.out.println("Client says: " + clientMessage);
                
                // Get client address and port
                InetAddress clientAddress = receivePacket.getAddress();
                int clientPort = receivePacket.getPort();
                
                // Send response back to client
                String response = "Message received: " + clientMessage;
                sendData = response.getBytes();
                DatagramPacket sendPacket = new DatagramPacket(
                    sendData, sendData.length, clientAddress, clientPort);
                serverSocket.send(sendPacket);
            }
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// UDP CLIENT - DATAGRAM SOCKET
// ========================================

import java.net.*;

public class UDPClient {
    public static void main(String[] args) {
        try {
            // Create datagram socket
            DatagramSocket clientSocket = new DatagramSocket();
            InetAddress serverAddress = InetAddress.getByName("localhost");
            
            // Prepare data to send
            String message = "Hello UDP Server!";
            byte[] sendData = message.getBytes();
            
            // Create and send packet
            DatagramPacket sendPacket = new DatagramPacket(
                sendData, sendData.length, serverAddress, 9876);
            clientSocket.send(sendPacket);
            System.out.println("Message sent to server");
            
            // Receive response
            byte[] receiveData = new byte[1024];
            DatagramPacket receivePacket = new DatagramPacket(
                receiveData, receiveData.length);
            clientSocket.receive(receivePacket);
            
            String serverResponse = new String(
                receivePacket.getData(), 0, receivePacket.getLength());
            System.out.println("Server says: " + serverResponse);
            
            // Close socket
            clientSocket.close();
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// INETADDRESS - IP ADDRESS HANDLING
// ========================================

import java.net.*;

public class InetAddressDemo {
    public static void main(String[] args) {
        try {
            // Get local host address
            InetAddress localHost = InetAddress.getLocalHost();
            System.out.println("Local Host: " + localHost.getHostName());
            System.out.println("Local IP: " + localHost.getHostAddress());
            
            // Get address by hostname
            InetAddress google = InetAddress.getByName("www.google.com");
            System.out.println("Google IP: " + google.getHostAddress());
            
            // Get all addresses for a hostname
            InetAddress[] addresses = InetAddress.getAllByName("www.google.com");
            System.out.println("All Google IPs:");
            for (InetAddress addr : addresses) {
                System.out.println("  " + addr.getHostAddress());
            }
            
            // Check if reachable (ping)
            boolean reachable = google.isReachable(5000); // 5 second timeout
            System.out.println("Is Google reachable? " + reachable);
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// URL - READING WEB CONTENT
// ========================================

import java.io.*;
import java.net.*;

public class URLDemo {
    public static void main(String[] args) {
        try {
            // Create URL object
            URL url = new URL("https://www.example.com");
            
            System.out.println("Protocol: " + url.getProtocol());
            System.out.println("Host: " + url.getHost());
            System.out.println("Port: " + url.getPort());
            System.out.println("Path: " + url.getPath());
            
            // Read content from URL
            BufferedReader in = new BufferedReader(
                new InputStreamReader(url.openStream()));
            
            String inputLine;
            System.out.println("\\nContent:");
            while ((inputLine = in.readLine()) != null) {
                System.out.println(inputLine);
            }
            in.close();
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// URLCONNECTION - HTTP REQUEST
// ========================================

import java.io.*;
import java.net.*;

public class URLConnectionDemo {
    public static void main(String[] args) {
        try {
            URL url = new URL("https://api.example.com/data");
            URLConnection connection = url.openConnection();
            
            // Set request properties
            connection.setRequestProperty("User-Agent", "Java Client");
            connection.setConnectTimeout(5000);
            connection.setReadTimeout(5000);
            
            // Get response headers
            System.out.println("Content-Type: " + 
                connection.getContentType());
            System.out.println("Content-Length: " + 
                connection.getContentLength());
            System.out.println("Date: " + 
                new java.util.Date(connection.getDate()));
            
            // Read response
            BufferedReader in = new BufferedReader(
                new InputStreamReader(connection.getInputStream()));
            
            String inputLine;
            StringBuilder response = new StringBuilder();
            while ((inputLine = in.readLine()) != null) {
                response.append(inputLine);
            }
            in.close();
            
            System.out.println("Response: " + response.toString());
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// TRY-WITH-RESOURCES - AUTOMATIC CLOSING
// ========================================

import java.io.*;
import java.net.*;

public class TCPServerWithResources {
    public static void main(String[] args) {
        // Automatic resource management
        try (ServerSocket serverSocket = new ServerSocket(5000);
             Socket clientSocket = serverSocket.accept();
             BufferedReader in = new BufferedReader(
                 new InputStreamReader(clientSocket.getInputStream()));
             PrintWriter out = new PrintWriter(
                 clientSocket.getOutputStream(), true)) {
            
            System.out.println("Client connected");
            
            String message = in.readLine();
            System.out.println("Received: " + message);
            
            out.println("Response: " + message);
            
            // Resources automatically closed
            
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// HTTP GET REQUEST
// ========================================

import java.io.*;
import java.net.*;

public class HTTPGetRequest {
    public static void main(String[] args) {
        try {
            URL url = new URL("https://jsonplaceholder.typicode.com/posts/1");
            HttpURLConnection connection = (HttpURLConnection) url.openConnection();
            
            // Set request method
            connection.setRequestMethod("GET");
            connection.setRequestProperty("Accept", "application/json");
            
            // Get response code
            int responseCode = connection.getResponseCode();
            System.out.println("Response Code: " + responseCode);
            
            if (responseCode == HttpURLConnection.HTTP_OK) {
                BufferedReader in = new BufferedReader(
                    new InputStreamReader(connection.getInputStream()));
                
                String inputLine;
                StringBuilder response = new StringBuilder();
                
                while ((inputLine = in.readLine()) != null) {
                    response.append(inputLine);
                }
                in.close();
                
                System.out.println("Response: " + response.toString());
            }
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// HTTP POST REQUEST
// ========================================

import java.io.*;
import java.net.*;

public class HTTPPostRequest {
    public static void main(String[] args) {
        try {
            URL url = new URL("https://jsonplaceholder.typicode.com/posts");
            HttpURLConnection connection = (HttpURLConnection) url.openConnection();
            
            // Set request method and properties
            connection.setRequestMethod("POST");
            connection.setRequestProperty("Content-Type", "application/json");
            connection.setDoOutput(true);
            
            // JSON data to send
            String jsonData = "{\\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1}";
            
            // Write data to output stream
            try (OutputStream os = connection.getOutputStream()) {
                byte[] input = jsonData.getBytes("utf-8");
                os.write(input, 0, input.length);
            }
            
            // Get response
            int responseCode = connection.getResponseCode();
            System.out.println("Response Code: " + responseCode);
            
            if (responseCode == HttpURLConnection.HTTP_CREATED) {
                BufferedReader in = new BufferedReader(
                    new InputStreamReader(connection.getInputStream()));
                
                String inputLine;
                StringBuilder response = new StringBuilder();
                
                while ((inputLine = in.readLine()) != null) {
                    response.append(inputLine);
                }
                in.close();
                
                System.out.println("Response: " + response.toString());
            }
            
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
''',
    revisionPoints: [
      'Networking in Java allows communication over TCP/IP networks',
      'java.net package provides networking support',
      'TCP is reliable and connection-oriented',
      'UDP is unreliable and connectionless',
      'Socket is used for TCP client communication',
      'ServerSocket is used for TCP server communication',
      'DatagramSocket is used for UDP communication',
      'DatagramPacket represents UDP data packets',
      'InetAddress represents IP addresses and hostnames',
      'URL represents Uniform Resource Locators',
      'URLConnection connects to URLs and handles HTTP',
      'TCP guarantees order of delivery',
      'UDP does not guarantee delivery or order',
      'UDP is faster than TCP',
      'Always close streams and sockets to prevent resource leaks',
      'Try-with-resources automatically closes resources',
      'Multi-threaded servers handle multiple clients simultaneously',
      'HttpURLConnection handles HTTP requests',
    ],
    quizQuestions: [
      Question(
        question: 'Which class is used for TCP server communication?',
        options: ['Socket', 'ServerSocket', 'DatagramSocket', 'NetworkSocket'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which protocol is connection-oriented and reliable?',
        options: ['UDP', 'TCP', 'HTTP', 'FTP'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used for UDP communication?',
        options: ['Socket', 'ServerSocket', 'DatagramSocket', 'UDPSocket'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which class represents IP addresses in Java?',
        options: ['IPAddress', 'InetAddress', 'NetworkAddress', 'Address'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does UDP stand for?',
        options: ['User Datagram Protocol', 'Universal Data Protocol', 'Unified Delivery Protocol', 'User Data Packet'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which method accepts client connections in ServerSocket?',
        options: ['connect()', 'accept()', 'listen()', 'receive()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class represents UDP data packets?',
        options: ['DataPacket', 'UDPPacket', 'DatagramPacket', 'Packet'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which protocol is faster but unreliable?',
        options: ['TCP', 'UDP', 'HTTP', 'FTP'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used for HTTP connections?',
        options: ['URLConnection', 'HTTPConnection', 'WebConnection', 'NetConnection'],
        correctIndex: 0,
      ),
      Question(
        question: 'What package provides networking support in Java?',
        options: ['java.io', 'java.net', 'java.network', 'java.socket'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which protocol guarantees order of delivery?',
        options: ['UDP', 'TCP', 'Both', 'Neither'],
        correctIndex: 1,
      ),
      Question(
        question: 'What should be used to automatically close network resources?',
        options: ['finally block', 'try-with-resources', 'close() method', 'finalize() method'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_serialization',
    title: '17. Java Serialization and Deserialization',
    explanation: '''## Java Serialization and Deserialization

### A. Introduction

**Definition:**

* **Serialization**: Process of converting a **Java object into a byte stream** so it can be **saved to a file or sent over a network**.
* **Deserialization**: Process of **reconstructing the object from the byte stream** back into a Java object.

**Key Points:**

* Used for **persistence, caching, and communication**.
* Classes must implement the **`Serializable` interface**.
* **Transient fields** are **not serialized**.

---

### B. Serializable Interface

* Marker interface (`java.io.Serializable`) with **no methods**.
* Indicates the class can be serialized.

```java
import java.io.Serializable;

class Student implements Serializable {
    private static final long serialVersionUID = 1L; // version control
    String name;
    int age;
    transient String password; // won't be serialized

    Student(String name, int age, String password) {
        this.name = name;
        this.age = age;
        this.password = password;
    }
}
```

---

### C. Serialization Example

```java
import java.io.*;

public class SerializationDemo {
    public static void main(String[] args) {
        Student s = new Student("Alice", 20, "secret123");

        try (FileOutputStream fileOut = new FileOutputStream("student.ser");
             ObjectOutputStream out = new ObjectOutputStream(fileOut)) {
            out.writeObject(s); // Serialize object
            System.out.println("Object serialized successfully!");
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

**Notes:**

* `ObjectOutputStream` writes the object to **file or stream**.
* `transient` fields are skipped (not serialized).
* `serialVersionUID` ensures **class version compatibility** during deserialization.

---

### D. Deserialization Example

```java
import java.io.*;

public class DeserializationDemo {
    public static void main(String[] args) {
        try (FileInputStream fileIn = new FileInputStream("student.ser");
             ObjectInputStream in = new ObjectInputStream(fileIn)) {

            Student s = (Student) in.readObject(); // Deserialize object
            System.out.println("Name: " + s.name);
            System.out.println("Age: " + s.age);
            System.out.println("Password: " + s.password); // null because transient
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}
```

**Notes:**

* Cast object returned from `readObject()` to original type.
* Handles **`ClassNotFoundException`**.
* `transient` fields are **not restored**.

---

### E. Custom Serialization

* You can **control serialization** using `writeObject` and `readObject` methods.

```java
private void writeObject(ObjectOutputStream oos) throws IOException {
    oos.defaultWriteObject(); // default serialization
    oos.writeUTF(password);   // manually serialize transient field
}

private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException {
    ois.defaultReadObject();
    password = ois.readUTF(); // manually restore transient field
}
```

---

### F. Serializable vs Externalizable

| Feature     | Serializable                      | Externalizable                                 |
| ----------- | --------------------------------- | ---------------------------------------------- |
| Interface   | `java.io.Serializable`            | `java.io.Externalizable`                       |
| Methods     | No methods, default serialization | Must override `writeExternal` & `readExternal` |
| Control     | Limited control over fields       | Full control of serialization                  |
| Performance | Slightly slower                   | Faster, manual serialization                   |

---

### G. Best Practices / Exam Tips

* Always define **`serialVersionUID`** to avoid `InvalidClassException`.
* Mark **sensitive fields as transient** (passwords, secrets).
* Prefer **try-with-resources** for streams.
* Understand **difference between Serializable and Externalizable**.
* Serialization is **heavily used in caching, RMI, and messaging systems**.
''',
    codeSnippet: '''
// ========================================
// BASIC SERIALIZATION
// ========================================

import java.io.*;

// Class must implement Serializable
class Student implements Serializable {
    // serialVersionUID for version control
    private static final long serialVersionUID = 1L;
    
    String name;
    int age;
    transient String password; // Won't be serialized
    
    public Student(String name, int age, String password) {
        this.name = name;
        this.age = age;
        this.password = password;
    }
    
    @Override
    public String toString() {
        return "Student{name='" + name + "', age=" + age + 
               ", password='" + password + "'}";
    }
}

public class SerializationDemo {
    public static void main(String[] args) {
        Student student = new Student("Alice", 20, "secret123");
        
        // Serialize object to file
        try (FileOutputStream fileOut = new FileOutputStream("student.ser");
             ObjectOutputStream out = new ObjectOutputStream(fileOut)) {
            
            out.writeObject(student);
            System.out.println("Object serialized successfully!");
            System.out.println("Serialized: " + student);
            
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// BASIC DESERIALIZATION
// ========================================

import java.io.*;

public class DeserializationDemo {
    public static void main(String[] args) {
        try (FileInputStream fileIn = new FileInputStream("student.ser");
             ObjectInputStream in = new ObjectInputStream(fileIn)) {
            
            // Deserialize object from file
            Student student = (Student) in.readObject();
            
            System.out.println("Object deserialized successfully!");
            System.out.println("Name: " + student.name);
            System.out.println("Age: " + student.age);
            System.out.println("Password: " + student.password); // null (transient)
            
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// SERIALIZING MULTIPLE OBJECTS
// ========================================

import java.io.*;
import java.util.*;

public class MultipleObjectsSerialization {
    public static void main(String[] args) {
        List<Student> students = new ArrayList<>();
        students.add(new Student("Alice", 20, "pass1"));
        students.add(new Student("Bob", 22, "pass2"));
        students.add(new Student("Charlie", 21, "pass3"));
        
        // Serialize list of objects
        try (ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream("students.ser"))) {
            
            out.writeObject(students);
            System.out.println("Multiple objects serialized!");
            
        } catch (IOException e) {
            e.printStackTrace();
        }
        
        // Deserialize list of objects
        try (ObjectInputStream in = new ObjectInputStream(
                new FileInputStream("students.ser"))) {
            
            @SuppressWarnings("unchecked")
            List<Student> loadedStudents = (List<Student>) in.readObject();
            
            System.out.println("\\nDeserialized students:");
            for (Student s : loadedStudents) {
                System.out.println("  " + s.name + " - " + s.age);
            }
            
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// CUSTOM SERIALIZATION
// ========================================

import java.io.*;

class Employee implements Serializable {
    private static final long serialVersionUID = 1L;
    
    String name;
    double salary;
    transient String password; // Won't be serialized by default
    
    public Employee(String name, double salary, String password) {
        this.name = name;
        this.salary = salary;
        this.password = password;
    }
    
    // Custom serialization method
    private void writeObject(ObjectOutputStream oos) throws IOException {
        oos.defaultWriteObject(); // Serialize non-transient fields
        
        // Manually serialize transient field (encrypted)
        String encryptedPassword = encrypt(password);
        oos.writeUTF(encryptedPassword);
    }
    
    // Custom deserialization method
    private void readObject(ObjectInputStream ois) 
            throws IOException, ClassNotFoundException {
        ois.defaultReadObject(); // Deserialize non-transient fields
        
        // Manually deserialize transient field (decrypted)
        String encryptedPassword = ois.readUTF();
        password = decrypt(encryptedPassword);
    }
    
    private String encrypt(String text) {
        // Simple encryption (reverse string)
        return new StringBuilder(text).reverse().toString();
    }
    
    private String decrypt(String text) {
        // Simple decryption (reverse back)
        return new StringBuilder(text).reverse().toString();
    }
    
    @Override
    public String toString() {
        return "Employee{name='" + name + "', salary=" + salary + 
               ", password='" + password + "'}";
    }
}

public class CustomSerializationDemo {
    public static void main(String[] args) {
        Employee emp = new Employee("John", 50000, "myPassword123");
        
        // Serialize
        try (ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream("employee.ser"))) {
            out.writeObject(emp);
            System.out.println("Serialized: " + emp);
        } catch (IOException e) {
            e.printStackTrace();
        }
        
        // Deserialize
        try (ObjectInputStream in = new ObjectInputStream(
                new FileInputStream("employee.ser"))) {
            Employee loadedEmp = (Employee) in.readObject();
            System.out.println("Deserialized: " + loadedEmp);
            System.out.println("Password restored: " + loadedEmp.password);
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// EXTERNALIZABLE INTERFACE
// ========================================

import java.io.*;

class Product implements Externalizable {
    private static final long serialVersionUID = 1L;
    
    String name;
    double price;
    int quantity;
    
    // No-arg constructor required for Externalizable
    public Product() {}
    
    public Product(String name, double price, int quantity) {
        this.name = name;
        this.price = price;
        this.quantity = quantity;
    }
    
    // Manual serialization - full control
    @Override
    public void writeExternal(ObjectOutput out) throws IOException {
        out.writeUTF(name);
        out.writeDouble(price);
        out.writeInt(quantity);
        System.out.println("Manual serialization performed");
    }
    
    // Manual deserialization - full control
    @Override
    public void readExternal(ObjectInput in) 
            throws IOException, ClassNotFoundException {
        name = in.readUTF();
        price = in.readDouble();
        quantity = in.readInt();
        System.out.println("Manual deserialization performed");
    }
    
    @Override
    public String toString() {
        return "Product{name='" + name + "', price=" + price + 
               ", quantity=" + quantity + "}";
    }
}

public class ExternalizableDemo {
    public static void main(String[] args) {
        Product product = new Product("Laptop", 999.99, 5);
        
        // Serialize
        try (ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream("product.ser"))) {
            out.writeObject(product);
            System.out.println("Serialized: " + product);
        } catch (IOException e) {
            e.printStackTrace();
        }
        
        // Deserialize
        try (ObjectInputStream in = new ObjectInputStream(
                new FileInputStream("product.ser"))) {
            Product loadedProduct = (Product) in.readObject();
            System.out.println("Deserialized: " + loadedProduct);
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// SERIALVERSIONUID IMPORTANCE
// ========================================

import java.io.*;

class Person implements Serializable {
    // Explicitly define serialVersionUID
    private static final long serialVersionUID = 1L;
    
    String name;
    int age;
    
    // If you add a new field later, keep serialVersionUID same
    // String address; // New field added
    
    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }
}

/*
Without serialVersionUID:
- Java generates one automatically based on class structure
- If class changes, auto-generated UID changes
- Deserialization fails with InvalidClassException

With explicit serialVersionUID:
- You control version compatibility
- Can deserialize old objects even after class changes
- Must be declared: private static final long
*/

// ========================================
// INHERITANCE AND SERIALIZATION
// ========================================

import java.io.*;

// Parent class NOT implementing Serializable
class Animal {
    String species;
    
    public Animal() {
        System.out.println("Animal no-arg constructor called");
    }
    
    public Animal(String species) {
        this.species = species;
    }
}

// Child class implementing Serializable
class Dog extends Animal implements Serializable {
    private static final long serialVersionUID = 1L;
    
    String name;
    int age;
    
    public Dog(String species, String name, int age) {
        super(species);
        this.name = name;
        this.age = age;
    }
    
    @Override
    public String toString() {
        return "Dog{species='" + species + "', name='" + name + 
               "', age=" + age + "}";
    }
}

public class InheritanceSerializationDemo {
    public static void main(String[] args) {
        Dog dog = new Dog("Canine", "Buddy", 3);
        
        // Serialize
        try (ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream("dog.ser"))) {
            out.writeObject(dog);
            System.out.println("Serialized: " + dog);
        } catch (IOException e) {
            e.printStackTrace();
        }
        
        // Deserialize
        try (ObjectInputStream in = new ObjectInputStream(
                new FileInputStream("dog.ser"))) {
            Dog loadedDog = (Dog) in.readObject();
            System.out.println("Deserialized: " + loadedDog);
            // Note: species will be null because Animal is not Serializable
            // Animal's no-arg constructor is called during deserialization
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// STATIC FIELDS AND SERIALIZATION
// ========================================

import java.io.*;

class Counter implements Serializable {
    private static final long serialVersionUID = 1L;
    
    static int staticCount = 0;  // Static fields are NOT serialized
    int instanceCount = 0;        // Instance fields ARE serialized
    
    public Counter(int instanceCount) {
        this.instanceCount = instanceCount;
        staticCount++;
    }
    
    @Override
    public String toString() {
        return "Counter{staticCount=" + staticCount + 
               ", instanceCount=" + instanceCount + "}";
    }
}

public class StaticFieldSerializationDemo {
    public static void main(String[] args) {
        Counter counter = new Counter(10);
        Counter.staticCount = 100;
        
        System.out.println("Before serialization: " + counter);
        
        // Serialize
        try (ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream("counter.ser"))) {
            out.writeObject(counter);
        } catch (IOException e) {
            e.printStackTrace();
        }
        
        // Change static field
        Counter.staticCount = 999;
        
        // Deserialize
        try (ObjectInputStream in = new ObjectInputStream(
                new FileInputStream("counter.ser"))) {
            Counter loadedCounter = (Counter) in.readObject();
            System.out.println("After deserialization: " + loadedCounter);
            // staticCount will be 999 (current value, not serialized value)
            // instanceCount will be 10 (serialized value)
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}

// ========================================
// SERIALIZATION TO BYTE ARRAY
// ========================================

import java.io.*;

public class ByteArraySerializationDemo {
    public static void main(String[] args) {
        Student student = new Student("Alice", 20, "pass123");
        
        // Serialize to byte array
        byte[] data = serializeToByteArray(student);
        System.out.println("Serialized to " + data.length + " bytes");
        
        // Deserialize from byte array
        Student deserializedStudent = deserializeFromByteArray(data);
        System.out.println("Deserialized: " + deserializedStudent.name + 
                         ", " + deserializedStudent.age);
    }
    
    public static byte[] serializeToByteArray(Object obj) {
        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
             ObjectOutputStream oos = new ObjectOutputStream(baos)) {
            
            oos.writeObject(obj);
            return baos.toByteArray();
            
        } catch (IOException e) {
            e.printStackTrace();
            return null;
        }
    }
    
    public static <T> T deserializeFromByteArray(byte[] data) {
        try (ByteArrayInputStream bais = new ByteArrayInputStream(data);
             ObjectInputStream ois = new ObjectInputStream(bais)) {
            
            @SuppressWarnings("unchecked")
            T obj = (T) ois.readObject();
            return obj;
            
        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
            return null;
        }
    }
}

// ========================================
// DEEP COPY USING SERIALIZATION
// ========================================

import java.io.*;

public class DeepCopyDemo {
    public static void main(String[] args) {
        Student original = new Student("Alice", 20, "pass");
        
        // Deep copy using serialization
        Student copy = deepCopy(original);
        
        // Modify copy
        copy.name = "Bob";
        copy.age = 25;
        
        System.out.println("Original: " + original.name + ", " + original.age);
        System.out.println("Copy: " + copy.name + ", " + copy.age);
    }
    
    @SuppressWarnings("unchecked")
    public static <T extends Serializable> T deepCopy(T object) {
        try {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            ObjectOutputStream oos = new ObjectOutputStream(baos);
            oos.writeObject(object);
            
            ByteArrayInputStream bais = new ByteArrayInputStream(
                baos.toByteArray());
            ObjectInputStream ois = new ObjectInputStream(bais);
            
            return (T) ois.readObject();
            
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }
}
''',
    revisionPoints: [
      'Serialization converts Java objects into byte streams',
      'Deserialization reconstructs objects from byte streams',
      'Classes must implement Serializable interface',
      'Serializable is a marker interface with no methods',
      'transient keyword prevents field serialization',
      'Static fields are not serialized',
      'serialVersionUID ensures version compatibility',
      'ObjectOutputStream is used for serialization',
      'ObjectInputStream is used for deserialization',
      'Externalizable provides full control over serialization',
      'Externalizable requires no-arg constructor',
      'Custom serialization uses writeObject and readObject methods',
      'Serialization is used for persistence, caching, and communication',
      'Always define serialVersionUID to avoid InvalidClassException',
      'Mark sensitive fields as transient',
      'Use try-with-resources for automatic stream closing',
      'Deserialization throws ClassNotFoundException',
      'Parent class fields are not serialized if parent is not Serializable',
    ],
    quizQuestions: [
      Question(
        question: 'Which interface must a class implement to be serialized?',
        options: ['Cloneable', 'Serializable', 'Comparable', 'Iterable'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which keyword prevents a field from being serialized?',
        options: ['volatile', 'final', 'transient', 'static'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which class is used to serialize objects?',
        options: ['ObjectWriter', 'ObjectOutputStream', 'FileOutputStream', 'DataOutputStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used to deserialize objects?',
        options: ['ObjectReader', 'ObjectInputStream', 'FileInputStream', 'DataInputStream'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is serialVersionUID used for?',
        options: ['Object identification', 'Version compatibility', 'Security', 'Performance'],
        correctIndex: 1,
      ),
      Question(
        question: 'Are static fields serialized?',
        options: ['Yes', 'No', 'Only if marked public', 'Only if final'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which interface provides full control over serialization?',
        options: ['Serializable', 'Externalizable', 'Cloneable', 'Writable'],
        correctIndex: 1,
      ),
      Question(
        question: 'What exception is thrown during deserialization if class is not found?',
        options: ['IOException', 'ClassNotFoundException', 'FileNotFoundException', 'SerializationException'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method is used for custom serialization?',
        options: ['serialize()', 'writeObject()', 'save()', 'write()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What happens to transient fields during deserialization?',
        options: ['Set to null/default', 'Throw exception', 'Keep old value', 'Set to zero'],
        correctIndex: 0,
      ),
      Question(
        question: 'Does Externalizable require a no-arg constructor?',
        options: ['Yes', 'No', 'Optional', 'Only for subclasses'],
        correctIndex: 0,
      ),
      Question(
        question: 'Is Serializable a marker interface?',
        options: ['Yes', 'No', 'Sometimes', 'Deprecated'],
        correctIndex: 0,
      ),
    ],
  ),
  Topic(
    id: 'java_jvm_performance',
    title: '18. JVM Architecture and Performance Optimization',
    explanation: '''## JVM Architecture and Performance Optimization

### A. Introduction

**Definition:**

* **JVM (Java Virtual Machine)** is an **abstract computing machine** that enables **Java programs to run on any platform**.
* JVM interprets **bytecode** generated by the **Java compiler** and executes it on the host machine.

**Key Points:**

* Java is **platform-independent** because JVM abstracts the underlying hardware.
* JVM manages **memory, garbage collection, and execution of bytecode**.
* Performance optimization is critical for **high-performance applications**.

---

### B. JVM Architecture

**Components of JVM:**

| Component               | Description                                                                                         |
| ----------------------- | --------------------------------------------------------------------------------------------------- |
| **Class Loader**        | Loads `.class` files into JVM memory. Includes **Bootstrap, Extension, Application class loaders**. |
| **Method Area**         | Stores class-level data: methods, metadata, and static variables.                                   |
| **Heap**                | Runtime memory for **objects and arrays**. Managed by Garbage Collector (GC).                       |
| **Stack**               | Stores **method call frames**: local variables, operand stack. Each thread has its own stack.       |
| **PC Register**         | Program Counter for current thread.                                                                 |
| **Native Method Stack** | For executing **native (C/C++) methods**.                                                           |
| **Execution Engine**    | Executes bytecode. Contains **Interpreter, JIT compiler, Garbage Collector**.                       |

**JVM Execution Flow:**

1. **Class Loading** → Bytecode loaded by Class Loader.
2. **Bytecode Verification** → Ensures security and correctness.
3. **Execution** → Interpreter/JIT executes bytecode.
4. **Memory Management** → Heap allocation and garbage collection.

---

### C. JVM Memory Management

**Memory Areas:**

| Area             | Purpose                                         |
| ---------------- | ----------------------------------------------- |
| **Heap**         | Objects and arrays. GC managed.                 |
| **Stack**        | Method execution frames and local variables.    |
| **Method Area**  | Class definitions, static variables, constants. |
| **PC Register**  | Tracks current instruction.                     |
| **Native Stack** | Execution of native methods.                    |

**Garbage Collection (GC):**

* Automatically removes **unused objects** from heap.
* Reduces **memory leaks**.
* JVM uses multiple GC algorithms.

---

### D. Garbage Collection Types

| GC Type                            | Description                                                     |
| ---------------------------------- | --------------------------------------------------------------- |
| **Serial GC**                      | Single-threaded, for small applications.                        |
| **Parallel GC**                    | Multi-threaded, for performance on multi-core CPUs.             |
| **CMS (Concurrent Mark-Sweep) GC** | Reduces pause times for large heaps.                            |
| **G1 GC (Garbage First)**          | Splits heap into regions, prioritizes reclaiming garbage first. |

**Key GC Concepts:**

* **Young Generation**: New objects. Uses **Minor GC**.
* **Old Generation**: Long-lived objects. Uses **Major GC / Full GC**.
* **Permanent/Metaspace**: Stores class metadata.

---

### E. Performance Optimization Tips

1. **Memory Management:**

   * Avoid **creating unnecessary objects**.
   * Use **object pooling** for expensive objects.
   * Keep references **null** if object is no longer needed.

2. **Efficient GC Tuning:**

   * Choose **G1 GC** for large heaps and low pause times.
   * Monitor GC using `-XX:+PrintGCDetails`.
   * Adjust **heap size** with `-Xms` (initial) and `-Xmx` (max).

3. **JVM Options for Performance:**

   * `-Xmx512m` → Max heap size 512 MB
   * `-Xms256m` → Initial heap size 256 MB
   * `-XX:+UseG1GC` → Use G1 Garbage Collector
   * `-XX:+PrintGC` → Print GC logs

4. **JIT Compiler:**

   * JVM uses **Just-In-Time (JIT) compilation** for **hot methods**, converting bytecode to native code at runtime.
   * Improves performance without changing source code.

5. **Thread and Stack Management:**

   * Avoid **stack overflow** by using recursion carefully.
   * Tune thread stack size with `-Xss` option if necessary.

---

### F. JVM Monitoring and Tools

| Tool          | Purpose                                     |
| ------------- | ------------------------------------------- |
| **jconsole**  | Monitor memory, threads, CPU usage.         |
| **jvisualvm** | Analyze heap, thread dumps, GC performance. |
| **jstack**    | Get thread dump for debugging deadlocks.    |
| **jmap**      | Print heap memory summary.                  |

---

### G. Best Practices / Exam Tips

* Always understand **JVM memory model** for exams and interviews.
* Know **difference between Stack and Heap**.
* GC tuning is often asked in interviews; know **Minor vs Major GC**.
* Use **profiling tools** for large applications to identify bottlenecks.
* Optimize **object creation, memory usage, and thread management** for performance.
''',
    codeSnippet: '''
// ========================================
// JVM MEMORY OPTIONS
// ========================================

/*
Common JVM Options:

Memory Configuration:
-Xms<size>        Set initial heap size (e.g., -Xms512m)
-Xmx<size>        Set maximum heap size (e.g., -Xmx2g)
-Xss<size>        Set thread stack size (e.g., -Xss1m)
-XX:MaxMetaspaceSize=<size>  Set max metaspace size

Garbage Collection:
-XX:+UseSerialGC       Use Serial GC
-XX:+UseParallelGC     Use Parallel GC
-XX:+UseConcMarkSweepGC  Use CMS GC
-XX:+UseG1GC           Use G1 GC (recommended for large heaps)

GC Logging:
-XX:+PrintGC           Print basic GC info
-XX:+PrintGCDetails    Print detailed GC info
-XX:+PrintGCTimeStamps Print GC timestamps
-Xloggc:<file>         Log GC to file

Performance:
-XX:+TieredCompilation Enable tiered compilation
-XX:+AggressiveOpts    Enable aggressive optimizations

Example:
java -Xms512m -Xmx2g -XX:+UseG1GC -XX:+PrintGCDetails MyApp
*/

// ========================================
// JVM ARCHITECTURE COMPONENTS
// ========================================

/*
JVM ARCHITECTURE:

1. CLASS LOADER SUBSYSTEM
   - Bootstrap ClassLoader: Loads core Java classes (rt.jar)
   - Extension ClassLoader: Loads extension libraries
   - Application ClassLoader: Loads application classes

2. RUNTIME DATA AREAS
   a) Method Area (Metaspace in Java 8+)
      - Stores class metadata, static variables, constants
      - Shared among all threads
   
   b) Heap
      - Stores objects and arrays
      - Shared among all threads
      - Managed by Garbage Collector
      - Divided into:
        * Young Generation (Eden, S0, S1)
        * Old Generation (Tenured)
   
   c) Stack (per thread)
      - Stores method frames (local variables, operands)
      - LIFO structure
      - Each thread has its own stack
   
   d) PC Register (per thread)
      - Program Counter: tracks current instruction
   
   e) Native Method Stack (per thread)
      - For native (C/C++) method execution

3. EXECUTION ENGINE
   - Interpreter: Executes bytecode line by line
   - JIT Compiler: Compiles hot methods to native code
   - Garbage Collector: Automatic memory management
*/

// ========================================
// PERFORMANCE PROFILING
// ========================================

public class PerformanceProfiling {
    public static void main(String[] args) {
        // Method 1: System.nanoTime()
        long startTime = System.nanoTime();
        
        // Code to measure
        performTask();
        
        long endTime = System.nanoTime();
        long duration = endTime - startTime;
        
        System.out.println("Execution time: " + duration + " ns");
        System.out.println("Execution time: " + (duration / 1_000_000) + " ms");
        
        // Method 2: System.currentTimeMillis()
        long start = System.currentTimeMillis();
        performTask();
        long end = System.currentTimeMillis();
        System.out.println("Time taken: " + (end - start) + " ms");
    }
    
    private static void performTask() {
        // Simulate some work
        int sum = 0;
        for (int i = 0; i < 1_000_000; i++) {
            sum += i;
        }
    }
}

// ========================================
// MEMORY USAGE MONITORING
// ========================================

public class MemoryMonitoring {
    public static void main(String[] args) {
        Runtime runtime = Runtime.getRuntime();
        
        // Get memory information
        long maxMemory = runtime.maxMemory();     // Maximum heap memory
        long totalMemory = runtime.totalMemory(); // Current heap size
        long freeMemory = runtime.freeMemory();   // Free memory
        long usedMemory = totalMemory - freeMemory;
        
        System.out.println("Max Memory: " + (maxMemory / 1024 / 1024) + " MB");
        System.out.println("Total Memory: " + (totalMemory / 1024 / 1024) + " MB");
        System.out.println("Free Memory: " + (freeMemory / 1024 / 1024) + " MB");
        System.out.println("Used Memory: " + (usedMemory / 1024 / 1024) + " MB");
        
        // Request garbage collection (not guaranteed)
        System.gc();
        
        // Memory after GC
        freeMemory = runtime.freeMemory();
        System.out.println("Free Memory after GC: " + (freeMemory / 1024 / 1024) + " MB");
    }
}

// ========================================
// STACK VS HEAP DEMONSTRATION
// ========================================

public class StackVsHeapDemo {
    public static void main(String[] args) {
        // Primitive variables stored in STACK
        int x = 10;              // Stack
        int y = 20;              // Stack
        
        // Object reference in STACK, object in HEAP
        Person person = new Person("Alice", 25); // Reference in Stack, Object in Heap
        
        // Method call creates new frame in STACK
        int result = add(x, y);  // Method frame in Stack
        
        System.out.println(result);
    }
    
    // Each method call creates a new stack frame
    public static int add(int a, int b) {
        int sum = a + b;  // Local variables in Stack
        return sum;       // Frame removed after return
    }
}

class Person {
    String name;  // Instance variables in Heap
    int age;      // Instance variables in Heap
    
    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }
}

// ========================================
// GARBAGE COLLECTION DEMONSTRATION
// ========================================

public class GarbageCollectionDemo {
    public static void main(String[] args) {
        // Create objects
        for (int i = 0; i < 1000; i++) {
            Person p = new Person("Person" + i, 20 + i);
            // p becomes eligible for GC after each iteration
        }
        
        // Object becomes eligible for GC
        Person person1 = new Person("John", 30);
        person1 = null;  // Now eligible for GC
        
        // Request garbage collection (not guaranteed to run immediately)
        System.gc();
        
        // finalize() is called before GC (deprecated in Java 9+)
    }
}

// ========================================
// OBJECT POOLING FOR PERFORMANCE
// ========================================

import java.util.*;

public class ObjectPoolDemo {
    private static final int POOL_SIZE = 10;
    private static List<Connection> connectionPool = new ArrayList<>();
    
    static {
        // Initialize pool
        for (int i = 0; i < POOL_SIZE; i++) {
            connectionPool.add(new Connection());
        }
    }
    
    public static Connection getConnection() {
        if (!connectionPool.isEmpty()) {
            return connectionPool.remove(0);  // Reuse object
        } else {
            return new Connection();  // Create new if pool empty
        }
    }
    
    public static void releaseConnection(Connection conn) {
        connectionPool.add(conn);  // Return to pool
    }
    
    public static void main(String[] args) {
        // Reuse connections instead of creating new ones
        Connection conn1 = getConnection();
        conn1.execute();
        releaseConnection(conn1);
        
        Connection conn2 = getConnection();  // Reuses conn1
        conn2.execute();
        releaseConnection(conn2);
    }
}

class Connection {
    public void execute() {
        System.out.println("Executing query...");
    }
}

// ========================================
// JIT COMPILER OPTIMIZATION
// ========================================

public class JITCompilerDemo {
    public static void main(String[] args) {
        // Hot method: called frequently
        // JIT compiler will optimize this at runtime
        long startTime = System.nanoTime();
        
        for (int i = 0; i < 100_000; i++) {
            calculateSum(1000);  // Hot method
        }
        
        long endTime = System.nanoTime();
        System.out.println("Time: " + (endTime - startTime) / 1_000_000 + " ms");
        
        // First few iterations: Interpreted
        // After threshold: JIT compiles to native code
        // Subsequent calls: Execute native code (faster)
    }
    
    private static int calculateSum(int n) {
        int sum = 0;
        for (int i = 1; i <= n; i++) {
            sum += i;
        }
        return sum;
    }
}

// ========================================
// MEMORY LEAK EXAMPLE (AVOID THIS)
// ========================================

import java.util.*;

public class MemoryLeakDemo {
    private static List<byte[]> leakyList = new ArrayList<>();
    
    public static void main(String[] args) {
        // Memory leak: objects never removed from list
        while (true) {
            byte[] data = new byte[1024 * 1024];  // 1 MB
            leakyList.add(data);  // Never removed!
            
            System.out.println("Allocated: " + leakyList.size() + " MB");
            
            // This will eventually cause OutOfMemoryError
            // Solution: Remove unused objects or use weak references
        }
    }
}

// ========================================
// PREVENTING MEMORY LEAKS
// ========================================

import java.util.*;

public class PreventMemoryLeak {
    private List<String> cache = new ArrayList<>();
    
    public void addToCache(String item) {
        cache.add(item);
    }
    
    public void clearCache() {
        cache.clear();  // Remove references
        cache = null;   // Make eligible for GC
    }
    
    public static void main(String[] args) {
        PreventMemoryLeak obj = new PreventMemoryLeak();
        
        // Use the cache
        for (int i = 0; i < 1000; i++) {
            obj.addToCache("Item " + i);
        }
        
        // Clear when done
        obj.clearCache();
        
        // Request GC
        System.gc();
    }
}

// ========================================
// WEAK REFERENCE FOR CACHING
// ========================================

import java.lang.ref.WeakReference;

public class WeakReferenceDemo {
    public static void main(String[] args) {
        // Strong reference
        Person person = new Person("Alice", 25);
        
        // Weak reference
        WeakReference<Person> weakRef = new WeakReference<>(person);
        
        System.out.println("Before GC: " + weakRef.get());
        
        // Remove strong reference
        person = null;
        
        // Request GC
        System.gc();
        
        // Weak reference may be cleared
        System.out.println("After GC: " + weakRef.get());  // May be null
    }
}

// ========================================
// STACK OVERFLOW EXAMPLE
// ========================================

public class StackOverflowDemo {
    public static void main(String[] args) {
        try {
            recursiveMethod(1);
        } catch (StackOverflowError e) {
            System.out.println("Stack overflow occurred!");
        }
    }
    
    // Infinite recursion causes stack overflow
    private static void recursiveMethod(int count) {
        System.out.println("Call: " + count);
        recursiveMethod(count + 1);  // No base case!
    }
    
    // Solution: Add base case or use iteration
    private static void safeRecursion(int count, int max) {
        if (count > max) return;  // Base case
        System.out.println("Call: " + count);
        safeRecursion(count + 1, max);
    }
}

// ========================================
// GENERATIONS IN HEAP
// ========================================

/*
HEAP STRUCTURE:

Young Generation:
- Eden Space: New objects created here
- Survivor Space 0 (S0): Objects that survive one GC
- Survivor Space 1 (S1): Objects that survive multiple GCs
- Minor GC: Cleans Young Generation

Old Generation (Tenured):
- Long-lived objects promoted from Young Generation
- Major GC / Full GC: Cleans Old Generation (slower)

Metaspace (Java 8+):
- Class metadata, static variables
- Replaces PermGen from Java 7
- Can grow dynamically

OBJECT LIFECYCLE:
1. Created in Eden
2. Survives GC → moves to S0/S1
3. Survives multiple GCs → promoted to Old Generation
4. Not referenced → eligible for GC
*/

// ========================================
// MEASURING OBJECT SIZE
// ========================================

import java.lang.instrument.Instrumentation;

public class ObjectSizeDemo {
    private static Instrumentation instrumentation;
    
    public static void premain(String args, Instrumentation inst) {
        instrumentation = inst;
    }
    
    public static long getObjectSize(Object obj) {
        return instrumentation.getObjectSize(obj);
    }
    
    public static void main(String[] args) {
        // Approximate sizes (varies by JVM):
        // Empty object: ~16 bytes (header)
        // Reference: 4-8 bytes
        // int: 4 bytes
        // long: 8 bytes
        
        Person p = new Person("Alice", 25);
        // Size depends on: object header + fields + padding
    }
}
''',
    revisionPoints: [
      'JVM is an abstract computing machine for platform independence',
      'JVM interprets bytecode and executes it on host machine',
      'Class Loader loads .class files into JVM memory',
      'Method Area stores class-level data and static variables',
      'Heap stores objects and arrays, managed by GC',
      'Stack stores method call frames and local variables',
      'PC Register tracks current instruction for each thread',
      'Native Method Stack executes native C/C++ methods',
      'Execution Engine contains Interpreter, JIT, and GC',
      'JIT compiler converts hot methods to native code',
      'Minor GC cleans Young Generation',
      'Major GC cleans Old Generation',
      'Serial GC is single-threaded for small applications',
      'Parallel GC is multi-threaded for multi-core CPUs',
      'G1 GC is recommended for large heaps and low pause times',
      '-Xms sets initial heap size',
      '-Xmx sets maximum heap size',
      'Object pooling improves performance for expensive objects',
    ],
    quizQuestions: [
      Question(
        question: 'Which JVM component compiles bytecode to native code?',
        options: ['Class Loader', 'JIT Compiler', 'Garbage Collector', 'Interpreter'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which memory area stores objects and arrays?',
        options: ['Stack', 'Heap', 'Method Area', 'PC Register'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which memory area stores method call frames?',
        options: ['Stack', 'Heap', 'Method Area', 'PC Register'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which GC is recommended for large heaps?',
        options: ['Serial GC', 'Parallel GC', 'G1 GC', 'CMS GC'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does -Xmx JVM option set?',
        options: ['Initial heap size', 'Maximum heap size', 'Stack size', 'Metaspace size'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does -Xms JVM option set?',
        options: ['Initial heap size', 'Maximum heap size', 'Stack size', 'Metaspace size'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which GC cleans the Young Generation?',
        options: ['Minor GC', 'Major GC', 'Full GC', 'Permanent GC'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which GC cleans the Old Generation?',
        options: ['Minor GC', 'Major GC', 'Young GC', 'Eden GC'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which component loads .class files into JVM?',
        options: ['Interpreter', 'Class Loader', 'JIT Compiler', 'Execution Engine'],
        correctIndex: 1,
      ),
      Question(
        question: 'Where are static variables stored in JVM?',
        options: ['Stack', 'Heap', 'Method Area', 'PC Register'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does JIT stand for?',
        options: ['Java Internal Tool', 'Just-In-Time', 'Java Integrated Testing', 'Just-In-Thread'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which tool is used to monitor JVM memory and threads?',
        options: ['javac', 'jconsole', 'jar', 'javadoc'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_security',
    title: '19. Java Security and Access Modifiers',
    explanation: '''## Java Security and Access Modifiers

### A. Introduction

**Definition:**

* **Java Security** ensures that **Java programs are safe** from threats such as unauthorized access, code tampering, and malicious inputs.
* **Access Modifiers** control **visibility of classes, methods, and variables**, enabling encapsulation and data protection.

**Key Points:**

* Java provides **built-in security features** like **Access Control, Cryptography, and ClassLoader restrictions**.
* Access modifiers enforce **data encapsulation** at compile-time.
* Correct usage prevents **unauthorized access** and enhances **code maintainability**.

---

### B. Access Modifiers in Java

| Modifier         | Class | Package | Subclass | World |
| ---------------- | ----- | ------- | -------- | ----- |
| **private**      | Yes   | No      | No       | No    |
| **default (no)** | Yes   | Yes     | No       | No    |
| **protected**    | Yes   | Yes     | Yes      | No    |
| **public**       | Yes   | Yes     | Yes      | Yes   |

**Usage Examples:**

```java
package com.example;

public class Demo {
    private int privateVar = 1;
    int defaultVar = 2;
    protected int protectedVar = 3;
    public int publicVar = 4;

    public void show() {
        System.out.println(privateVar + " " + defaultVar + " " + protectedVar + " " + publicVar);
    }
}
```

**Notes:**

* Use **private** for internal fields.
* Use **protected** for inheritance.
* Use **public** for APIs.
* Default is **package-private**.

---

### C. Java Security Features

**1. Security Manager**

* Enforces **runtime access control**.
* Can **restrict file access, network access, thread execution, and system properties**.

```java
System.setSecurityManager(new SecurityManager());
System.getSecurityManager().checkRead("C:/secret.txt"); // Throws SecurityException if not allowed
```

**2. Java Cryptography API (JCA)**

* Provides **encryption, decryption, digital signatures**.
* Example: **AES encryption**

```java
import javax.crypto.Cipher;
import javax.crypto.KeyGenerator;
import javax.crypto.SecretKey;

public class CryptoDemo {
    public static void main(String[] args) throws Exception {
        KeyGenerator keyGen = KeyGenerator.getInstance("AES");
        keyGen.init(128);
        SecretKey key = keyGen.generateKey();

        Cipher cipher = Cipher.getInstance("AES");
        cipher.init(Cipher.ENCRYPT_MODE, key);

        String data = "Hello Java Security!";
        byte[] encrypted = cipher.doFinal(data.getBytes());
        System.out.println("Encrypted: " + new String(encrypted));
    }
}
```

**3. Java Authentication & Authorization**

* **JAAS (Java Authentication and Authorization Service)** manages user login and access control.

**4. Digital Signatures**

* Ensures **data integrity and authentication**.

```java
import java.security.*;

KeyPairGenerator keyGen = KeyPairGenerator.getInstance("RSA");
KeyPair pair = keyGen.generateKeyPair();
Signature sig = Signature.getInstance("SHA256withRSA");

sig.initSign(pair.getPrivate());
sig.update("Hello".getBytes());
byte[] signature = sig.sign();
System.out.println("Signature: " + signature);
```

---

### D. Best Practices / Exam Tips

* Always **use private fields** and **public/protected getters/setters**.
* Enable **Security Manager** in sensitive applications.
* Use **strong encryption algorithms** (AES, RSA) for sensitive data.
* Use **digital signatures** to verify integrity.
* Access modifiers are **heavily asked in Java OOP interviews**.
* Understand **difference between compile-time (access modifiers) and runtime (security manager, JAAS) security**.
''',
    codeSnippet: '''
// ========================================
// ACCESS MODIFIERS - OVERVIEW
// ========================================

package com.example;

public class AccessModifiersDemo {
    // Private: Only accessible within this class
    private int privateVar = 1;
    
    // Default (package-private): Accessible within same package
    int defaultVar = 2;
    
    // Protected: Accessible in same package and subclasses
    protected int protectedVar = 3;
    
    // Public: Accessible everywhere
    public int publicVar = 4;
    
    // Private method
    private void privateMethod() {
        System.out.println("Private method - only in this class");
    }
    
    // Default method
    void defaultMethod() {
        System.out.println("Default method - same package");
    }
    
    // Protected method
    protected void protectedMethod() {
        System.out.println("Protected method - same package + subclasses");
    }
    
    // Public method
    public void publicMethod() {
        System.out.println("Public method - accessible everywhere");
    }
    
    public void accessAll() {
        // All members accessible within same class
        System.out.println(privateVar);
        System.out.println(defaultVar);
        System.out.println(protectedVar);
        System.out.println(publicVar);
        
        privateMethod();
        defaultMethod();
        protectedMethod();
        publicMethod();
    }
}

// ========================================
// SAME PACKAGE ACCESS
// ========================================

package com.example;

public class SamePackageClass {
    public void testAccess() {
        AccessModifiersDemo obj = new AccessModifiersDemo();
        
        // System.out.println(obj.privateVar);    // ERROR - private
        System.out.println(obj.defaultVar);       // OK - same package
        System.out.println(obj.protectedVar);     // OK - same package
        System.out.println(obj.publicVar);        // OK - public
        
        // obj.privateMethod();   // ERROR - private
        obj.defaultMethod();      // OK - same package
        obj.protectedMethod();    // OK - same package
        obj.publicMethod();       // OK - public
    }
}

// ========================================
// DIFFERENT PACKAGE - SUBCLASS
// ========================================

package com.other;

import com.example.AccessModifiersDemo;

public class SubclassInDifferentPackage extends AccessModifiersDemo {
    public void testAccess() {
        // System.out.println(privateVar);    // ERROR - private
        // System.out.println(defaultVar);    // ERROR - different package
        System.out.println(protectedVar);     // OK - subclass
        System.out.println(publicVar);        // OK - public
        
        // privateMethod();   // ERROR - private
        // defaultMethod();   // ERROR - different package
        protectedMethod();    // OK - subclass
        publicMethod();       // OK - public
    }
}

// ========================================
// DIFFERENT PACKAGE - NO INHERITANCE
// ========================================

package com.other;

import com.example.AccessModifiersDemo;

public class DifferentPackageClass {
    public void testAccess() {
        AccessModifiersDemo obj = new AccessModifiersDemo();
        
        // System.out.println(obj.privateVar);    // ERROR - private
        // System.out.println(obj.defaultVar);    // ERROR - different package
        // System.out.println(obj.protectedVar);  // ERROR - not subclass
        System.out.println(obj.publicVar);        // OK - public
        
        // obj.privateMethod();   // ERROR - private
        // obj.defaultMethod();   // ERROR - different package
        // obj.protectedMethod(); // ERROR - not subclass
        obj.publicMethod();       // OK - public
    }
}

// ========================================
// ENCAPSULATION WITH PRIVATE FIELDS
// ========================================

public class Person {
    // Private fields - cannot be accessed directly
    private String name;
    private int age;
    private String ssn;
    
    // Public constructor
    public Person(String name, int age, String ssn) {
        this.name = name;
        setAge(age);  // Use setter for validation
        this.ssn = ssn;
    }
    
    // Public getter for name
    public String getName() {
        return name;
    }
    
    // Public setter for name
    public void setName(String name) {
        if (name != null && !name.isEmpty()) {
            this.name = name;
        }
    }
    
    // Public getter for age
    public int getAge() {
        return age;
    }
    
    // Public setter with validation
    public void setAge(int age) {
        if (age > 0 && age < 150) {
            this.age = age;
        } else {
            throw new IllegalArgumentException("Invalid age");
        }
    }
    
    // No setter for SSN - read-only after creation
    public String getSSN() {
        // Return masked SSN for security
        return "***-**-" + ssn.substring(ssn.length() - 4);
    }
}

// ========================================
// SECURITY MANAGER
// ========================================

import java.io.*;

public class SecurityManagerDemo {
    public static void main(String[] args) {
        // Enable Security Manager
        System.setSecurityManager(new SecurityManager());
        
        try {
            // Attempt to read a file
            SecurityManager sm = System.getSecurityManager();
            if (sm != null) {
                sm.checkRead("C:/sensitive.txt");
            }
            
            // If allowed, proceed
            FileReader fr = new FileReader("C:/sensitive.txt");
            System.out.println("File read successful");
            
        } catch (SecurityException e) {
            System.out.println("Security Exception: Access denied");
        } catch (IOException e) {
            System.out.println("IO Exception: " + e.getMessage());
        }
        
        // Check other permissions
        try {
            SecurityManager sm = System.getSecurityManager();
            if (sm != null) {
                sm.checkWrite("C:/output.txt");
                sm.checkConnect("www.example.com", 80);
                sm.checkCreateClassLoader();
            }
        } catch (SecurityException e) {
            System.out.println("Permission denied: " + e.getMessage());
        }
    }
}

// ========================================
// AES ENCRYPTION
// ========================================

import javax.crypto.*;
import javax.crypto.spec.*;
import java.util.Base64;

public class AESEncryptionDemo {
    public static void main(String[] args) throws Exception {
        // Generate AES key
        KeyGenerator keyGen = KeyGenerator.getInstance("AES");
        keyGen.init(128);  // 128-bit key
        SecretKey secretKey = keyGen.generateKey();
        
        String plainText = "Hello Java Security!";
        
        // Encrypt
        Cipher cipher = Cipher.getInstance("AES");
        cipher.init(Cipher.ENCRYPT_MODE, secretKey);
        byte[] encryptedBytes = cipher.doFinal(plainText.getBytes());
        String encrypted = Base64.getEncoder().encodeToString(encryptedBytes);
        
        System.out.println("Original: " + plainText);
        System.out.println("Encrypted: " + encrypted);
        
        // Decrypt
        cipher.init(Cipher.DECRYPT_MODE, secretKey);
        byte[] decryptedBytes = cipher.doFinal(
            Base64.getDecoder().decode(encrypted));
        String decrypted = new String(decryptedBytes);
        
        System.out.println("Decrypted: " + decrypted);
    }
}

// ========================================
// RSA ENCRYPTION
// ========================================

import java.security.*;
import javax.crypto.*;
import java.util.Base64;

public class RSAEncryptionDemo {
    public static void main(String[] args) throws Exception {
        // Generate RSA key pair
        KeyPairGenerator keyGen = KeyPairGenerator.getInstance("RSA");
        keyGen.initialize(2048);
        KeyPair keyPair = keyGen.generateKeyPair();
        
        PublicKey publicKey = keyPair.getPublic();
        PrivateKey privateKey = keyPair.getPrivate();
        
        String plainText = "Secure Message";
        
        // Encrypt with public key
        Cipher cipher = Cipher.getInstance("RSA");
        cipher.init(Cipher.ENCRYPT_MODE, publicKey);
        byte[] encryptedBytes = cipher.doFinal(plainText.getBytes());
        String encrypted = Base64.getEncoder().encodeToString(encryptedBytes);
        
        System.out.println("Original: " + plainText);
        System.out.println("Encrypted: " + encrypted);
        
        // Decrypt with private key
        cipher.init(Cipher.DECRYPT_MODE, privateKey);
        byte[] decryptedBytes = cipher.doFinal(
            Base64.getDecoder().decode(encrypted));
        String decrypted = new String(decryptedBytes);
        
        System.out.println("Decrypted: " + decrypted);
    }
}

// ========================================
// DIGITAL SIGNATURE
// ========================================

import java.security.*;
import java.util.Base64;

public class DigitalSignatureDemo {
    public static void main(String[] args) throws Exception {
        // Generate key pair
        KeyPairGenerator keyGen = KeyPairGenerator.getInstance("RSA");
        keyGen.initialize(2048);
        KeyPair keyPair = keyGen.generateKeyPair();
        
        String message = "This is a signed message";
        
        // Create signature
        Signature signature = Signature.getInstance("SHA256withRSA");
        signature.initSign(keyPair.getPrivate());
        signature.update(message.getBytes());
        byte[] digitalSignature = signature.sign();
        
        System.out.println("Message: " + message);
        System.out.println("Signature: " + 
            Base64.getEncoder().encodeToString(digitalSignature));
        
        // Verify signature
        Signature verifySignature = Signature.getInstance("SHA256withRSA");
        verifySignature.initVerify(keyPair.getPublic());
        verifySignature.update(message.getBytes());
        boolean isValid = verifySignature.verify(digitalSignature);
        
        System.out.println("Signature valid: " + isValid);
        
        // Verify with tampered message
        verifySignature.initVerify(keyPair.getPublic());
        verifySignature.update("Tampered message".getBytes());
        boolean isTamperedValid = verifySignature.verify(digitalSignature);
        
        System.out.println("Tampered signature valid: " + isTamperedValid);
    }
}

// ========================================
// MESSAGE DIGEST (HASHING)
// ========================================

import java.security.MessageDigest;
import java.util.Base64;

public class MessageDigestDemo {
    public static void main(String[] args) throws Exception {
        String password = "mySecurePassword123";
        
        // SHA-256 hash
        MessageDigest md = MessageDigest.getInstance("SHA-256");
        byte[] hash = md.digest(password.getBytes());
        String hashString = Base64.getEncoder().encodeToString(hash);
        
        System.out.println("Password: " + password);
        System.out.println("SHA-256 Hash: " + hashString);
        
        // MD5 hash (not recommended for security)
        MessageDigest md5 = MessageDigest.getInstance("MD5");
        byte[] md5Hash = md5.digest(password.getBytes());
        String md5String = Base64.getEncoder().encodeToString(md5Hash);
        
        System.out.println("MD5 Hash: " + md5String);
        
        // Verify password
        byte[] inputHash = md.digest("mySecurePassword123".getBytes());
        boolean matches = MessageDigest.isEqual(hash, inputHash);
        System.out.println("Password matches: " + matches);
    }
}

// ========================================
// SECURE RANDOM NUMBER GENERATION
// ========================================

import java.security.SecureRandom;

public class SecureRandomDemo {
    public static void main(String[] args) {
        // Use SecureRandom instead of Random for security
        SecureRandom secureRandom = new SecureRandom();
        
        // Generate random numbers
        int randomInt = secureRandom.nextInt();
        long randomLong = secureRandom.nextLong();
        double randomDouble = secureRandom.nextDouble();
        
        System.out.println("Random Int: " + randomInt);
        System.out.println("Random Long: " + randomLong);
        System.out.println("Random Double: " + randomDouble);
        
        // Generate random bytes (for keys, tokens)
        byte[] randomBytes = new byte[16];
        secureRandom.nextBytes(randomBytes);
        System.out.println("Random Bytes: " + 
            java.util.Base64.getEncoder().encodeToString(randomBytes));
        
        // Generate secure token
        String token = generateSecureToken(32);
        System.out.println("Secure Token: " + token);
    }
    
    public static String generateSecureToken(int length) {
        SecureRandom random = new SecureRandom();
        byte[] bytes = new byte[length];
        random.nextBytes(bytes);
        return java.util.Base64.getUrlEncoder()
            .withoutPadding()
            .encodeToString(bytes);
    }
}

// ========================================
// PASSWORD ENCRYPTION
// ========================================

import javax.crypto.*;
import javax.crypto.spec.*;
import java.security.*;
import java.util.Base64;

public class PasswordEncryption {
    public static void main(String[] args) throws Exception {
        String password = "myPassword123";
        
        // Generate salt
        SecureRandom random = new SecureRandom();
        byte[] salt = new byte[16];
        random.nextBytes(salt);
        
        // Generate key from password
        SecretKeyFactory factory = SecretKeyFactory.getInstance("PBKDF2WithHmacSHA256");
        KeySpec spec = new PBEKeySpec(password.toCharArray(), salt, 65536, 128);
        SecretKey tmp = factory.generateSecret(spec);
        SecretKey secret = new SecretKeySpec(tmp.getEncoded(), "AES");
        
        System.out.println("Password-based key generated");
        System.out.println("Salt: " + Base64.getEncoder().encodeToString(salt));
        System.out.println("Key: " + Base64.getEncoder().encodeToString(secret.getEncoded()));
    }
}

// ========================================
// IMMUTABLE CLASS FOR SECURITY
// ========================================

public final class ImmutablePerson {
    // Final class - cannot be extended
    // Private final fields - cannot be modified
    private final String name;
    private final int age;
    
    public ImmutablePerson(String name, int age) {
        this.name = name;
        this.age = age;
    }
    
    // Only getters, no setters
    public String getName() {
        return name;
    }
    
    public int getAge() {
        return age;
    }
    
    // No methods that modify state
    // Thread-safe by design
}

// ========================================
// ACCESS CONTROL WITH INTERFACES
// ========================================

public interface ReadOnly {
    String getName();
    int getAge();
}

public interface ReadWrite extends ReadOnly {
    void setName(String name);
    void setAge(int age);
}

public class User implements ReadWrite {
    private String name;
    private int age;
    
    @Override
    public String getName() { return name; }
    
    @Override
    public int getAge() { return age; }
    
    @Override
    public void setName(String name) { this.name = name; }
    
    @Override
    public void setAge(int age) { this.age = age; }
}

// Usage: Restrict access based on interface
public class AccessControlDemo {
    public static void main(String[] args) {
        User user = new User();
        user.setName("Alice");
        user.setAge(25);
        
        // Pass as ReadOnly - cannot modify
        displayUser(user);
        
        // Pass as ReadWrite - can modify
        modifyUser(user);
    }
    
    private static void displayUser(ReadOnly user) {
        System.out.println(user.getName() + " - " + user.getAge());
        // user.setName("Bob"); // ERROR - ReadOnly interface
    }
    
    private static void modifyUser(ReadWrite user) {
        user.setName("Bob");
        user.setAge(30);
    }
}
''',
    revisionPoints: [
      'Private members are accessible only within the same class',
      'Default (package-private) members are accessible within the same package',
      'Protected members are accessible in same package and subclasses',
      'Public members are accessible everywhere',
      'Access modifiers enforce data encapsulation at compile-time',
      'Security Manager enforces runtime access control',
      'Java Cryptography API provides encryption and decryption',
      'AES is a symmetric encryption algorithm',
      'RSA is an asymmetric encryption algorithm',
      'Digital signatures ensure data integrity and authentication',
      'JAAS manages user authentication and authorization',
      'Always use private fields with public getters/setters',
      'Use strong encryption algorithms like AES and RSA',
      'MessageDigest is used for hashing passwords',
      'SecureRandom should be used for cryptographic operations',
      'Immutable classes are thread-safe and secure',
      'Compile-time security uses access modifiers',
      'Runtime security uses Security Manager and JAAS',
    ],
    quizQuestions: [
      Question(
        question: 'Which access modifier makes a member accessible only within its class?',
        options: ['public', 'protected', 'private', 'default'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which access modifier is accessible everywhere?',
        options: ['public', 'protected', 'private', 'default'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which access modifier is accessible in same package and subclasses?',
        options: ['public', 'protected', 'private', 'default'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the default access modifier (no modifier specified)?',
        options: ['public', 'protected', 'private', 'package-private'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which component enforces runtime access control in Java?',
        options: ['Compiler', 'Security Manager', 'JVM', 'ClassLoader'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which encryption algorithm is symmetric?',
        options: ['RSA', 'AES', 'DSA', 'ECC'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which encryption algorithm is asymmetric?',
        options: ['AES', 'DES', 'RSA', '3DES'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does JAAS stand for?',
        options: ['Java Access and Security', 'Java Authentication and Authorization Service', 'Java Application Security', 'Java Advanced Security'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which class is used for generating message digests (hashing)?',
        options: ['Cipher', 'MessageDigest', 'Signature', 'KeyGenerator'],
        correctIndex: 1,
      ),
      Question(
        question: 'What should be used instead of Random for cryptographic operations?',
        options: ['Math.random()', 'Random', 'SecureRandom', 'ThreadLocalRandom'],
        correctIndex: 2,
      ),
      Question(
        question: 'What ensures data integrity and authentication?',
        options: ['Encryption', 'Digital Signature', 'Hashing', 'Encoding'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which modifier should be used for API methods?',
        options: ['private', 'protected', 'public', 'default'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_unit_testing',
    title: '20. Unit Testing with JUnit & Mockito',
    explanation: '''## Unit Testing with JUnit & Mockito

### A. Introduction

**Definition:**

* **Unit Testing**: Testing individual units or methods of a class to ensure they work as expected.
* **JUnit**: A **popular Java testing framework** for writing and running unit tests.
* **Mockito**: A **mocking framework** that simulates behavior of dependencies to isolate the class under test.

**Key Points:**

* Unit testing ensures **code correctness, maintainability, and early bug detection**.
* JUnit provides **annotations, assertions, and test runners**.
* Mockito allows **mocking external dependencies** like databases, APIs, or services.

---

### B. JUnit Basics

**1. Setting Up JUnit**

* Add dependency (Maven example):

```xml
<dependency>
    <groupId>org.junit.jupiter</groupId>
    <artifactId>junit-jupiter-api</artifactId>
    <version>5.10.0</version>
    <scope>test</scope>
</dependency>
```

**2. Writing a Simple Test**

```java
import static org.junit.jupiter.api.Assertions.*;
import org.junit.jupiter.api.Test;

class CalculatorTest {

    @Test
    void testAdd() {
        Calculator calc = new Calculator();
        int result = calc.add(5, 3);
        assertEquals(8, result, "5 + 3 should equal 8");
    }

    @Test
    void testDivideByZero() {
        Calculator calc = new Calculator();
        assertThrows(ArithmeticException.class, () -> calc.divide(10, 0));
    }
}
```

**Key Annotations:**

| Annotation    | Purpose                         |
| ------------- | ------------------------------- |
| `@Test`       | Marks a method as a test method |
| `@BeforeEach` | Runs before each test           |
| `@AfterEach`  | Runs after each test            |
| `@BeforeAll`  | Runs once before all tests      |
| `@AfterAll`   | Runs once after all tests       |
| `@Disabled`   | Skips the test                  |

---

### C. Mockito Basics

**1. Setting Up Mockito**

* Add dependency (Maven example):

```xml
<dependency>
    <groupId>org.mockito</groupId>
    <artifactId>mockito-core</artifactId>
    <version>5.6.0</version>
    <scope>test</scope>
</dependency>
```

**2. Creating a Mock**

```java
import static org.mockito.Mockito.*;
import org.junit.jupiter.api.Test;

class UserServiceTest {

    @Test
    void testGetUser() {
        UserRepository repo = mock(UserRepository.class);
        when(repo.getUserById(1)).thenReturn(new User(1, "Alice"));

        UserService service = new UserService(repo);
        User user = service.getUser(1);

        assertEquals("Alice", user.getName());
        verify(repo).getUserById(1); // verify method call
    }
}
```

**Key Mockito Methods:**

| Method                      | Purpose                     |
| --------------------------- | --------------------------- |
| `mock(Class)`               | Create a mock object        |
| `when(...).thenReturn(...)` | Define behavior of mock     |
| `verify(...)`               | Verify if method was called |
| `doThrow(...).when(...)`    | Simulate exceptions         |

---

### D. Combining JUnit & Mockito

**Example: Service with Dependency**

```java
class EmailService {
    void sendEmail(String msg) { /* actual sending */ }
}

class UserManager {
    private EmailService emailService;

    UserManager(EmailService emailService) {
        this.emailService = emailService;
    }

    void notifyUser(String msg) {
        emailService.sendEmail(msg);
    }
}

// Test
class UserManagerTest {

    @Test
    void testNotifyUser() {
        EmailService mockEmail = mock(EmailService.class);
        UserManager userManager = new UserManager(mockEmail);

        userManager.notifyUser("Welcome!");
        verify(mockEmail).sendEmail("Welcome!");
    }
}
```

* **Isolation**: The test does not send real emails; it uses a mock.

---

### E. Best Practices / Exam Tips

* **Write small, independent tests** for each method.
* Use **assertions** to check expected outcomes.
* **Mock external dependencies** to isolate the unit under test.
* Use **`@BeforeEach`** to initialize reusable objects.
* Use **`verify`** in Mockito to ensure expected interactions.
* JUnit + Mockito is **widely asked in interviews for backend and Java roles**.
''',
    codeSnippet: '''
// ========================================
// BASIC JUNIT TEST
// ========================================

import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class Calculator {
    public int add(int a, int b) {
        return a + b;
    }
    
    public int subtract(int a, int b) {
        return a - b;
    }
    
    public int multiply(int a, int b) {
        return a * b;
    }
    
    public int divide(int a, int b) {
        if (b == 0) {
            throw new ArithmeticException("Division by zero");
        }
        return a / b;
    }
}

class CalculatorTest {
    
    @Test
    void testAdd() {
        Calculator calc = new Calculator();
        int result = calc.add(5, 3);
        assertEquals(8, result, "5 + 3 should equal 8");
    }
    
    @Test
    void testSubtract() {
        Calculator calc = new Calculator();
        assertEquals(2, calc.subtract(5, 3));
    }
    
    @Test
    void testMultiply() {
        Calculator calc = new Calculator();
        assertEquals(15, calc.multiply(5, 3));
    }
    
    @Test
    void testDivide() {
        Calculator calc = new Calculator();
        assertEquals(2, calc.divide(6, 3));
    }
    
    @Test
    void testDivideByZero() {
        Calculator calc = new Calculator();
        assertThrows(ArithmeticException.class, () -> {
            calc.divide(10, 0);
        });
    }
}

// ========================================
// JUNIT ANNOTATIONS
// ========================================

import org.junit.jupiter.api.*;

class LifecycleTest {
    
    @BeforeAll
    static void setupAll() {
        System.out.println("@BeforeAll - runs once before all tests");
    }
    
    @AfterAll
    static void tearDownAll() {
        System.out.println("@AfterAll - runs once after all tests");
    }
    
    @BeforeEach
    void setup() {
        System.out.println("@BeforeEach - runs before each test");
    }
    
    @AfterEach
    void tearDown() {
        System.out.println("@AfterEach - runs after each test");
    }
    
    @Test
    void test1() {
        System.out.println("Test 1 executing");
        assertTrue(true);
    }
    
    @Test
    void test2() {
        System.out.println("Test 2 executing");
        assertTrue(true);
    }
    
    @Test
    @Disabled("Not ready yet")
    void skippedTest() {
        System.out.println("This test is skipped");
    }
}

// ========================================
// JUNIT ASSERTIONS
// ========================================

import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class AssertionsTest {
    
    @Test
    void testAssertEquals() {
        assertEquals(10, 5 + 5);
        assertEquals("Hello", "Hello");
    }
    
    @Test
    void testAssertNotEquals() {
        assertNotEquals(10, 5 + 4);
    }
    
    @Test
    void testAssertTrue() {
        assertTrue(5 > 3);
    }
    
    @Test
    void testAssertFalse() {
        assertFalse(5 < 3);
    }
    
    @Test
    void testAssertNull() {
        String str = null;
        assertNull(str);
    }
    
    @Test
    void testAssertNotNull() {
        String str = "Hello";
        assertNotNull(str);
    }
    
    @Test
    void testAssertSame() {
        String str1 = "Hello";
        String str2 = str1;
        assertSame(str1, str2);
    }
    
    @Test
    void testAssertArrayEquals() {
        int[] expected = {1, 2, 3};
        int[] actual = {1, 2, 3};
        assertArrayEquals(expected, actual);
    }
    
    @Test
    void testAssertThrows() {
        assertThrows(ArithmeticException.class, () -> {
            int result = 10 / 0;
        });
    }
    
    @Test
    void testAssertAll() {
        assertAll("person",
            () -> assertEquals("John", "John"),
            () -> assertEquals(30, 30),
            () -> assertTrue(true)
        );
    }
}

// ========================================
// MOCKITO - CREATING MOCKS
// ========================================

import static org.mockito.Mockito.*;
import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

interface UserRepository {
    User findById(int id);
    void save(User user);
    void delete(int id);
}

class User {
    private int id;
    private String name;
    
    public User(int id, String name) {
        this.id = id;
        this.name = name;
    }
    
    public int getId() { return id; }
    public String getName() { return name; }
}

class UserService {
    private UserRepository repository;
    
    public UserService(UserRepository repository) {
        this.repository = repository;
    }
    
    public User getUser(int id) {
        return repository.findById(id);
    }
    
    public void createUser(User user) {
        repository.save(user);
    }
}

class UserServiceTest {
    
    @Test
    void testGetUser() {
        // Create mock
        UserRepository mockRepo = mock(UserRepository.class);
        
        // Define behavior
        when(mockRepo.findById(1)).thenReturn(new User(1, "Alice"));
        
        // Use in service
        UserService service = new UserService(mockRepo);
        User user = service.getUser(1);
        
        // Assert
        assertEquals("Alice", user.getName());
        assertEquals(1, user.getId());
        
        // Verify interaction
        verify(mockRepo).findById(1);
    }
    
    @Test
    void testCreateUser() {
        UserRepository mockRepo = mock(UserRepository.class);
        UserService service = new UserService(mockRepo);
        
        User newUser = new User(2, "Bob");
        service.createUser(newUser);
        
        // Verify save was called
        verify(mockRepo).save(newUser);
    }
}

// ========================================
// MOCKITO - STUBBING
// ========================================

import java.util.List;

class StubbingTest {
    
    @Test
    void testStubbingReturnValues() {
        List<String> mockList = mock(List.class);
        
        // Stub methods
        when(mockList.get(0)).thenReturn("first");
        when(mockList.get(1)).thenReturn("second");
        when(mockList.size()).thenReturn(2);
        
        // Test
        assertEquals("first", mockList.get(0));
        assertEquals("second", mockList.get(1));
        assertEquals(2, mockList.size());
    }
    
    @Test
    void testStubbingExceptions() {
        List<String> mockList = mock(List.class);
        
        // Stub to throw exception
        when(mockList.get(10)).thenThrow(new IndexOutOfBoundsException());
        
        // Test
        assertThrows(IndexOutOfBoundsException.class, () -> {
            mockList.get(10);
        });
    }
    
    @Test
    void testStubbingVoidMethods() {
        UserRepository mockRepo = mock(UserRepository.class);
        
        // Stub void method to throw exception
        doThrow(new RuntimeException("Save failed"))
            .when(mockRepo).save(any(User.class));
        
        // Test
        assertThrows(RuntimeException.class, () -> {
            mockRepo.save(new User(1, "Test"));
        });
    }
}

// ========================================
// MOCKITO - VERIFICATION
// ========================================

class VerificationTest {
    
    @Test
    void testVerifyMethodCalls() {
        List<String> mockList = mock(List.class);
        
        mockList.add("one");
        mockList.add("two");
        mockList.clear();
        
        // Verify method calls
        verify(mockList).add("one");
        verify(mockList).add("two");
        verify(mockList).clear();
    }
    
    @Test
    void testVerifyNumberOfInvocations() {
        List<String> mockList = mock(List.class);
        
        mockList.add("once");
        mockList.add("twice");
        mockList.add("twice");
        
        // Verify exact number of calls
        verify(mockList, times(1)).add("once");
        verify(mockList, times(2)).add("twice");
        verify(mockList, never()).add("never");
        verify(mockList, atLeastOnce()).add("once");
        verify(mockList, atLeast(2)).add("twice");
        verify(mockList, atMost(3)).add("twice");
    }
    
    @Test
    void testVerifyNoInteractions() {
        List<String> mockList = mock(List.class);
        
        // Verify no methods were called
        verifyNoInteractions(mockList);
    }
}

// ========================================
// MOCKITO - ARGUMENT MATCHERS
// ========================================

class ArgumentMatchersTest {
    
    @Test
    void testAnyMatcher() {
        UserRepository mockRepo = mock(UserRepository.class);
        
        when(mockRepo.findById(anyInt())).thenReturn(new User(1, "Any User"));
        
        User user = mockRepo.findById(123);
        assertEquals("Any User", user.getName());
        
        verify(mockRepo).findById(anyInt());
    }
    
    @Test
    void testSpecificMatchers() {
        List<String> mockList = mock(List.class);
        
        when(mockList.contains(anyString())).thenReturn(true);
        
        assertTrue(mockList.contains("test"));
        verify(mockList).contains(anyString());
    }
}

// ========================================
// MOCKITO WITH @MOCK ANNOTATION
// ========================================

import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.junit.jupiter.api.extension.ExtendWith;

@ExtendWith(MockitoExtension.class)
class AnnotationTest {
    
    @Mock
    private UserRepository mockRepository;
    
    @Test
    void testWithAnnotation() {
        when(mockRepository.findById(1)).thenReturn(new User(1, "Alice"));
        
        UserService service = new UserService(mockRepository);
        User user = service.getUser(1);
        
        assertEquals("Alice", user.getName());
    }
}

// ========================================
// COMPLETE EXAMPLE - EMAIL SERVICE
// ========================================

interface EmailService {
    void sendEmail(String to, String subject, String body);
    boolean isEmailValid(String email);
}

class UserNotificationService {
    private UserRepository userRepository;
    private EmailService emailService;
    
    public UserNotificationService(UserRepository userRepository, 
                                   EmailService emailService) {
        this.userRepository = userRepository;
        this.emailService = emailService;
    }
    
    public void notifyUser(int userId, String message) {
        User user = userRepository.findById(userId);
        if (user != null) {
            emailService.sendEmail(
                user.getName() + "@example.com",
                "Notification",
                message
            );
        }
    }
    
    public boolean canNotifyUser(int userId) {
        User user = userRepository.findById(userId);
        if (user == null) return false;
        
        String email = user.getName() + "@example.com";
        return emailService.isEmailValid(email);
    }
}

class UserNotificationServiceTest {
    
    @Test
    void testNotifyUser() {
        // Create mocks
        UserRepository mockRepo = mock(UserRepository.class);
        EmailService mockEmail = mock(EmailService.class);
        
        // Stub behavior
        when(mockRepo.findById(1)).thenReturn(new User(1, "Alice"));
        
        // Test
        UserNotificationService service = 
            new UserNotificationService(mockRepo, mockEmail);
        service.notifyUser(1, "Welcome!");
        
        // Verify interactions
        verify(mockRepo).findById(1);
        verify(mockEmail).sendEmail(
            "Alice@example.com",
            "Notification",
            "Welcome!"
        );
    }
    
    @Test
    void testCanNotifyUser() {
        UserRepository mockRepo = mock(UserRepository.class);
        EmailService mockEmail = mock(EmailService.class);
        
        when(mockRepo.findById(1)).thenReturn(new User(1, "Bob"));
        when(mockEmail.isEmailValid("Bob@example.com")).thenReturn(true);
        
        UserNotificationService service = 
            new UserNotificationService(mockRepo, mockEmail);
        
        assertTrue(service.canNotifyUser(1));
    }
    
    @Test
    void testNotifyNonExistentUser() {
        UserRepository mockRepo = mock(UserRepository.class);
        EmailService mockEmail = mock(EmailService.class);
        
        when(mockRepo.findById(999)).thenReturn(null);
        
        UserNotificationService service = 
            new UserNotificationService(mockRepo, mockEmail);
        service.notifyUser(999, "Test");
        
        // Should not send email for non-existent user
        verify(mockEmail, never()).sendEmail(anyString(), anyString(), anyString());
    }
}

// ========================================
// PARAMETERIZED TESTS
// ========================================

import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.ValueSource;
import org.junit.jupiter.params.provider.CsvSource;

class ParameterizedTests {
    
    @ParameterizedTest
    @ValueSource(ints = {1, 2, 3, 4, 5})
    void testPositiveNumbers(int number) {
        assertTrue(number > 0);
    }
    
    @ParameterizedTest
    @CsvSource({
        "1, 1, 2",
        "2, 3, 5",
        "5, 5, 10"
    })
    void testAddition(int a, int b, int expected) {
        Calculator calc = new Calculator();
        assertEquals(expected, calc.add(a, b));
    }
}

// ========================================
// TEST DRIVEN DEVELOPMENT (TDD)
// ========================================

// Step 1: Write test first (it will fail)
class StringUtilsTest {
    
    @Test
    void testReverseString() {
        StringUtils utils = new StringUtils();
        assertEquals("olleh", utils.reverse("hello"));
    }
}

// Step 2: Implement minimum code to pass
class StringUtils {
    public String reverse(String str) {
        return new StringBuilder(str).reverse().toString();
    }
}

// Step 3: Refactor if needed
''',
    revisionPoints: [
      'Unit testing tests individual units or methods of a class',
      'JUnit is a popular Java testing framework',
      'Mockito is a mocking framework for simulating dependencies',
      '@Test annotation marks a method as a test method',
      '@BeforeEach runs before each test method',
      '@AfterEach runs after each test method',
      '@BeforeAll runs once before all tests',
      '@AfterAll runs once after all tests',
      '@Disabled skips a test',
      'assertEquals checks if two values are equal',
      'assertThrows checks if expected exception is thrown',
      'mock() creates a mock object in Mockito',
      'when().thenReturn() defines behavior of mock',
      'verify() verifies if a method was called',
      'doThrow() simulates exceptions in void methods',
      'Mocking isolates the unit under test',
      'Use @BeforeEach to initialize reusable objects',
      'JUnit and Mockito are widely used in backend testing',
    ],
    quizQuestions: [
      Question(
        question: 'Which annotation marks a JUnit test method?',
        options: ['@TestMethod', '@Test', '@UnitTest', '@TestCase'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation runs before each test?',
        options: ['@Before', '@BeforeEach', '@Setup', '@Init'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which method creates a mock object in Mockito?',
        options: ['create()', 'mock()', 'mockObject()', 'newMock()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Mockito method defines mock behavior?',
        options: ['when().thenReturn()', 'stub()', 'define()', 'behavior()'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which method verifies if a method was called?',
        options: ['check()', 'verify()', 'assert()', 'confirm()'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which assertion checks if two values are equal?',
        options: ['assertEqual', 'assertEquals', 'checkEquals', 'testEquals'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation skips a test in JUnit?',
        options: ['@Skip', '@Ignore', '@Disabled', '@Exclude'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which annotation runs once before all tests?',
        options: ['@BeforeAll', '@BeforeClass', '@SetupAll', '@InitAll'],
        correctIndex: 0,
      ),
      Question(
        question: 'What does TDD stand for?',
        options: ['Test Data Development', 'Test Driven Development', 'Total Development Design', 'Technical Development Documentation'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which assertion checks if an exception is thrown?',
        options: ['assertException', 'expectException', 'assertThrows', 'throwsException'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the purpose of mocking in unit tests?',
        options: ['Speed up tests', 'Isolate dependencies', 'Reduce code', 'Add features'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Mockito method simulates exceptions in void methods?',
        options: ['throwException()', 'doThrow()', 'mockThrow()', 'simulateException()'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_build_tools',
    title: '21. Build Tools: Maven and Gradle',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Build Tools** automate **compilation, dependency management, packaging, and deployment** of Java applications.
* **Maven**: A **declarative build tool** using XML (`pom.xml`) to manage projects.
* **Gradle**: A **modern, flexible build tool** using **Groovy or Kotlin DSL**, supports incremental builds.

**Key Points:**

* Both tools manage **dependencies, plugins, and project lifecycle**.
* Maven is **convention-based**, while Gradle is **flexible and scriptable**.
* Widely used in **enterprise Java projects, Spring Boot, and Android development**.

---

## **B. Maven**

**1. Project Object Model (POM)**

* Core file: `pom.xml` defines **project info, dependencies, plugins, and build instructions**.

**Example: pom.xml**

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
                             http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example</groupId>
    <artifactId>MyApp</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <!-- JUnit dependency -->
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-api</artifactId>
            <version>5.10.0</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <!-- Maven Compiler Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.10.1</version>
                <configuration>
                    <source>17</source>
                    <target>17</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

**2. Maven Lifecycle Commands**

| Command       | Purpose                            |
| ------------- | ---------------------------------- |
| `mvn clean`   | Delete target folder (clean build) |
| `mvn compile` | Compile source code                |
| `mvn test`    | Run unit tests                     |
| `mvn package` | Create JAR/WAR                     |
| `mvn install` | Install artifact in local repo     |
| `mvn deploy`  | Deploy artifact to remote repo     |

**3. Dependency Management**

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
    <version>3.2.0</version>
</dependency>
```

* Maven automatically downloads **dependencies from Maven Central**.

---

## **C. Gradle**

**1. Project Structure**

* **Build file**: `build.gradle` (Groovy DSL) or `build.gradle.kts` (Kotlin DSL)

**Example: build.gradle (Groovy)**

```groovy
plugins {
    id 'java'
}

group 'com.example'
version '1.0-SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.10.0'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.10.0'
}

test {
    useJUnitPlatform()
}
```

**2. Gradle Commands**

| Command           | Purpose                            |
| ----------------- | ---------------------------------- |
| `gradle clean`    | Delete build folder                |
| `gradle build`    | Compile, test, and package project |
| `gradle test`     | Run tests                          |
| `gradle run`      | Execute application                |
| `gradle assemble` | Build JAR/WAR without testing      |

**3. Dependency Management**

```groovy
dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-web:3.2.0'
}
```

* Gradle downloads dependencies from **Maven Central or other repositories**.

---

## **D. Maven vs Gradle Comparison**

| Feature               | Maven                       | Gradle                         |
| --------------------- | --------------------------- | ------------------------------ |
| Build Script          | XML (`pom.xml`)             | Groovy/Kotlin (`build.gradle`) |
| Flexibility           | Low (convention-based)      | High (customizable tasks)      |
| Performance           | Slower                      | Faster (incremental builds)    |
| Popularity            | Enterprise, legacy projects | Modern Java & Android projects |
| Dependency Management | Yes                         | Yes, more flexible             |

---

## **E. Best Practices / Exam Tips**

* Always define **groupId, artifactId, version** in Maven/Gradle.
* Use **repositories** wisely to manage dependencies.
* Prefer **Gradle** for **incremental builds and custom tasks**.
* Use **Maven for standard enterprise projects**.
* Understand **build lifecycle, dependency resolution, and plugin usage**.
''',
    codeSnippet: '''
// Maven pom.xml - Complete Example
<project xmlns="http://maven.apache.org/POM/4.0.0">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example</groupId>
    <artifactId>MyApp</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-api</artifactId>
            <version>5.10.0</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.10.1</version>
                <configuration>
                    <source>17</source>
                    <target>17</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>

// Gradle build.gradle - Complete Example
plugins {
    id 'java'
}

group 'com.example'
version '1.0-SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.10.0'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.10.0'
    implementation 'org.springframework.boot:spring-boot-starter-web:3.2.0'
}

test {
    useJUnitPlatform()
}
''',
    revisionPoints: [
      'Build tools automate compilation, dependency management, packaging, and deployment',
      'Maven is a declarative build tool using XML (pom.xml)',
      'Gradle is a modern, flexible build tool using Groovy or Kotlin DSL',
      'Both tools manage dependencies, plugins, and project lifecycle',
      'Maven is convention-based, while Gradle is flexible and scriptable',
      'pom.xml defines project info, dependencies, plugins, and build instructions',
      'Maven lifecycle commands: clean, compile, test, package, install, deploy',
      'Maven automatically downloads dependencies from Maven Central',
      'Gradle uses build.gradle (Groovy) or build.gradle.kts (Kotlin)',
      'Gradle commands: clean, build, test, run, assemble',
      'Gradle supports incremental builds for better performance',
      'Maven uses XML format, Gradle uses Groovy/Kotlin DSL',
      'Gradle is faster due to incremental builds',
      'Maven is popular in enterprise and legacy projects',
      'Gradle is preferred for modern Java and Android projects',
      'Always define groupId, artifactId, and version in build files',
      'Use repositories to manage dependencies (Maven Central, etc.)',
      'Understand build lifecycle, dependency resolution, and plugin usage',
    ],
    quizQuestions: [
      Question(
        question: 'What file does Maven use for project configuration?',
        options: ['build.gradle', 'pom.xml', 'settings.xml', 'maven.config'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which build tool uses Groovy or Kotlin DSL?',
        options: ['Maven', 'Ant', 'Gradle', 'Make'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the Maven command to compile source code?',
        options: ['mvn build', 'mvn compile', 'mvn run', 'mvn make'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Maven command creates a JAR or WAR file?',
        options: ['mvn install', 'mvn deploy', 'mvn package', 'mvn build'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the default repository Maven downloads dependencies from?',
        options: ['Maven Hub', 'Maven Central', 'Java Repository', 'Central Hub'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which Gradle command compiles, tests, and packages the project?',
        options: ['gradle compile', 'gradle package', 'gradle build', 'gradle assemble'],
        correctIndex: 2,
      ),
      Question(
        question: 'What are the three required elements in a Maven POM?',
        options: ['name, version, description', 'groupId, artifactId, version', 'id, name, version', 'group, artifact, build'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which build tool is known for incremental builds?',
        options: ['Maven', 'Gradle', 'Ant', 'Make'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the Maven "clean" command do?',
        options: ['Removes all dependencies', 'Deletes the target folder', 'Clears the cache', 'Formats the code'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which build tool is more flexible and customizable?',
        options: ['Maven (convention-based)', 'Gradle (scriptable)', 'Both equally flexible', 'Neither is flexible'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the Gradle equivalent of Maven\'s pom.xml?',
        options: ['gradle.xml', 'build.gradle', 'settings.gradle', 'config.gradle'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which build tool is primarily used for Android development?',
        options: ['Maven', 'Ant', 'Gradle', 'Make'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'java_spring_boot',
    title: '22. Spring Boot Framework Basics',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Spring Boot** is a **Java-based framework** that simplifies the development of **standalone, production-ready Spring applications**.
* It **auto-configures** applications, reducing boilerplate configuration.

**Key Points:**

* Provides **embedded servers** (Tomcat, Jetty) – no need to deploy WAR files.
* Supports **REST APIs, database integration, security, and microservices**.
* Uses **convention over configuration** for rapid development.

---

## **B. Features of Spring Boot**

| Feature            | Description                                                                       |
| ------------------ | --------------------------------------------------------------------------------- |
| Auto Configuration | Automatically configures Spring beans and libraries based on dependencies.        |
| Embedded Server    | Runs apps with embedded Tomcat/Jetty; no external server needed.                  |
| Starter POMs       | Predefined Maven/Gradle dependencies for quick setup (`spring-boot-starter-web`). |
| Actuator           | Provides **health, metrics, and monitoring** endpoints.                           |
| Spring Initializr  | Web-based tool to quickly generate projects with dependencies.                    |

---

## **C. Creating a Spring Boot Project**

**1. Using Spring Initializr**

* Website: [https://start.spring.io](https://start.spring.io)
* Select:
  * **Project**: Maven/Gradle
  * **Language**: Java
  * **Spring Boot version**: e.g., 3.2.0
  * **Dependencies**: Web, JPA, MySQL, etc.

**2. Project Structure**

```
myapp/
 ├─ src/main/java/com/example/myapp
 │   └─ MyAppApplication.java
 ├─ src/main/resources
 │   └─ application.properties
 └─ pom.xml / build.gradle
```

**3. Main Application Class**

```java
package com.example.myapp;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication // Enables auto-configuration
public class MyAppApplication {
    public static void main(String[] args) {
        SpringApplication.run(MyAppApplication.class, args);
    }
}
```

---

## **D. Creating a REST Controller**

```java
package com.example.myapp.controller;

import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class HelloController {

    @GetMapping("/hello")
    public String sayHello() {
        return "Hello, Spring Boot!";
    }
}
```

* `@RestController`: Marks class as REST endpoint.
* `@GetMapping("/hello")`: Maps HTTP GET request to `sayHello()` method.

---

## **E. Configuration with `application.properties`**

```properties
server.port=8081
spring.datasource.url=jdbc:mysql://localhost:3306/mydb
spring.datasource.username=root
spring.datasource.password=root123
spring.jpa.hibernate.ddl-auto=update
```

**Notes:**

* Change server port, database connection, and JPA settings.
* Supports **YAML (`application.yml`)** for structured configuration.

---

## **F. Dependency Injection**

```java
package com.example.myapp.service;

import org.springframework.stereotype.Service;

@Service
public class GreetingService {
    public String greet() {
        return "Hello from Service!";
    }
}

// Inject into Controller
@RestController
public class HelloController {
    private final GreetingService service;

    public HelloController(GreetingService service) {
        this.service = service;
    }

    @GetMapping("/greet")
    public String greet() {
        return service.greet();
    }
}
```

* Spring **automatically injects beans** using constructor injection.

---

## **G. Spring Boot Actuator (Monitoring)**

```xml
<!-- Maven dependency -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

* Access endpoints: `/actuator/health`, `/actuator/metrics`
* Provides **app health, metrics, and monitoring tools**.

---

## **H. Best Practices / Exam Tips**

* Use **Spring Initializr** for rapid project setup.
* Prefer **`@RestController` + `@GetMapping/@PostMapping`** for REST APIs.
* Always **externalize configuration** in `application.properties` or `application.yml`.
* Use **dependency injection** instead of `new` keyword for beans.
* Know **starter POMs**: `spring-boot-starter-web`, `spring-boot-starter-data-jpa`, `spring-boot-starter-security`.
* Actuator is useful for **interview questions on monitoring and production readiness**.
''',
    codeSnippet: '''
// Spring Boot Main Application
package com.example.myapp;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class MyAppApplication {
    public static void main(String[] args) {
        SpringApplication.run(MyAppApplication.class, args);
    }
}

// REST Controller
package com.example.myapp.controller;

import org.springframework.web.bind.annotation.*;

@RestController
public class HelloController {

    @GetMapping("/hello")
    public String sayHello() {
        return "Hello, Spring Boot!";
    }

    @PostMapping("/greet")
    public String greet(@RequestBody String name) {
        return "Hello, " + name + "!";
    }
}

// Service with Dependency Injection
package com.example.myapp.service;

import org.springframework.stereotype.Service;

@Service
public class GreetingService {
    public String greet() {
        return "Hello from Service!";
    }
}

@RestController
public class GreetingController {
    private final GreetingService service;

    public GreetingController(GreetingService service) {
        this.service = service;
    }

    @GetMapping("/greet")
    public String greet() {
        return service.greet();
    }
}

// application.properties
server.port=8081
spring.datasource.url=jdbc:mysql://localhost:3306/mydb
spring.datasource.username=root
spring.datasource.password=root123
spring.jpa.hibernate.ddl-auto=update
''',
    revisionPoints: [
      'Spring Boot simplifies development of standalone, production-ready Spring applications',
      'Auto-configures applications, reducing boilerplate configuration',
      'Provides embedded servers (Tomcat, Jetty) - no need to deploy WAR files',
      'Supports REST APIs, database integration, security, and microservices',
      'Uses convention over configuration for rapid development',
      'Auto Configuration automatically configures Spring beans based on dependencies',
      'Embedded Server allows running apps without external server setup',
      'Starter POMs provide predefined dependencies for quick setup',
      'Spring Boot Actuator provides health, metrics, and monitoring endpoints',
      'Spring Initializr is a web-based tool to quickly generate projects',
      '@SpringBootApplication annotation enables auto-configuration',
      '@RestController marks a class as a REST endpoint',
      '@GetMapping and @PostMapping map HTTP requests to methods',
      'application.properties externalizes configuration (port, database, etc.)',
      'YAML (application.yml) is supported for structured configuration',
      'Dependency injection automatically injects beans using constructors',
      'Use @Service annotation to mark service layer classes',
      'Actuator endpoints: /actuator/health, /actuator/metrics',
    ],
    quizQuestions: [
      Question(
        question: 'Which annotation marks a Spring Boot application?',
        options: ['@Application', '@SpringBootApplication', '@SpringApp', '@Boot'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the default embedded server in Spring Boot?',
        options: ['Jetty', 'Tomcat', 'WebSphere', 'GlassFish'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation marks a class as a REST controller?',
        options: ['@Controller', '@RestController', '@RestEndpoint', '@APIController'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of Spring Boot Actuator?',
        options: ['Database management', 'Monitoring and health checks', 'Security configuration', 'Template rendering'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which file is used for Spring Boot configuration?',
        options: ['config.xml', 'spring.properties', 'application.properties', 'boot.config'],
        correctIndex: 2,
      ),
      Question(
        question: 'What annotation is used for HTTP GET requests?',
        options: ['@Get', '@GetMapping', '@HTTPGet', '@RequestGet'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which tool generates Spring Boot projects quickly?',
        options: ['Spring Generator', 'Spring Initializr', 'Spring Creator', 'Spring Builder'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a Spring Boot Starter POM?',
        options: ['A database driver', 'A predefined set of dependencies', 'A configuration file', 'A testing framework'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation marks a service layer class?',
        options: ['@Component', '@Service', '@Bean', '@Layer'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does Spring Boot use for rapid development?',
        options: ['Configuration over convention', 'Convention over configuration', 'Manual configuration', 'XML-based setup'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which annotation is used for HTTP POST requests?',
        options: ['@Post', '@PostMapping', '@HTTPPost', '@RequestPost'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the Actuator health endpoint?',
        options: ['/health', '/actuator/health', '/status', '/monitor/health'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'java_best_practices',
    title: '23. Java Project Best Practices & Code Optimization',
    explanation: '''
## **A. Introduction**

**Definition:**

* **Java Project Best Practices** are a set of guidelines to ensure **readable, maintainable, and scalable code**.
* **Code Optimization** improves **performance, memory usage, and efficiency** of Java applications.

**Key Points:**

* Follows **OOP principles, clean code standards, and design patterns**.
* Optimization improves **runtime speed, memory footprint, and response time**.
* Best practices reduce **bugs, technical debt, and maintenance cost**.

---

## **B. Project Structure Best Practices**

**1. Standard Directory Layout**

```
myapp/
 ├─ src/main/java/com/example/myapp      // Source code
 │   ├─ controller
 │   ├─ service
 │   ├─ repository
 │   └─ model
 ├─ src/main/resources                  // Configuration files
 ├─ src/test/java/com/example/myapp     // Unit tests
 ├─ pom.xml / build.gradle              // Build tools
 └─ README.md
```

**2. Naming Conventions**

* **Classes:** PascalCase (`CustomerService`)
* **Methods/variables:** camelCase (`getCustomerName`)
* **Constants:** UPPER_CASE (`MAX_SIZE`)
* **Packages:** lowercase (`com.example.myapp`)

**3. Use Layered Architecture**

* **Controller Layer:** REST endpoints / UI interaction
* **Service Layer:** Business logic
* **Repository/DAO Layer:** Data access
* **Model/Entity Layer:** Data objects

---

## **C. Coding Best Practices**

| Best Practice               | Description                                                                             |
| --------------------------- | --------------------------------------------------------------------------------------- |
| Follow **SOLID principles** | Single Responsibility, Open/Closed, Liskov, Interface Segregation, Dependency Inversion |
| **DRY principle**           | Don't Repeat Yourself – reuse code and avoid duplication                                |
| **Meaningful naming**       | Use descriptive names for variables, methods, and classes                               |
| **Consistent formatting**   | Use IDE code formatter / linting                                                        |
| **Error handling**          | Proper exception handling with `try-catch` and custom exceptions                        |
| **Logging**                 | Use `SLF4J` or `Log4j` instead of `System.out.println`                                  |
| **Code comments**           | Explain why, not what – code should be self-explanatory                                 |

---

## **D. Performance Optimization Techniques**

**1. Memory Optimization**

* Use **primitive types** instead of wrapper classes when possible.
* Avoid **creating unnecessary objects** in loops.
* Use **StringBuilder** for string concatenation inside loops.

```java
StringBuilder sb = new StringBuilder();
for(int i=0; i<100; i++){
    sb.append(i);
}
String result = sb.toString();
```

**2. Collection Optimization**

* Use **ArrayList** for fast access, **LinkedList** for frequent insertions/deletions.
* Use **HashMap** for fast key-value lookups.
* Avoid **raw types** – always use generics (`Map<String, Integer>`).

**3. Algorithm & Data Structures**

* Choose **efficient algorithms** (O(log n) vs O(n²))
* Avoid nested loops for large data sets when possible

**4. Lazy Initialization**

* Load objects **only when needed**.

```java
private Connection connection;

public Connection getConnection() {
    if (connection == null) {
        connection = new Connection();
    }
    return connection;
}
```

**5. Caching**

* Cache **frequently used data** to reduce database or network calls.
* Use **ConcurrentHashMap** or **Ehcache/Redis** for production-level caching.

**6. Multithreading**

* Use **ExecutorService** for parallel tasks.
* Avoid **creating too many threads**; reuse threads from a pool.

**7. Profiling & Monitoring**

* Use **JVisualVM, JProfiler, or YourKit** to monitor memory leaks, CPU usage, and thread activity.

---

## **E. Testing & Code Quality**

* Write **unit tests** with **JUnit**.
* Use **Mockito** for mocking dependencies.
* Run **static code analysis** using **SonarQube or Checkstyle**.
* Maintain **high test coverage** to reduce bugs.

---

## **F. Version Control & Build Management**

* Use **Git** for version control.
* Use **Maven or Gradle** for build automation.
* Maintain **branching strategy** (feature, develop, main/master).

---

## **G. Best Practices / Exam Tips**

* Always **follow layered architecture**.
* Prioritize **clean, readable, and maintainable code** over micro-optimizations.
* Use **profiling tools** for optimization questions in interviews.
* Follow **SOLID, DRY, and KISS (Keep It Simple, Stupid) principles**.
* Be ready to explain **why certain data structures or patterns are chosen** for performance and scalability.
''',
    codeSnippet: '''
// Naming Conventions
class CustomerService {}         // PascalCase for classes
void getCustomerName() {}        // camelCase for methods
final int MAX_SIZE = 100;        // UPPER_CASE for constants
package com.example.myapp;       // lowercase for packages

// String Optimization
StringBuilder sb = new StringBuilder();
for(int i=0; i<100; i++){
    sb.append(i);
}
String result = sb.toString();

// Lazy Initialization
private Connection connection;

public Connection getConnection() {
    if (connection == null) {
        connection = new Connection();
    }
    return connection;
}

// Proper Exception Handling
try {
    // risky operation
    readFile("data.txt");
} catch (IOException e) {
    logger.error("File read error", e);
    throw new CustomException("Failed to read file", e);
}

// Use Generics
Map<String, Integer> userAges = new HashMap<>();
List<Customer> customers = new ArrayList<>();

// Layered Architecture Example
@RestController
public class CustomerController {
    private final CustomerService service;
    
    public CustomerController(CustomerService service) {
        this.service = service;
    }
    
    @GetMapping("/customers")
    public List<Customer> getAll() {
        return service.getAllCustomers();
    }
}

@Service
public class CustomerService {
    private final CustomerRepository repository;
    
    public CustomerService(CustomerRepository repository) {
        this.repository = repository;
    }
    
    public List<Customer> getAllCustomers() {
        return repository.findAll();
    }
}
''',
    revisionPoints: [
      'Java best practices ensure readable, maintainable, and scalable code',
      'Code optimization improves performance, memory usage, and efficiency',
      'Best practices follow OOP principles, clean code standards, and design patterns',
      'Standard directory layout: src/main/java, src/test/java, resources',
      'Naming conventions: Classes (PascalCase), methods/variables (camelCase), constants (UPPER_CASE)',
      'Use layered architecture: Controller, Service, Repository, Model layers',
      'Follow SOLID principles for object-oriented design',
      'DRY principle: Don\'t Repeat Yourself - reuse code and avoid duplication',
      'Use meaningful, descriptive names for variables, methods, and classes',
      'Proper error handling with try-catch and custom exceptions',
      'Use SLF4J or Log4j for logging instead of System.out.println',
      'Use StringBuilder for string concatenation inside loops',
      'Use ArrayList for fast access, LinkedList for frequent insertions/deletions',
      'Use HashMap for fast key-value lookups',
      'Lazy initialization loads objects only when needed',
      'Cache frequently used data to reduce database or network calls',
      'Use ExecutorService for parallel tasks and thread pooling',
      'Write unit tests with JUnit and use Mockito for mocking',
      'Use Git for version control and Maven/Gradle for build automation',
      'Prioritize clean, readable code over micro-optimizations',
      'Use profiling tools (JVisualVM, JProfiler) to monitor performance',
      'Follow KISS principle: Keep It Simple, Stupid',
    ],
    quizQuestions: [
      Question(
        question: 'Which is better for multiple string concatenations in loops?',
        options: ['String +', 'StringBuilder', 'concat()', 'String.format()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What naming convention is used for Java class names?',
        options: ['camelCase', 'snake_case', 'PascalCase', 'UPPER_CASE'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does DRY principle stand for?',
        options: ['Do Repeat Yourself', 'Don\'t Repeat Yourself', 'Define Reusable Years', 'Debug Runtime Yearly'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which layer handles business logic in layered architecture?',
        options: ['Controller', 'Service', 'Repository', 'Model'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the naming convention for constants in Java?',
        options: ['camelCase', 'PascalCase', 'UPPER_CASE', 'lowercase'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which collection is best for fast key-value lookups?',
        options: ['ArrayList', 'LinkedList', 'HashMap', 'TreeSet'],
        correctIndex: 2,
      ),
      Question(
        question: 'What should be used for logging instead of System.out.println?',
        options: ['Console.log', 'SLF4J or Log4j', 'Debug.print', 'Logger.out'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which tool is used for profiling Java applications?',
        options: ['JUnit', 'Maven', 'JVisualVM', 'Git'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does SOLID stand for in OOP?',
        options: ['Simple Object Language Interface Design', 'Single responsibility, Open/closed, Liskov, Interface segregation, Dependency inversion', 'Standard Object Layered Interface Development', 'Structured Optimized Logic In Development'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is better for frequent insertions and deletions?',
        options: ['ArrayList', 'LinkedList', 'HashMap', 'TreeSet'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is lazy initialization?',
        options: ['Loading all objects at startup', 'Loading objects only when needed', 'Delaying program execution', 'Postponing class loading'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which version control system is commonly used?',
        options: ['SVN', 'Git', 'Mercurial', 'CVS'],
        correctIndex: 1,
      ),
    ],
  ),
];

// DBMS Topics
final List<Topic> dbmsTopics = [
  Topic(
    id: 'dbms_intro',
    title: '1. Introduction to DBMS',
    explanation: '''## Database Management Systems (DBMS)

A Database Management System (DBMS) is a software system designed to define, create, maintain, and control access to databases. It provides a systematic way to create, retrieve, update, and manage data. A DBMS serves as an interface between the database and its end users or application programs, ensuring that data is consistently organized and remains easily accessible.

### Evolution of Database Systems
1. **File-Based Systems**: Early method of storing data in independent files
   - Disadvantages: Data redundancy, inconsistency, difficulty in access, isolation, integrity problems

2. **Database Systems**: Modern approach to centralized data management
   - Advantages: Data integration, sharing, minimal redundancy, consistency, standards enforcement

### DBMS Architecture

#### Three-Level Architecture (ANSI/SPARC)
1. **External Level (View Level)**
   - Highest level of abstraction
   - Describes different user views
   - Each view describes part of the database relevant to a particular user group

2. **Conceptual Level (Logical Level)**
   - Describes what data is stored and relationships
   - Focuses on entities, attributes, and relationships
   - Independent of both hardware and user views

3. **Internal Level (Physical Level)**
   - Describes how data is physically stored
   - Deals with storage allocation, indexing, access paths, compression, encryption
   - Closest to physical storage methods

#### Components of DBMS
1. **Query Processor**: Interprets and executes database queries
2. **Storage Manager**: Handles data storage, retrieval, and updates
3. **Transaction Manager**: Ensures database consistency during concurrent operations
4. **Recovery Manager**: Restores database to consistent state after failures
5. **Security Manager**: Controls access to database objects

### Data Models
Data models define the logical structure of a database and fundamentally determine how data can be stored, organized, and manipulated.

1. **Hierarchical Model**
   - Tree-like structure with parent-child relationships
   - Each child has only one parent
   - Example: IBM's Information Management System (IMS)

2. **Network Model**
   - Extension of hierarchical model
   - Allows many-to-many relationships
   - Example: Integrated Data Store (IDS)

3. **Relational Model**
   - Based on relational algebra
   - Data stored in tables (relations) with rows and columns
   - Relationships established through keys
   - Example: Oracle, MySQL, SQL Server, PostgreSQL

4. **Object-Oriented Model**
   - Data stored as objects (as in object-oriented programming)
   - Supports complex data types and inheritance
   - Example: ObjectDB, db4o

5. **Object-Relational Model**
   - Hybrid of relational and object-oriented models
   - Example: PostgreSQL, Oracle

6. **NoSQL Models**
   - Key-Value Stores: Redis, DynamoDB
   - Document Stores: MongoDB, CouchDB
   - Column-Family Stores: Cassandra, HBase
   - Graph Databases: Neo4j, OrientDB

### Database Languages
1. **Data Definition Language (DDL)**: Create, alter, drop database objects
2. **Data Manipulation Language (DML)**: Insert, update, delete, retrieve data
3. **Data Control Language (DCL)**: Grant, revoke permissions
4. **Transaction Control Language (TCL)**: Commit, rollback transactions

### DBMS Advantages
1. **Data Independence**: Application programs remain unaffected by changes in storage structure or access strategy
2. **Efficient Data Access**: Optimized methods for data retrieval and storage
3. **Data Integrity and Security**: Enforces constraints and access controls
4. **Data Administration**: Centralized control of data
5. **Concurrent Access and Crash Recovery**: Supports multiple users and recovers from failures
6. **Reduced Application Development Time**: Standardized access methods

### DBMS Disadvantages
1. **Complexity**: Difficult to design and maintain
2. **Size**: Large software requiring substantial hardware resources
3. **Cost**: Expensive to implement and maintain
4. **Performance**: Additional processing overhead
5. **Higher Impact of Failure**: Centralized approach means failures affect all applications

### Key DBMS Functions
1. **Data Dictionary Management**: Stores metadata
2. **Data Storage Management**: Controls storage and retrieval of data
3. **Data Transformation and Presentation**: Transforms logical requests to physical data retrieval
4. **Security Management**: Controls access to data
5. **Multiuser Access Control**: Implements locking to prevent update issues
6. **Backup and Recovery Management**: Protects data from system failure
7. **Data Integrity Management**: Enforces data rules and constraints
8. **Database Access Languages**: Provides query languages like SQL
9. **Database Communication Interfaces**: Allows access from applications
10. **Transaction Management**: Maintains database consistency during concurrent operations

### Database Transactions
A transaction is a logical unit of work that must be entirely completed or entirely aborted. Transactions follow ACID properties:

1. **Atomicity**: Transaction is all-or-nothing
2. **Consistency**: Transaction preserves database consistency
3. **Isolation**: Transactions are isolated from each other
4. **Durability**: Completed transactions persist even after system failures
''',
    codeSnippet: '''
-- Example SQL statements demonstrating database operations

-- DDL: Creating a table (Data Definition Language)
CREATE TABLE employees (
    employee_id INT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    hire_date DATE DEFAULT CURRENT_DATE,
    department_id INT,
    salary DECIMAL(10,2) CHECK (salary > 0),
    FOREIGN KEY (department_id) REFERENCES departments(department_id)
);

-- DML: Inserting data (Data Manipulation Language)
INSERT INTO employees (employee_id, first_name, last_name, email, department_id, salary)
VALUES (101, 'John', 'Smith', 'john.smith@example.com', 1, 50000.00);

-- DML: Querying data
SELECT e.employee_id, e.first_name, e.last_name, d.department_name
FROM employees e
JOIN departments d ON e.department_id = d.department_id
WHERE e.salary > 45000.00
ORDER BY e.last_name, e.first_name;

-- DML: Updating data
UPDATE employees
SET salary = salary * 1.05
WHERE department_id = 1;

-- DML: Deleting data
DELETE FROM employees
WHERE hire_date < '2010-01-01';

-- TCL: Transaction Control
BEGIN TRANSACTION;
    INSERT INTO departments (department_id, department_name) VALUES (5, 'Research');
    UPDATE employees SET department_id = 5 WHERE employee_id IN (101, 102, 103);
COMMIT;

-- DCL: Data Control Language
GRANT SELECT, INSERT ON employees TO hr_user;
REVOKE DELETE ON employees FROM general_user;
''',
    revisionPoints: [
      'DBMS is software that facilitates database creation, maintenance, and controlled access to data',
      'The three-level DBMS architecture consists of external (view), conceptual (logical), and internal (physical) levels',
      'Data models include hierarchical, network, relational, object-oriented, and NoSQL variants',
      'ACID properties (Atomicity, Consistency, Isolation, Durability) ensure reliable transaction processing',
      'Data independence allows changes to storage structures without affecting application programs',
      'SQL is divided into DDL, DML, DCL, and TCL for different database operations',
      'Normalization reduces data redundancy and improves data integrity',
      'Indexing improves query performance at the cost of additional storage and maintenance',
      'Concurrency control prevents interference between simultaneous transactions',
      'Recovery management ensures data can be restored after system failures'
    ],
    quizQuestions: [
      Question(
        question: 'Which of the following best describes the relationship between a database and a DBMS?',
        options: [
          'They are the same thing, just different terminology',
          'A database is the software, while DBMS is the actual data',
          'A database is a collection of data, while DBMS is the software to manage it',
          'A database is a specific table, while DBMS is a collection of related tables'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'In the three-level DBMS architecture, which level is responsible for describing how data is physically stored?',
        options: ['External level', 'Conceptual level', 'Internal level', 'Schema level'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the primary purpose of data abstraction in a DBMS?',
        options: [
          'To make database programming more challenging',
          'To hide complex implementation details and show only relevant data',
          'To create abstract data types in object-oriented databases only',
          'To abstract away the need for database administrators'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is NOT one of the ACID properties of database transactions?',
        options: ['Atomicity', 'Consistency', 'Idempotence', 'Durability'],
        correctIndex: 2,
      ),
      Question(
        question: 'What distinguishes the relational data model from hierarchical and network models?',
        options: [
          'It stores data in tables with relationships through keys',
          'It can only handle one-to-many relationships',
          'It was developed earlier than other models',
          'It requires specialized hardware to operate efficiently'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'Which type of database would be most appropriate for storing and analyzing highly interconnected data with complex relationships?',
        options: ['Relational database', 'Document-oriented database', 'Graph database', 'Key-value store'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is data independence in the context of database systems?',
        options: [
          'The ability to modify the database schema without affecting application programs',
          'The ability to store data independently without relationships',
          'The characteristic that data is not dependent on any constraints',
          'The feature that databases operate independently of the operating system'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'Which of these statements about database normalization is correct?',
        options: [
          'It improves query performance by denormalizing data',
          'It increases data redundancy to speed up read operations',
          'It organizes data to reduce redundancy and improve data integrity',
          'It is the process of converting non-relational databases to relational ones'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'The statement "INSERT INTO employees VALUES (101, \'John\', \'Smith\', 50000)" belongs to which category of SQL?',
        options: ['DDL (Data Definition Language)', 'DML (Data Manipulation Language)', 'DCL (Data Control Language)', 'TCL (Transaction Control Language)'],
        correctIndex: 1,
      ),
      Question(
        question: 'In a database system, which component is responsible for implementing the ACID properties?',
        options: ['Query Processor', 'Storage Manager', 'Transaction Manager', 'Security Manager'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'sql_basics',
    title: '2. SQL Basics',
    explanation: '''## SQL: Structured Query Language

SQL (Structured Query Language) is a domain-specific language used for managing and manipulating relational databases. Developed by IBM in the 1970s, SQL has become the standard language for relational database management systems (RDBMS).

### SQL Language Categories

#### 1. Data Definition Language (DDL)
Commands that define and modify database structure:
- **CREATE**: Creates new database objects (databases, tables, views, etc.)
- **ALTER**: Modifies existing database objects
- **DROP**: Removes database objects
- **TRUNCATE**: Removes all records from a table, including all spaces allocated for the records
- **COMMENT**: Adds comments to the data dictionary
- **RENAME**: Renames database objects

```sql
-- Create a new table
CREATE TABLE Students (
    student_id INT PRIMARY KEY,
    name VARCHAR(50),
    age INT,
    course VARCHAR(50)
);

-- Alter table to add a new column
ALTER TABLE Students
ADD email VARCHAR(100);

-- Rename the table
ALTER TABLE Students
RENAME TO CollegeStudents;

-- Drop (delete) the table
DROP TABLE CollegeStudents;

-- Truncate (delete all data but keep structure)
TRUNCATE TABLE Students;
```

#### 2. Data Manipulation Language (DML)
Commands that manipulate data within database objects:
- **SELECT**: Retrieves data from database
- **INSERT**: Adds new data into a table
- **UPDATE**: Modifies existing data
- **DELETE**: Removes data from a table
- **MERGE**: Combines INSERT and UPDATE operations (UPSERT)
- **CALL**: Calls a PL/SQL or Java subprogram
- **EXPLAIN PLAN**: Explains access path to data
- **LOCK TABLE**: Controls concurrency

```sql
-- Insert new record
INSERT INTO Students (student_id, name, age, course)
VALUES (1, 'Vardhan', 21, 'Computer Science');

-- Update existing record
UPDATE Students
SET course = 'AI and ML'
WHERE student_id = 1;

-- Delete specific record
DELETE FROM Students
WHERE student_id = 1;

-- Retrieve data
SELECT * FROM Students;
```

#### 3. Data Control Language (DCL)
Commands that control access to data in the database:
- **GRANT**: Gives user access privileges to database
- **REVOKE**: Withdraws access privileges
- **AUDIT**: Tracks the operations of users

```sql
-- Grant permission to user
GRANT SELECT, INSERT ON Students TO user1;

-- Revoke permission from user
REVOKE INSERT ON Students FROM user1;
```

#### 4. Transaction Control Language (TCL)
Commands that manage changes by DML statements:
- **COMMIT**: Saves work done
- **SAVEPOINT**: Identifies a point in a transaction to which you can later roll back
- **ROLLBACK**: Restores database to original since the last COMMIT
- **SET TRANSACTION**: Changes transaction options like isolation level

```sql
-- Start a transaction
BEGIN TRANSACTION;

-- Perform DML operations
UPDATE Students SET age = 22 WHERE student_id = 1;

-- Commit changes permanently
COMMIT;

-- Or rollback to undo changes
ROLLBACK;

-- Savepoint example
SAVEPOINT before_update;

UPDATE Students SET age = 25 WHERE student_id = 2;

ROLLBACK TO before_update;
```

### SQL Data Types

#### Numeric Types
- **INT/INTEGER**: Whole numbers
- **SMALLINT, BIGINT**: Smaller/larger range integers
- **DECIMAL(p,s)/NUMERIC(p,s)**: Exact numeric with precision p and scale s
- **FLOAT(n)**: Approximate numeric with mantissa precision n
- **REAL, DOUBLE PRECISION**: Hardware-dependent floating point

#### Character String Types
- **CHAR(n)**: Fixed-length character string
- **VARCHAR(n)**: Variable-length character string
- **TEXT**: Variable-length character string with very large maximum size

#### Binary String Types
- **BINARY(n)**: Fixed-length binary string
- **VARBINARY(n)**: Variable-length binary string
- **BLOB**: Binary large object

#### Date/Time Types
- **DATE**: Date (year, month, day)
- **TIME**: Time (hour, minute, second)
- **TIMESTAMP**: Date and time
- **INTERVAL**: Period of time

#### Boolean Type
- **BOOLEAN**: True/False values

#### Other Types
- **XML**: XML data
- **JSON**: JSON data (newer systems)
- **ARRAY**: Collection of similar data
- **User-defined types**: Custom types

### SQL Syntax Elements

#### 1. Basic SELECT Statement
```sql
SELECT column1, column2, ... 
FROM table_name
WHERE condition
GROUP BY column1, column2, ...
HAVING condition
ORDER BY column1 [ASC|DESC], column2 [ASC|DESC], ...
LIMIT count OFFSET skip;
```

#### 2. Operators
- **Arithmetic**: +, -, *, /, %
- **Comparison**: =, <>, !=, >, <, >=, <=
- **Logical**: AND, OR, NOT
- **Special**: BETWEEN, LIKE, IN, IS NULL, EXISTS

#### 3. Functions
- **Aggregate**: COUNT(), SUM(), AVG(), MIN(), MAX()
- **String**: CONCAT(), SUBSTRING(), LENGTH(), UPPER(), LOWER()
- **Date/Time**: NOW(), CURRENT_DATE(), DATE_FORMAT(), DATEDIFF()
- **Mathematical**: ROUND(), CEIL(), FLOOR(), ABS(), RAND()
- **Conversion**: CAST(), CONVERT()

### SQL Joins
Combines rows from two or more tables based on a related column:
- **INNER JOIN**: Returns records with matching values in both tables
- **LEFT JOIN**: Returns all records from left table and matched from right
- **RIGHT JOIN**: Returns all records from right table and matched from left
- **FULL JOIN**: Returns all records when there is a match in either table
- **CROSS JOIN**: Returns the Cartesian product of both tables
- **SELF JOIN**: Joins a table to itself

### SQL Subqueries
A query nested within another query:
- **Single-row subquery**: Returns only one row
- **Multiple-row subquery**: Returns multiple rows
- **Multiple-column subquery**: Returns multiple columns
- **Correlated subquery**: References columns from outer query
- **Nested subquery**: A subquery within another subquery

### Advanced SQL Features

#### 1. Views
Virtual tables derived from one or more tables:
```sql
CREATE VIEW view_name AS
SELECT column1, column2, ...
FROM table_name
WHERE condition;
```

#### 2. Stored Procedures
A prepared SQL code that can be saved and reused:
```sql
CREATE PROCEDURE procedure_name
AS
sql_statement
GO;
```

#### 3. Triggers
SQL code that automatically executes in response to events:
```sql
CREATE TRIGGER trigger_name
BEFORE|AFTER INSERT|UPDATE|DELETE
ON table_name
FOR EACH ROW
BEGIN
    -- trigger logic
END;
```

#### 4. Indexes
Improves the speed of data retrieval:
```sql
CREATE INDEX index_name
ON table_name (column1, column2, ...);
```

#### 5. Transactions
A sequence of operations performed as a single logical unit:
```sql
BEGIN TRANSACTION;
    -- SQL statements
COMMIT;
-- or
ROLLBACK;
```

### SQL Constraints
Rules enforced on data columns:
- **NOT NULL**: Ensures a column cannot have NULL value
- **UNIQUE**: Ensures all values in a column are different
- **PRIMARY KEY**: A combination of NOT NULL and UNIQUE
- **FOREIGN KEY**: Ensures referential integrity
- **CHECK**: Ensures values in a column satisfy a specific condition
- **DEFAULT**: Sets a default value for a column
''',
    codeSnippet: '''
-- Create a database
CREATE DATABASE school_management;

-- Use the database
USE school_management;

-- Create tables with relationships and constraints
CREATE TABLE Departments (
    DepartmentID INT PRIMARY KEY,
    DepartmentName VARCHAR(100) NOT NULL,
    Location VARCHAR(100),
    Budget DECIMAL(15,2) CHECK (Budget > 0),
    EstablishedDate DATE
);

CREATE TABLE Professors (
    ProfessorID INT PRIMARY KEY,
    FirstName VARCHAR(50) NOT NULL,
    LastName VARCHAR(50) NOT NULL,
    Email VARCHAR(100) UNIQUE,
    HireDate DATE DEFAULT CURRENT_DATE,
    Salary DECIMAL(10,2),
    DepartmentID INT,
    FOREIGN KEY (DepartmentID) REFERENCES Departments(DepartmentID)
);

CREATE TABLE Courses (
    CourseID VARCHAR(10) PRIMARY KEY,
    Title VARCHAR(100) NOT NULL,
    Credits INT CHECK (Credits BETWEEN 1 AND 6),
    DepartmentID INT,
    ProfessorID INT,
    FOREIGN KEY (DepartmentID) REFERENCES Departments(DepartmentID),
    FOREIGN KEY (ProfessorID) REFERENCES Professors(ProfessorID)
);

CREATE TABLE Students (
    StudentID INT PRIMARY KEY,
    FirstName VARCHAR(50) NOT NULL,
    LastName VARCHAR(50) NOT NULL,
    DateOfBirth DATE,
    Email VARCHAR(100) UNIQUE,
    EnrollmentDate DATE,
    MajorDepartmentID INT,
    FOREIGN KEY (MajorDepartmentID) REFERENCES Departments(DepartmentID)
);

CREATE TABLE Enrollments (
    EnrollmentID INT PRIMARY KEY,
    StudentID INT,
    CourseID VARCHAR(10),
    EnrollmentDate DATE DEFAULT CURRENT_DATE,
    Grade CHAR(1) CHECK (Grade IN ('A', 'B', 'C', 'D', 'F', 'I')),
    FOREIGN KEY (StudentID) REFERENCES Students(StudentID),
    FOREIGN KEY (CourseID) REFERENCES Courses(CourseID),
    UNIQUE (StudentID, CourseID) -- Prevent duplicate enrollments
);

-- Insert sample data
INSERT INTO Departments VALUES 
(1, 'Computer Science', 'North Building', 500000.00, '1990-09-01'),
(2, 'Mathematics', 'East Building', 350000.00, '1985-09-01'),
(3, 'Physics', 'South Building', 450000.00, '1988-09-01');

INSERT INTO Professors VALUES 
(101, 'John', 'Smith', 'john.smith@university.edu', '2005-08-15', 85000.00, 1),
(102, 'Mary', 'Johnson', 'mary.johnson@university.edu', '2010-01-10', 78000.00, 1),
(103, 'Robert', 'Williams', 'robert.williams@university.edu', '2008-06-20', 82000.00, 2),
(104, 'Patricia', 'Brown', 'patricia.brown@university.edu', '2015-07-30', 75000.00, 3);

-- Basic SELECT statement
SELECT * FROM Professors WHERE Salary > 80000;

-- JOIN operations
SELECT c.CourseID, c.Title, p.FirstName, p.LastName, d.DepartmentName
FROM Courses c
JOIN Professors p ON c.ProfessorID = p.ProfessorID
JOIN Departments d ON c.DepartmentID = d.DepartmentID;

-- Aggregation with GROUP BY
SELECT d.DepartmentName, COUNT(p.ProfessorID) AS NumberOfProfessors, 
       AVG(p.Salary) AS AverageSalary
FROM Departments d
LEFT JOIN Professors p ON d.DepartmentID = p.DepartmentID
GROUP BY d.DepartmentID, d.DepartmentName
HAVING COUNT(p.ProfessorID) > 0
ORDER BY COUNT(p.ProfessorID) DESC;

-- Subquery example
SELECT s.FirstName, s.LastName
FROM Students s
WHERE s.StudentID IN (
    SELECT e.StudentID
    FROM Enrollments e
    JOIN Courses c ON e.CourseID = c.CourseID
    WHERE c.ProfessorID = 101
);

-- Transaction example
BEGIN TRANSACTION;
    UPDATE Professors SET Salary = Salary * 1.05 WHERE DepartmentID = 1;
    INSERT INTO Courses VALUES ('CS400', 'Advanced Database Systems', 4, 1, 102);
COMMIT;
''',
    revisionPoints: [
      'SQL is a standardized language for relational database operations divided into DDL, DML, DCL, and TCL categories',
      'DDL (Data Definition Language) handles database structure with commands like CREATE, ALTER, DROP, TRUNCATE',
      'DML (Data Manipulation Language) manipulates data with SELECT, INSERT, UPDATE, DELETE commands',
      'SQL JOIN operations (INNER, LEFT, RIGHT, FULL) combine data from multiple tables based on related columns',
      'SQL constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, CHECK) ensure data integrity',
      'Normalization (1NF through 5NF) reduces redundancy and improves data integrity',
      'SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX) perform calculations on data sets',
      'SQL transactions ensure ACID properties through COMMIT, ROLLBACK, and SAVEPOINT commands',
      'SQL indexes improve query performance at the cost of additional storage and slower modifications',
      'SQL subqueries (nested queries) allow complex data retrieval operations'
    ],
    quizQuestions: [
      Question(
        question: 'Which statement correctly describes a SQL INNER JOIN?',
        options: [
          'Returns all records from both tables, regardless of matching values',
          'Returns only the records that have matching values in both tables',
          'Returns all records from the left table and matched from the right',
          'Returns only records that do not have matching values in either table'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'In SQL normalization, which normal form eliminates transitive dependencies?',
        options: ['First Normal Form (1NF)', 'Second Normal Form (2NF)', 'Third Normal Form (3NF)', 'Fourth Normal Form (4NF)'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the output of the following SQL query?\nSELECT COUNT(*) FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees);',
        options: [
          'The total number of employees',
          'The average salary of all employees',
          'The number of employees with salary above the average',
          'The highest salary in the Employees table'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which SQL constraint ensures that a relationship between two tables remains valid?',
        options: ['CHECK constraint', 'UNIQUE constraint', 'PRIMARY KEY constraint', 'FOREIGN KEY constraint'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which SQL clause is used to filter groups in a GROUP BY query?',
        options: ['WHERE', 'HAVING', 'FILTER', 'GROUP FILTER'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the time complexity of a query using a B-tree index in SQL?',
        options: ['O(1)', 'O(log n)', 'O(n)', 'O(n log n)'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is NOT a valid aggregate function in SQL?',
        options: ['COUNT()', 'SUM()', 'AVG()', 'MEDIAN()'],
        correctIndex: 3,
      ),
      Question(
        question: 'What happens when you execute a ROLLBACK statement in SQL?',
        options: [
          'The current transaction is committed',
          'All changes since the last COMMIT are undone',
          'The database server restarts',
          'All tables are truncated'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which technique would be most efficient for retrieving the second highest salary from an Employees table?',
        options: [
          'Using ORDER BY and LIMIT/OFFSET',
          'Using a self-join',
          'Using MAX() aggregate function',
          'Using GROUP BY with COUNT()'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'In a well-designed relational database, what typically connects tables together?',
        options: [
          'Primary keys only',
          'Foreign keys referencing primary keys',
          'CHECK constraints',
          'Identical column names in both tables'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which statement is used to delete data from a database?',
        options: ['REMOVE', 'DELETE', 'COLLAPSE', 'TRUNCATE'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which SQL clause is used to filter records?',
        options: ['WHERE', 'HAVING', 'FILTER', 'GROUP BY'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which SQL command is used to modify data in a database?',
        options: ['MODIFY', 'ALTER', 'UPDATE', 'CHANGE'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the correct SQL syntax to select all columns from a table named "Customers"?',
        options: ['SELECT [ALL] FROM Customers;', 'SELECT * FROM Customers;', 'SELECT Customers;', 'SELECT ALL Customers;'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'normalization',
    title: '3. Database Normalization',
    explanation: '''# 📘 DATABASE NORMALIZATION — FULL GUIDE WITH EXAMPLES

## 🧠 What is Database Normalization?

**Normalization** is the process of organizing data in a database efficiently.
It helps to:

* Reduce **data redundancy** (duplicate data)
* Improve **data integrity** (accuracy & consistency)
* Make **updates, insertions, and deletions** easier

## ⚙️ Real-World Example Setup

Let's consider an **"Online Bookstore"** database.

We'll start with an **un-normalized table** and then normalize it step-by-step.

## 🧩 Step 0: Unnormalized Table (UNF)

### 🧾 Table: `BookOrders`

| OrderID | CustomerName | CustomerPhone | BooksOrdered                | TotalAmount |
| ------- | ------------ | ------------- | --------------------------- | ----------- |
| 1       | John         | 9876543210    | The Alchemist, Harry Potter | 750         |
| 2       | Emma         | 8765432109    | Harry Potter                | 450         |
| 3       | John         | 9876543210    | Inferno                     | 350         |

### 🔍 Issues:

* Multiple books in one column → **repeated / grouped data**
* Redundant customer info (John appears twice)

## 🧱 1NF (First Normal Form)

### ✅ Rule:

1. Each cell must hold **a single value**
2. Each record must be **unique**

### ✅ Conversion to 1NF

We separate multiple books into individual rows.

### 🧾 Table: `BookOrders_1NF`

| OrderID | CustomerName | CustomerPhone | BookTitle     | Price |
| ------- | ------------ | ------------- | ------------- | ----- |
| 1       | John         | 9876543210    | The Alchemist | 300   |
| 1       | John         | 9876543210    | Harry Potter  | 450   |
| 2       | Emma         | 8765432109    | Harry Potter  | 450   |
| 3       | John         | 9876543210    | Inferno       | 350   |

### 💻 SQL Example

```sql
CREATE TABLE BookOrders_1NF (
    OrderID INT,
    CustomerName VARCHAR(50),
    CustomerPhone VARCHAR(15),
    BookTitle VARCHAR(100),
    Price DECIMAL(10,2)
);

INSERT INTO BookOrders_1NF VALUES
(1, 'John', '9876543210', 'The Alchemist', 300),
(1, 'John', '9876543210', 'Harry Potter', 450),
(2, 'Emma', '8765432109', 'Harry Potter', 450),
(3, 'John', '9876543210', 'Inferno', 350);
```

## 🧱 2NF (Second Normal Form)

### ✅ Rule:

1. Must be in **1NF**
2. No **partial dependency** —
   Every non-key column must depend on the **whole primary key**, not part of it.

### 🧩 Problem:

In `BookOrders_1NF`,
`CustomerName` and `CustomerPhone` depend only on `OrderID` (not on `BookTitle`).

Hence, we split into separate tables.

### ✅ Conversion to 2NF

#### 🧾 Table: `Customers`

| CustomerID | CustomerName | CustomerPhone |
| ---------- | ------------ | ------------- |
| 1          | John         | 9876543210    |
| 2          | Emma         | 8765432109    |

#### 🧾 Table: `Orders`

| OrderID | CustomerID |
| ------- | ---------- |
| 1       | 1          |
| 2       | 2          |
| 3       | 1          |

#### 🧾 Table: `OrderDetails`

| OrderID | BookTitle     | Price |
| ------- | ------------- | ----- |
| 1       | The Alchemist | 300   |
| 1       | Harry Potter  | 450   |
| 2       | Harry Potter  | 450   |
| 3       | Inferno       | 350   |

### 💻 SQL Example

```sql
CREATE TABLE Customers (
    CustomerID INT PRIMARY KEY,
    CustomerName VARCHAR(50),
    CustomerPhone VARCHAR(15)
);

CREATE TABLE Orders (
    OrderID INT PRIMARY KEY,
    CustomerID INT,
    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)
);

CREATE TABLE OrderDetails (
    OrderID INT,
    BookTitle VARCHAR(100),
    Price DECIMAL(10,2),
    FOREIGN KEY (OrderID) REFERENCES Orders(OrderID)
);

INSERT INTO Customers VALUES
(1, 'John', '9876543210'),
(2, 'Emma', '8765432109');

INSERT INTO Orders VALUES
(1, 1), (2, 2), (3, 1);

INSERT INTO OrderDetails VALUES
(1, 'The Alchemist', 300),
(1, 'Harry Potter', 450),
(2, 'Harry Potter', 450),
(3, 'Inferno', 350);
```

## 🧱 3NF (Third Normal Form)

### ✅ Rule:

1. Must be in **2NF**
2. No **transitive dependency** (non-key attribute depending on another non-key attribute)

### 🧩 Problem:

If we add a `BookTitle` and its `Author`, `Price`, etc.,
these values depend on `BookTitle`, not `OrderID`.

### ✅ Conversion to 3NF

Split book info into its own table.

#### 🧾 Table: `Books`

| BookID | BookTitle     | Author       | Price |
| ------ | ------------- | ------------ | ----- |
| 1      | The Alchemist | Paulo Coelho | 300   |
| 2      | Harry Potter  | J.K. Rowling | 450   |
| 3      | Inferno       | Dan Brown    | 350   |

#### 🧾 Updated `OrderDetails`

| OrderID | BookID |
| ------- | ------ |
| 1       | 1      |
| 1       | 2      |
| 2       | 2      |
| 3       | 3      |

### 💻 SQL Example

```sql
CREATE TABLE Books (
    BookID INT PRIMARY KEY,
    BookTitle VARCHAR(100),
    Author VARCHAR(100),
    Price DECIMAL(10,2)
);

INSERT INTO Books VALUES
(1, 'The Alchemist', 'Paulo Coelho', 300),
(2, 'Harry Potter', 'J.K. Rowling', 450),
(3, 'Inferno', 'Dan Brown', 350);

CREATE TABLE OrderDetails_3NF (
    OrderID INT,
    BookID INT,
    FOREIGN KEY (OrderID) REFERENCES Orders(OrderID),
    FOREIGN KEY (BookID) REFERENCES Books(BookID)
);

INSERT INTO OrderDetails_3NF VALUES
(1, 1), (1, 2), (2, 2), (3, 3);
```

## 🧱 BCNF (Boyce-Codd Normal Form)

### ✅ Rule:

Every determinant must be a **candidate key**.

Our 3NF structure already satisfies BCNF,
since every dependency is based on primary or foreign keys.

## 🧱 4NF (Fourth Normal Form)

### ✅ Rule:

1. Must already be in **BCNF**
2. Remove **multi-valued dependencies**
   → A record should not contain two or more **independent multi-valued facts**.

### 🧩 Example Scenario

Let's say now each **Author** can:

* Write **multiple books**
* Have **multiple awards**

These are **two independent multi-valued attributes**.

### ✅ Conversion to 4NF

Split into **two separate tables**:

### 🧾 Table: `AuthorBooks`

| AuthorName   | BookTitle     |
| ------------ | ------------- |
| Paulo Coelho | The Alchemist |
| J.K. Rowling | Harry Potter  |
| Dan Brown    | Inferno       |

### 🧾 Table: `AuthorAwards`

| AuthorName   | Award                |
| ------------ | -------------------- |
| Paulo Coelho | Golden Pen           |
| Paulo Coelho | Lifetime Achievement |
| J.K. Rowling | Hugo Award           |
| J.K. Rowling | British Book Award   |

### 💻 SQL Example

```sql
CREATE TABLE AuthorBooks (
    AuthorName VARCHAR(100),
    BookTitle VARCHAR(100),
    PRIMARY KEY (AuthorName, BookTitle)
);

CREATE TABLE AuthorAwards (
    AuthorName VARCHAR(100),
    Award VARCHAR(100),
    PRIMARY KEY (AuthorName, Award)
);

INSERT INTO AuthorBooks VALUES
('Paulo Coelho', 'The Alchemist'),
('J.K. Rowling', 'Harry Potter'),
('Dan Brown', 'Inferno');

INSERT INTO AuthorAwards VALUES
('Paulo Coelho', 'Golden Pen'),
('Paulo Coelho', 'Lifetime Achievement'),
('J.K. Rowling', 'Hugo Award'),
('J.K. Rowling', 'British Book Award');
```

## 🧱 5NF (Fifth Normal Form)

### ✅ Rule:

1. Must already be in **4NF**
2. Eliminate **join dependency** —
   Data should not be reconstructible by **joining multiple tables unnecessarily**.

### ✅ Conversion to 5NF

We divide into **three independent relations**:

#### 🧾 Table: `BookAuthors`

| BookTitle     | AuthorName   |
| ------------- | ------------ |
| The Alchemist | Paulo Coelho |
| Harry Potter  | J.K. Rowling |

#### 🧾 Table: `BookPublishers`

| BookTitle     | Publisher     |
| ------------- | ------------- |
| The Alchemist | HarperCollins |
| The Alchemist | Penguin       |
| Harry Potter  | Bloomsbury    |
| Harry Potter  | Scholastic    |

#### 🧾 Table: `AuthorPublishers`

| AuthorName   | Publisher     |
| ------------ | ------------- |
| Paulo Coelho | HarperCollins |
| Paulo Coelho | Penguin       |
| J.K. Rowling | Bloomsbury    |
| J.K. Rowling | Scholastic    |

### 💻 SQL Example

```sql
CREATE TABLE BookAuthors (
    BookTitle VARCHAR(100),
    AuthorName VARCHAR(100),
    PRIMARY KEY (BookTitle, AuthorName)
);

CREATE TABLE BookPublishers (
    BookTitle VARCHAR(100),
    Publisher VARCHAR(100),
    PRIMARY KEY (BookTitle, Publisher)
);

CREATE TABLE AuthorPublishers (
    AuthorName VARCHAR(100),
    Publisher VARCHAR(100),
    PRIMARY KEY (AuthorName, Publisher)
);

INSERT INTO BookAuthors VALUES
('The Alchemist', 'Paulo Coelho'),
('Harry Potter', 'J.K. Rowling');

INSERT INTO BookPublishers VALUES
('The Alchemist', 'HarperCollins'),
('The Alchemist', 'Penguin'),
('Harry Potter', 'Bloomsbury'),
('Harry Potter', 'Scholastic');

INSERT INTO AuthorPublishers VALUES
('Paulo Coelho', 'HarperCollins'),
('Paulo Coelho', 'Penguin'),
('J.K. Rowling', 'Bloomsbury'),
('J.K. Rowling', 'Scholastic');
```

## 🧩 Summary of All Normal Forms

| Normal Form | Main Rule                           | Example Fix                                  |
| ----------- | ----------------------------------- | -------------------------------------------- |
| **1NF**     | Atomic values (no repeating groups) | Split multi-book column                      |
| **2NF**     | No partial dependency               | Separate customer/order info                 |
| **3NF**     | No transitive dependency            | Create `Books` table                         |
| **BCNF**    | Every determinant is a key          | Already satisfied                            |
| **4NF**     | No multi-valued dependency          | Split Author-Book & Author-Award             |
| **5NF**     | No join dependency                  | Split into Author, Book, Publisher relations |

## 🎯 Benefits of Normalization

* Reduces **data redundancy**
* Improves **consistency & reliability**
* Simplifies **updates and deletions**
* Increases **database efficiency**

## 🧮 Quick SQL Check for Data Retrieval

```sql
SELECT c.CustomerName, b.BookTitle, b.Author, b.Price
FROM Customers c
JOIN Orders o ON c.CustomerID = o.CustomerID
JOIN OrderDetails od ON o.OrderID = od.OrderID
JOIN Books b ON od.BookID = b.BookID;
```
''',
    codeSnippet: '''
-- Complete Database Normalization Tutorial
-- Online Bookstore Example

-- ========================================
-- STEP 1: First Normal Form (1NF)
-- ========================================
CREATE TABLE BookOrders_1NF (
    OrderID INT,
    CustomerName VARCHAR(50),
    CustomerPhone VARCHAR(15),
    BookTitle VARCHAR(100),
    Price DECIMAL(10,2)
);

INSERT INTO BookOrders_1NF VALUES
(1, 'John', '9876543210', 'The Alchemist', 300),
(1, 'John', '9876543210', 'Harry Potter', 450),
(2, 'Emma', '8765432109', 'Harry Potter', 450),
(3, 'John', '9876543210', 'Inferno', 350);

-- ========================================
-- STEP 2: Second Normal Form (2NF)
-- ========================================
CREATE TABLE Customers (
    CustomerID INT PRIMARY KEY,
    CustomerName VARCHAR(50),
    CustomerPhone VARCHAR(15)
);

CREATE TABLE Orders (
    OrderID INT PRIMARY KEY,
    CustomerID INT,
    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)
);

CREATE TABLE OrderDetails (
    OrderID INT,
    BookTitle VARCHAR(100),
    Price DECIMAL(10,2),
    FOREIGN KEY (OrderID) REFERENCES Orders(OrderID)
);

INSERT INTO Customers VALUES
(1, 'John', '9876543210'),
(2, 'Emma', '8765432109');

INSERT INTO Orders VALUES
(1, 1), (2, 2), (3, 1);

INSERT INTO OrderDetails VALUES
(1, 'The Alchemist', 300),
(1, 'Harry Potter', 450),
(2, 'Harry Potter', 450),
(3, 'Inferno', 350);

-- ========================================
-- STEP 3: Third Normal Form (3NF)
-- ========================================
CREATE TABLE Books (
    BookID INT PRIMARY KEY,
    BookTitle VARCHAR(100),
    Author VARCHAR(100),
    Price DECIMAL(10,2)
);

CREATE TABLE OrderDetails_3NF (
    OrderID INT,
    BookID INT,
    PRIMARY KEY (OrderID, BookID),
    FOREIGN KEY (OrderID) REFERENCES Orders(OrderID),
    FOREIGN KEY (BookID) REFERENCES Books(BookID)
);

INSERT INTO Books VALUES
(1, 'The Alchemist', 'Paulo Coelho', 300),
(2, 'Harry Potter', 'J.K. Rowling', 450),
(3, 'Inferno', 'Dan Brown', 350);

INSERT INTO OrderDetails_3NF VALUES
(1, 1), (1, 2), (2, 2), (3, 3);

-- ========================================
-- STEP 4: Fourth Normal Form (4NF)
-- ========================================
CREATE TABLE AuthorBooks (
    AuthorName VARCHAR(100),
    BookTitle VARCHAR(100),
    PRIMARY KEY (AuthorName, BookTitle)
);

CREATE TABLE AuthorAwards (
    AuthorName VARCHAR(100),
    Award VARCHAR(100),
    PRIMARY KEY (AuthorName, Award)
);

INSERT INTO AuthorBooks VALUES
('Paulo Coelho', 'The Alchemist'),
('J.K. Rowling', 'Harry Potter'),
('Dan Brown', 'Inferno');

INSERT INTO AuthorAwards VALUES
('Paulo Coelho', 'Golden Pen'),
('Paulo Coelho', 'Lifetime Achievement'),
('J.K. Rowling', 'Hugo Award'),
('J.K. Rowling', 'British Book Award');

-- ========================================
-- STEP 5: Fifth Normal Form (5NF)
-- ========================================
CREATE TABLE BookAuthors (
    BookTitle VARCHAR(100),
    AuthorName VARCHAR(100),
    PRIMARY KEY (BookTitle, AuthorName)
);

CREATE TABLE BookPublishers (
    BookTitle VARCHAR(100),
    Publisher VARCHAR(100),
    PRIMARY KEY (BookTitle, Publisher)
);

CREATE TABLE AuthorPublishers (
    AuthorName VARCHAR(100),
    Publisher VARCHAR(100),
    PRIMARY KEY (AuthorName, Publisher)
);

INSERT INTO BookAuthors VALUES
('The Alchemist', 'Paulo Coelho'),
('Harry Potter', 'J.K. Rowling');

INSERT INTO BookPublishers VALUES
('The Alchemist', 'HarperCollins'),
('The Alchemist', 'Penguin'),
('Harry Potter', 'Bloomsbury'),
('Harry Potter', 'Scholastic');

INSERT INTO AuthorPublishers VALUES
('Paulo Coelho', 'HarperCollins'),
('Paulo Coelho', 'Penguin'),
('J.K. Rowling', 'Bloomsbury'),
('J.K. Rowling', 'Scholastic');

-- ========================================
-- Query to retrieve normalized data
-- ========================================
SELECT c.CustomerName, b.BookTitle, b.Author, b.Price
FROM Customers c
JOIN Orders o ON c.CustomerID = o.CustomerID
JOIN OrderDetails_3NF od ON o.OrderID = od.OrderID
JOIN Books b ON od.BookID = b.BookID;
''',
    revisionPoints: [
      '1NF: Each cell must contain a single value, and each record needs to be unique',
      '2NF: Table must be in 1NF and all non-key attributes must depend on the entire primary key',
      '3NF: Table must be in 2NF and no transitive dependencies (non-key attributes depending on other non-key attributes)',
      'BCNF: For any dependency A → B, A must be a super key',
      '4NF: No multi-valued dependencies - split independent multi-valued facts',
      '5NF: No join dependencies - data should not be unnecessarily reconstructible'
    ],
    quizQuestions: [
      Question(
        question: 'What is the main purpose of normalization?',
        options: ['Speed up database operations', 'Reduce data redundancy', 'Improve database security', 'Create more tables'],
        correctIndex: 1,
      ),
      Question(
        question: 'In the Online Bookstore example, which table violates 1NF?',
        options: [
          'BookOrders with multiple books in one column',
          'Customers table',
          'Orders table',
          'Books table'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'What is the minimum normal form most databases should achieve?',
        options: ['1NF', '2NF', '3NF', '4NF'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a partial dependency?',
        options: [
          'When a non-key attribute depends on part of the primary key',
          'When a primary key depends on a non-key attribute',
          'When all attributes depend on the primary key',
          'When a table has multiple primary keys'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'What is a transitive dependency?',
        options: [
          'When a non-key attribute depends on another non-key attribute',
          'When a primary key depends on a foreign key',
          'When an attribute depends on itself',
          'When a foreign key depends on a primary key'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'Which normal form deals with removing repeating groups?',
        options: ['1NF', '2NF', '3NF', 'BCNF'],
        correctIndex: 0,
      ),
      Question(
        question: 'In 2NF, why do we split CustomerName and CustomerPhone into a separate Customers table?',
        options: [
          'To improve query performance',
          'Because they depend only on CustomerID, not on the full composite key',
          'To save storage space',
          'Because of database security requirements'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does 4NF eliminate?',
        options: [
          'Repeating groups',
          'Partial dependencies',
          'Transitive dependencies',
          'Multi-valued dependencies'
        ],
        correctIndex: 3,
      ),
      Question(
        question: 'In the bookstore example, which is an example of multi-valued dependency addressed by 4NF?',
        options: [
          'A book can have multiple prices',
          'An author can write multiple books AND have multiple awards independently',
          'A customer can place multiple orders',
          'An order can contain multiple books'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main concern of 5NF (Fifth Normal Form)?',
        options: [
          'Eliminating repeating groups',
          'Removing partial dependencies',
          'Eliminating join dependencies',
          'Creating more foreign keys'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'In BCNF, every determinant must be a:',
        options: [
          'Foreign key',
          'Candidate key',
          'Non-key attribute',
          'Composite key'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which benefit is NOT achieved through normalization?',
        options: [
          'Reduced data redundancy',
          'Improved data consistency',
          'Faster query execution in all cases',
          'Simplified updates and deletions'
        ],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'transactions',
    title: '4. Database Transactions',
    explanation: '''## Database Transactions (Detailed Version)

### A. Introduction

**Definition:**

A **database transaction** is a **logical unit of work** in a database, consisting of **one or more SQL operations** that must be executed **completely or not at all**. Transactions ensure **data integrity, consistency, and reliability** in multi-user and failure-prone environments.

**Importance:**

- Prevents **data inconsistency** during concurrent operations.
- Ensures **rollback capability** in case of errors.
- Widely used in **banking, e-commerce, inventory management, and reservation systems**.

---

### B. ACID Properties

| Property        | Description                                                  | Example                                                      |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Atomicity**   | All operations succeed or fail as one unit.                  | A money transfer: both debit and credit must succeed.        |
| **Consistency** | Database must remain valid before and after transaction.     | Constraints like foreign keys and unique keys are preserved. |
| **Isolation**   | Transactions do not affect each other's intermediate states. | Two users updating same record will not interfere.           |
| **Durability**  | Once committed, changes are permanent, even after crashes.   | Data is saved on disk and survives system failure.           |

**Detailed ACID Explanations:**

#### Atomicity
All operations in a transaction are treated as a single unit, which either succeeds completely or fails completely. There is no partial execution. If any operation fails, all previous operations in the transaction are rolled back.

#### Consistency
The database must be in a consistent state before and after the transaction. All integrity constraints, rules, and triggers must be satisfied. The transaction takes the database from one valid state to another valid state.

#### Isolation
Concurrent transactions should not affect each other's execution. Each transaction should execute as if it's the only transaction in the system, even though multiple transactions may be executing simultaneously.

#### Durability
Once a transaction is committed, it will remain so, even in the event of power loss, crashes, or system failures. The changes are permanently recorded in the database.

---

### C. Transaction Commands

#### 1. Start Transaction

```sql
START TRANSACTION;
-- or
BEGIN;
```

Marks the beginning of a transaction block. All subsequent SQL statements will be part of this transaction until a COMMIT or ROLLBACK is issued.

#### 2. Commit – Make changes permanent

```sql
COMMIT;
```

Permanently saves all changes made during the transaction to the database. Once committed, the changes cannot be undone.

#### 3. Rollback – Undo changes

```sql
ROLLBACK;
```

Reverts all changes made during the transaction, restoring the database to its state before the transaction began.

#### 4. Savepoint – Partial rollback

```sql
SAVEPOINT sp1;
-- some operations
ROLLBACK TO sp1;
```

Creates a named point within a transaction to which you can later roll back, allowing partial rollback without aborting the entire transaction.

#### 5. Set Transaction Isolation Level

```sql
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
```

Defines the isolation level for the transaction, controlling how transaction integrity is visible to other users and systems.

---

### D. Example: Bank Transfer

```sql
-- Alice transfers \$100 to Bob
START TRANSACTION;

UPDATE accounts
SET balance = balance - 100
WHERE account_id = 1;  -- Alice

UPDATE accounts
SET balance = balance + 100
WHERE account_id = 2;  -- Bob

-- Ensure both succeed
COMMIT;

-- If any error occurs
ROLLBACK;
```

**Explanation:**

- If the first `UPDATE` succeeds but the second fails, `ROLLBACK` restores original balances.
- Guarantees **atomic money transfer**.
- Without transactions, money could disappear (debited but not credited) or appear from nowhere (credited but not debited).

**Extended Example with Error Handling:**

```sql
START TRANSACTION;

-- Check if Alice has sufficient balance
SELECT balance FROM accounts WHERE account_id = 1 FOR UPDATE;

-- If balance >= 100, proceed
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;

-- Add transaction log
INSERT INTO transaction_log (from_account, to_account, amount, timestamp)
VALUES (1, 2, 100, NOW());

COMMIT;
```

---

### E. Transaction Isolation Levels (Concurrency Control)

| Level                | Description                                        | Pros                          | Cons                          |
| -------------------- | -------------------------------------------------- | ----------------------------- | ----------------------------- |
| **Read Uncommitted** | Can read uncommitted changes (dirty read)          | Fastest                       | Risk of inconsistent data     |
| **Read Committed**   | Only reads committed changes                       | Prevents dirty reads          | Non-repeatable reads possible |
| **Repeatable Read**  | Same query returns same result in a transaction    | Prevents non-repeatable reads | Phantom reads possible        |
| **Serializable**     | Full isolation; executes transactions sequentially | Complete consistency          | Slowest, high locking         |

#### Examples of Anomalies:

**1. Dirty Read:**
Reading uncommitted data from another transaction. If that transaction rolls back, you've read data that never existed.

```sql
-- Transaction 1
UPDATE accounts SET balance = 5000 WHERE account_id = 1;
-- Not committed yet

-- Transaction 2 (READ UNCOMMITTED)
SELECT balance FROM accounts WHERE account_id = 1;  -- Reads 5000

-- Transaction 1
ROLLBACK;  -- Balance reverts to original
```

**2. Non-Repeatable Read:**
Same query returns different results in the same transaction because another transaction modified the data.

```sql
-- Transaction 1
SELECT balance FROM accounts WHERE account_id = 1;  -- Returns 1000

-- Transaction 2
UPDATE accounts SET balance = 2000 WHERE account_id = 1;
COMMIT;

-- Transaction 1
SELECT balance FROM accounts WHERE account_id = 1;  -- Returns 2000 (different!)
```

**3. Phantom Read:**
New rows added by another transaction appear unexpectedly in subsequent reads.

```sql
-- Transaction 1
SELECT COUNT(*) FROM orders WHERE status = 'pending';  -- Returns 10

-- Transaction 2
INSERT INTO orders (status) VALUES ('pending');
COMMIT;

-- Transaction 1
SELECT COUNT(*) FROM orders WHERE status = 'pending';  -- Returns 11 (phantom!)
```

---

### F. Transaction Properties in SQL

#### Autocommit Mode
Each SQL statement is treated as a transaction and is automatically committed. Can be disabled:

```sql
-- Disable autocommit
SET autocommit = 0;

-- Now you need manual commits
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1;
COMMIT;
```

#### Manual Commit
Explicit control over when changes are saved:

```sql
START TRANSACTION;
-- Multiple operations
COMMIT;  -- Or ROLLBACK;
```

#### Nested Transactions with Savepoints
Use **SAVEPOINTs** to rollback partially within a transaction:

```sql
START TRANSACTION;

INSERT INTO orders (order_id, customer_id) VALUES (1, 100);
SAVEPOINT after_order;

INSERT INTO order_items (order_id, item_id) VALUES (1, 500);
SAVEPOINT after_items;

-- Oops, wrong item
ROLLBACK TO after_items;

-- Re-insert correct item
INSERT INTO order_items (order_id, item_id) VALUES (1, 501);

COMMIT;  -- Saves order and correct item
```

---

### G. Best Practices

#### 1. Always commit or rollback transactions explicitly
Never leave transactions open indefinitely. Use proper error handling to ensure transactions are completed.

```sql
START TRANSACTION;
BEGIN TRY
    -- Operations
    COMMIT;
END TRY
BEGIN CATCH
    ROLLBACK;
    -- Handle error
END CATCH
```

#### 2. Use savepoints for complex multi-step transactions
Break complex transactions into logical steps with savepoints for better error recovery.

#### 3. Choose isolation level wisely
Balance consistency vs performance based on your application needs:
- **Banking:** Use SERIALIZABLE or REPEATABLE READ
- **Read-heavy applications:** READ COMMITTED is usually sufficient
- **Real-time analytics:** READ UNCOMMITTED (accept some inconsistency for speed)

#### 4. Keep transactions short-lived
Minimize the time a transaction holds locks to improve concurrency:
- Perform non-database operations outside transactions
- Avoid user interaction within transactions
- Read data, process, then write in quick succession

#### 5. Avoid unnecessary autocommit mode
For critical operations involving multiple steps, disable autocommit and use explicit transactions.

#### 6. Handle errors and exceptions properly
Always include error handling to catch and rollback failed transactions:

```sql
START TRANSACTION;
-- Check for errors after each operation
-- Use IF statements or exception handlers
-- ROLLBACK on error, COMMIT on success
```

---

### H. Exam Tips

1. **Be able to explain ACID properties with examples**
   - Atomicity: Money transfer example
   - Consistency: Maintaining foreign key constraints
   - Isolation: Preventing concurrent update conflicts
   - Durability: Surviving system crashes

2. **Know common SQL commands**
   - `START TRANSACTION` / `BEGIN`
   - `COMMIT`
   - `ROLLBACK`
   - `SAVEPOINT`
   - `SET TRANSACTION ISOLATION LEVEL`

3. **Understand isolation levels and anomalies**
   - Which levels prevent which anomalies
   - Trade-offs between consistency and performance
   - When to use each level

4. **Real-world scenario questions**
   - Bank transfers and fund management
   - Inventory updates and stock management
   - Reservation systems (hotels, flights, tickets)
   - E-commerce order processing

5. **Common Interview Questions:**
   - "What happens if a transaction fails midway?"
   - "How do isolation levels affect performance?"
   - "Explain the difference between ROLLBACK and ROLLBACK TO SAVEPOINT"
   - "Why is durability important in banking systems?"

---

### I. Advanced Concepts

#### Two-Phase Commit (Distributed Transactions)
Used in distributed databases to ensure all nodes commit or rollback together:

**Phase 1: Prepare**
- Coordinator asks all nodes: "Can you commit?"
- Each node prepares and responds "Yes" or "No"

**Phase 2: Commit/Abort**
- If all say "Yes": Coordinator sends COMMIT to all
- If any say "No": Coordinator sends ROLLBACK to all

#### Deadlock Detection
When two transactions wait for each other's locks:

```sql
-- Transaction 1
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
-- Waiting for lock on account_id = 2

-- Transaction 2
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;
-- Waiting for lock on account_id = 1

-- Deadlock! Database must abort one transaction
```

**Prevention Strategies:**
- Always acquire locks in the same order
- Use timeouts
- Deadlock detection algorithms

#### MVCC (Multi-Version Concurrency Control)
Modern databases use MVCC to allow:
- Readers don't block writers
- Writers don't block readers
- Each transaction sees a consistent snapshot

Used in: PostgreSQL, Oracle, MySQL InnoDB

---

### Summary

Database transactions are fundamental to maintaining data integrity in concurrent, multi-user environments. Understanding ACID properties, transaction commands, and isolation levels is crucial for designing reliable database applications. Always handle transactions explicitly, choose appropriate isolation levels, and follow best practices to ensure data consistency and system performance.
''',
    codeSnippet: '''
-- ========================================
-- 1. Basic Transaction Example
-- ========================================
START TRANSACTION;

UPDATE accounts
SET balance = balance - 100
WHERE account_id = 1;  -- Alice

UPDATE accounts
SET balance = balance + 100
WHERE account_id = 2;  -- Bob

COMMIT;

-- ========================================
-- 2. Transaction with Rollback
-- ========================================
START TRANSACTION;

INSERT INTO orders (order_id, customer_id, total) 
VALUES (101, 5, 500.00);

-- Error occurs
ROLLBACK;  -- Undo the insert

-- ========================================
-- 3. Using Savepoints
-- ========================================
START TRANSACTION;

INSERT INTO orders (order_id, customer_id) VALUES (1, 100);
SAVEPOINT after_order;

INSERT INTO order_items (order_id, item_id, quantity) 
VALUES (1, 500, 2);
SAVEPOINT after_first_item;

INSERT INTO order_items (order_id, item_id, quantity) 
VALUES (1, 501, 1);

-- Oops, wrong quantity for first item
ROLLBACK TO after_order;

-- Re-insert with correct data
INSERT INTO order_items (order_id, item_id, quantity) 
VALUES (1, 500, 3);
INSERT INTO order_items (order_id, item_id, quantity) 
VALUES (1, 501, 1);

COMMIT;

-- ========================================
-- 4. Setting Isolation Levels
-- ========================================
-- Read Uncommitted (lowest isolation)
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
START TRANSACTION;
SELECT * FROM accounts;
COMMIT;

-- Read Committed (prevents dirty reads)
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
START TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1;
COMMIT;

-- Repeatable Read (prevents non-repeatable reads)
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
START TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1;
-- Same query will return same result
SELECT balance FROM accounts WHERE account_id = 1;
COMMIT;

-- Serializable (highest isolation)
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
START TRANSACTION;
SELECT * FROM accounts WHERE balance > 1000;
COMMIT;

-- ========================================
-- 5. Bank Transfer with Error Handling
-- ========================================
START TRANSACTION;

-- Check sender balance
DECLARE @sender_balance DECIMAL(10,2);
SELECT @sender_balance = balance 
FROM accounts 
WHERE account_id = 1;

IF @sender_balance >= 100
BEGIN
    -- Sufficient balance, proceed
    UPDATE accounts 
    SET balance = balance - 100 
    WHERE account_id = 1;
    
    UPDATE accounts 
    SET balance = balance + 100 
    WHERE account_id = 2;
    
    -- Log transaction
    INSERT INTO transaction_log 
    (from_account, to_account, amount, trans_date)
    VALUES (1, 2, 100, GETDATE());
    
    COMMIT;
    SELECT 'Transfer successful' AS status;
END
ELSE
BEGIN
    ROLLBACK;
    SELECT 'Insufficient balance' AS status;
END

-- ========================================
-- 6. Autocommit Control
-- ========================================
-- Disable autocommit
SET autocommit = 0;

UPDATE accounts SET balance = balance + 100 WHERE account_id = 1;
UPDATE accounts SET balance = balance - 100 WHERE account_id = 2;

-- Must explicitly commit
COMMIT;

-- Re-enable autocommit
SET autocommit = 1;

-- ========================================
-- 7. Demonstrating Dirty Read Problem
-- ========================================
-- Transaction 1
START TRANSACTION;
UPDATE accounts SET balance = 5000 WHERE account_id = 1;
-- NOT COMMITTED YET

-- Transaction 2 (with READ UNCOMMITTED)
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
START TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1;  
-- Reads 5000 (dirty read!)
COMMIT;

-- Transaction 1
ROLLBACK;  -- Balance reverts, Transaction 2 read invalid data!

-- ========================================
-- 8. Preventing Dirty Reads
-- ========================================
-- Transaction 2 (with READ COMMITTED)
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
START TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1;  
-- Waits for Transaction 1 to commit/rollback
-- Only reads committed data
COMMIT;

-- ========================================
-- 9. Complex Multi-Step Transaction
-- ========================================
START TRANSACTION;

-- Step 1: Create order
INSERT INTO orders (order_id, customer_id, order_date, total)
VALUES (1001, 50, CURRENT_DATE, 0);

SAVEPOINT order_created;

-- Step 2: Add order items
INSERT INTO order_items (order_id, product_id, quantity, price)
VALUES (1001, 101, 2, 25.00);

INSERT INTO order_items (order_id, product_id, quantity, price)
VALUES (1001, 102, 1, 50.00);

SAVEPOINT items_added;

-- Step 3: Update order total
UPDATE orders 
SET total = (SELECT SUM(quantity * price) FROM order_items WHERE order_id = 1001)
WHERE order_id = 1001;

-- Step 4: Update inventory
UPDATE products SET stock = stock - 2 WHERE product_id = 101;
UPDATE products SET stock = stock - 1 WHERE product_id = 102;

-- If all successful
COMMIT;

-- If error at any step, can rollback to savepoint
-- ROLLBACK TO items_added;
-- or complete rollback
-- ROLLBACK;

-- ========================================
-- 10. Deadlock Example
-- ========================================
-- Transaction T1
START TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
-- Now waiting to update account_id = 2

-- Transaction T2 (running concurrently)
START TRANSACTION;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;
-- Now waiting to update account_id = 1

-- DEADLOCK! Database will detect and abort one transaction

-- Prevention: Always lock in same order
-- Both transactions should lock accounts in ascending ID order
''',
    revisionPoints: [
      'A database transaction is a logical unit of work consisting of one or more SQL operations that execute completely or not at all',
      'ACID properties: Atomicity (all or nothing), Consistency (valid state), Isolation (no interference), Durability (permanent changes)',
      'START TRANSACTION or BEGIN initiates a transaction block',
      'COMMIT permanently saves all transaction changes to the database',
      'ROLLBACK reverts all changes made during the transaction',
      'SAVEPOINT creates named points within a transaction for partial rollback',
      'Four isolation levels: READ UNCOMMITTED (fastest, least safe), READ COMMITTED (prevents dirty reads), REPEATABLE READ (prevents non-repeatable reads), SERIALIZABLE (slowest, most safe)',
      'Dirty read: reading uncommitted data that may be rolled back',
      'Non-repeatable read: same query returns different results within one transaction',
      'Phantom read: new rows appear in subsequent reads within same transaction',
      'Autocommit mode treats each SQL statement as a separate transaction',
      'Keep transactions short-lived to reduce locking and improve concurrency',
      'Always handle errors with proper ROLLBACK in exception handlers',
      'Two-phase commit protocol ensures distributed transaction consistency',
      'Deadlock occurs when transactions wait for each other\'s locks, requiring database intervention',
      'MVCC (Multi-Version Concurrency Control) allows readers and writers to work without blocking',
      'Bank transfers, inventory updates, and reservation systems are classic transaction use cases',
      'Choose isolation level based on consistency requirements vs performance needs',
      'Use FOR UPDATE clause to explicitly lock rows during SELECT operations',
      'Transaction logs ensure durability by recording all changes before committing',
    ],
    quizQuestions: [
      Question(
        question: 'What does the "A" in ACID stand for?',
        options: ['Authority', 'Authentication', 'Atomicity', 'Availability'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which statement is used to permanently save changes made in a transaction?',
        options: ['SAVE', 'COMMIT', 'PERSIST', 'STORE'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which statement is used to undo changes made in a transaction?',
        options: ['REVERT', 'UNDO', 'ROLLBACK', 'CANCEL'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which of the following is NOT an isolation level in SQL?',
        options: ['READ UNCOMMITTED', 'READ COMMITTED', 'REPEATABLE READ', 'REDUNDANT READ'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which ACID property ensures that the database remains in a valid state after a transaction?',
        options: ['Atomicity', 'Consistency', 'Isolation', 'Durability'],
        correctIndex: 1,
      ),
      Question(
        question: 'In a bank transfer transaction, what happens if the debit succeeds but credit fails?',
        options: [
          'The debit is kept and credit is skipped',
          'The entire transaction is rolled back',
          'Only the credit is retried',
          'The database crashes'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of anomaly occurs when a transaction reads uncommitted data that is later rolled back?',
        options: ['Phantom read', 'Non-repeatable read', 'Dirty read', 'Lost update'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which isolation level provides the highest consistency but lowest performance?',
        options: ['READ UNCOMMITTED', 'READ COMMITTED', 'REPEATABLE READ', 'SERIALIZABLE'],
        correctIndex: 3,
      ),
      Question(
        question: 'What is the purpose of a SAVEPOINT in a transaction?',
        options: [
          'To commit part of a transaction',
          'To create a backup of the database',
          'To allow partial rollback within a transaction',
          'To improve transaction performance'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What does the Durability property in ACID guarantee?',
        options: [
          'Transactions execute quickly',
          'Multiple users can access data simultaneously',
          'Committed changes survive system failures',
          'All operations in a transaction succeed together'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'In which scenario would you use the REPEATABLE READ isolation level?',
        options: [
          'When you need maximum speed and can tolerate inconsistency',
          'When you need to ensure the same query returns consistent results within a transaction',
          'When you want to read uncommitted changes',
          'When you never want to lock any rows'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What happens when autocommit mode is enabled?',
        options: [
          'All transactions must be manually committed',
          'Each SQL statement is automatically committed',
          'Transactions are never committed',
          'Only SELECT statements are committed'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'indexing',
    title: '5. Database Indexing',
    explanation: '''## Database Indexing

### A. Introduction

**Definition:**

A **database index** is a **special data structure** that improves the **speed of data retrieval** from a database table. It acts like a **table of contents** in a book, allowing the database to find rows **without scanning the entire table**.

**Importance:**

- Speeds up **SELECT queries**, **JOIN operations**, and **WHERE clause filtering**.
- Essential for **large databases** where full table scans are costly.
- Helps maintain **uniqueness** for **primary keys and unique constraints**.
- Can dramatically improve performance from minutes to milliseconds for large datasets.

**How It Works:**

Instead of scanning every row in a table, the database uses the index to quickly locate the desired data. Think of it like using an index in a textbook rather than reading every page to find a topic.

---

### B. Types of Indexes

| Index Type                      | Description                                                              | Example                                          |
| ------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------ |
| **Primary Key Index**           | Automatically created on primary key; ensures uniqueness                 | `id INT PRIMARY KEY`                             |
| **Unique Index**                | Ensures column values are unique                                         | `CREATE UNIQUE INDEX idx_email ON users(email);` |
| **Clustered Index**             | Determines **physical order** of rows in a table; usually on primary key | SQL Server: `PRIMARY KEY CLUSTERED`              |
| **Non-Clustered Index**         | Logical order separate from physical table order                         | `CREATE INDEX idx_name ON users(name);`          |
| **Composite/Multicolumn Index** | Index on multiple columns                                                | `CREATE INDEX idx_name_age ON users(name, age);` |
| **Full-Text Index**             | Optimized for text searching in large columns                            | `CREATE FULLTEXT INDEX ON articles(content)`     |

#### Detailed Index Type Explanations:

**1. Primary Key Index:**
- Automatically created when you define a primary key
- Enforces uniqueness and non-null constraint
- Usually the fastest way to access individual rows
- Example: Employee ID, Customer ID, Order ID

**2. Unique Index:**
- Similar to primary key but allows NULL values (in most databases)
- Prevents duplicate entries in indexed column(s)
- Used for email addresses, usernames, SSN, etc.

**3. Clustered Index:**
- Determines the **physical storage order** of data in the table
- Only **one clustered index per table** (because data can only be physically sorted one way)
- In most databases, primary key is clustered by default
- Very fast for range queries

**4. Non-Clustered Index:**
- Creates a separate structure pointing to actual data rows
- **Multiple non-clustered indexes** can exist on a table
- Contains a pointer to the clustered index or row location
- Slightly slower than clustered but more flexible

**5. Composite/Multicolumn Index:**
- Index on multiple columns together
- Most effective when queries filter on multiple columns
- Column order matters: (name, age) is different from (age, name)
- Best for queries like: WHERE name='John' AND age=30

**6. Full-Text Index:**
- Specialized index for searching text content
- Supports complex text searches: partial words, synonyms, relevance ranking
- Used in search engines, article databases, document management
- Example: Searching "database optimization" in articles

---

### C. Creating Indexes

#### 1. Single Column Index

```sql
CREATE INDEX idx_customer_name
ON customers(name);
```

**Speeds up queries like:**

```sql
SELECT * FROM customers WHERE name = 'Alice';
SELECT * FROM customers WHERE name LIKE 'Al%';
SELECT name, age FROM customers ORDER BY name;
```

#### 2. Composite Index

```sql
CREATE INDEX idx_name_age
ON customers(name, age);
```

**Optimized for queries using both columns:**

```sql
SELECT * FROM customers WHERE name='Alice' AND age=25;
SELECT * FROM customers WHERE name='Alice';  -- Also uses index
-- But NOT optimized for:
SELECT * FROM customers WHERE age=25;  -- Doesn't use index efficiently
```

**Important:** Composite index (A, B, C) can be used for:
- WHERE A = ? AND B = ? AND C = ?
- WHERE A = ? AND B = ?
- WHERE A = ?
- But NOT efficiently for: WHERE B = ? or WHERE C = ?

#### 3. Unique Index

```sql
CREATE UNIQUE INDEX idx_email
ON customers(email);
```

**Benefits:**
- Prevents **duplicate email entries**
- Automatically enforces data integrity
- Faster than regular index for lookups

#### 4. Creating Index with Options

```sql
-- Index with specific order
CREATE INDEX idx_salary_desc ON employees(salary DESC);

-- Partial/Filtered Index (PostgreSQL)
CREATE INDEX idx_active_users 
ON users(name) 
WHERE status = 'active';

-- Index with included columns (SQL Server)
CREATE INDEX idx_emp_name 
ON employees(name) 
INCLUDE (salary, department);
```

---

### D. Dropping Indexes

```sql
-- Standard SQL
DROP INDEX idx_customer_name;

-- MySQL specific
DROP INDEX idx_customer_name ON customers;

-- SQL Server
DROP INDEX customers.idx_customer_name;

-- PostgreSQL
DROP INDEX idx_customer_name;
```

**When to Drop Indexes:**
- Index is no longer used by queries
- Table is very small and full scan is faster
- Too many indexes are slowing down INSERT/UPDATE/DELETE
- Index is redundant (e.g., composite index makes single-column index unnecessary)

**Warning:** Use **carefully**, as removing indexes can **significantly slow queries**.

---

### E. How Indexing Works

#### 1. B-Tree Index (most common)

**Structure:**
- Stores data in a **balanced tree structure**
- Root → Branch → Leaf nodes
- Maintains sorted order
- Logarithmic time complexity: O(log n)

**Efficient for:**
- **Exact matches**: WHERE id = 100
- **Range queries**: WHERE salary BETWEEN 50000 AND 80000
- **Ordered retrieval**: ORDER BY name
- **Prefix searches**: WHERE name LIKE 'John%'

**Example query:**

```sql
SELECT * FROM orders 
WHERE order_date BETWEEN '2025-01-01' AND '2025-10-01';
```

**B-Tree Process:**
1. Start at root node
2. Navigate through branch nodes using comparison
3. Reach leaf node containing actual data or pointers
4. Total operations: O(log n) instead of O(n)

#### 2. Hash Index

**Structure:**
- Uses **hash function** to map keys to locations
- Very fast for exact match: O(1)
- Creates hash buckets for storing data

**Efficient for:**
- **Exact matches only**: WHERE id = 100
- **Equality comparisons**: WHERE email = 'user@example.com'

**NOT suitable for:**
- Range queries (WHERE salary > 50000)
- Partial matches (WHERE name LIKE 'John%')
- Sorting operations (ORDER BY)

**Example:**
```sql
-- Hash index works great for:
SELECT * FROM users WHERE user_id = 12345;

-- But NOT for:
SELECT * FROM users WHERE user_id > 10000;  -- Won't use hash index
```

#### 3. Bitmap Index

**Structure:**
- Uses **bitmaps** (arrays of bits: 0 and 1)
- Each distinct value gets a bitmap
- Best for **low-cardinality columns** (few unique values)

**Ideal for:**
- Boolean columns (true/false)
- Gender (M/F/Other)
- Status (Active/Inactive/Pending)
- Data warehousing and analytics

**Example:**
```sql
-- Bitmap index on status column
-- Status = 'Active': 11001010...
-- Status = 'Inactive': 00110101...

SELECT * FROM customers WHERE status = 'Active' AND gender = 'F';
-- Performs fast bitmap operations (AND, OR, NOT)
```

**Advantages:**
- Very space efficient for low-cardinality columns
- Fast for combining multiple conditions
- Excellent for analytical queries

**Disadvantages:**
- Not suitable for high-cardinality columns
- Slower for frequent updates

#### 4. Full-Text Index

**Structure:**
- Creates inverted index of words
- Maps each word to documents containing it
- Supports stemming, stop words, relevance ranking

**Optimized for:**
- **Searching keywords** in large text columns
- Natural language queries
- Document search applications

**Example:**

```sql
-- Create full-text index
CREATE FULLTEXT INDEX idx_article_content 
ON articles(title, content);

-- Search for articles containing keywords
SELECT * FROM articles 
WHERE MATCH(title, content) AGAINST('database optimization' IN NATURAL LANGUAGE MODE);

-- Search with boolean operators
SELECT * FROM articles 
WHERE MATCH(title, content) AGAINST('+database -mysql' IN BOOLEAN MODE);
```

---

### F. Index Usage in Queries

#### With Index (Fast):

```sql
-- Index on emp_id makes this instant
SELECT * FROM employees WHERE emp_id = 101;

-- Index on (department, salary) helps
SELECT * FROM employees 
WHERE department = 'IT' AND salary > 50000;

-- Index on order_date speeds this up
SELECT * FROM orders 
WHERE order_date BETWEEN '2025-01-01' AND '2025-12-31'
ORDER BY order_date;
```

#### Without Index (Slow – Full Table Scan):

```sql
-- No index on salary column
SELECT * FROM employees WHERE salary > 50000;
-- Database scans all rows one by one

-- Function on indexed column prevents index use
SELECT * FROM employees WHERE YEAR(hire_date) = 2020;
-- Should be: WHERE hire_date BETWEEN '2020-01-01' AND '2020-12-31'

-- Leading wildcard prevents index use
SELECT * FROM customers WHERE name LIKE '%son';
-- Should be: WHERE name LIKE 'son%' (if possible)
```

#### Checking Index Usage:

```sql
-- MySQL
EXPLAIN SELECT * FROM employees WHERE emp_id = 101;

-- Shows: type='const', key='PRIMARY' → using index efficiently
-- Shows: type='ALL' → full table scan (no index used)

-- PostgreSQL
EXPLAIN ANALYZE SELECT * FROM employees WHERE emp_id = 101;

-- SQL Server
SET STATISTICS IO ON;
SELECT * FROM employees WHERE emp_id = 101;
```

**Tip:** Index **columns used in WHERE, JOIN, ORDER BY, and GROUP BY clauses** for better performance.

---

### G. Best Practices

#### 1. Do not over-index
Each index consumes disk space and slows **INSERT, UPDATE, DELETE** operations. Every data modification must update all relevant indexes.

**Rule of thumb:**
- Small tables (<1000 rows): May not need indexes except primary key
- Medium tables (1000-100K rows): Index frequently queried columns
- Large tables (>100K rows): Carefully planned indexes are critical

#### 2. Index frequently queried columns
Monitor your queries and identify columns that appear often in:
- WHERE clauses
- JOIN conditions
- ORDER BY clauses
- GROUP BY clauses

#### 3. Use composite indexes for multi-column queries
Instead of creating separate indexes on (name) and (age), create one composite index on (name, age) if queries often filter on both.

**Example:**
```sql
-- Better: One composite index
CREATE INDEX idx_name_age ON customers(name, age);

-- Instead of: Two separate indexes
CREATE INDEX idx_name ON customers(name);
CREATE INDEX idx_age ON customers(age);
```

#### 4. Keep indexes updated
Consider rebuilding indexes periodically to:
- Remove fragmentation
- Update statistics
- Optimize performance

```sql
-- MySQL
OPTIMIZE TABLE customers;

-- SQL Server
ALTER INDEX ALL ON customers REBUILD;

-- PostgreSQL
REINDEX TABLE customers;
```

#### 5. Avoid indexing low-selectivity columns
Columns with few unique values (like boolean flags) provide minimal benefit and waste space.

**Low selectivity (avoid indexing):**
- Boolean: is_active (2 values)
- Gender: M/F/Other (3 values)
- Status: Active/Inactive (2-5 values)

**High selectivity (good for indexing):**
- Email addresses (unique)
- Employee IDs (unique)
- Phone numbers (mostly unique)
- Names (many variations)

#### 6. Monitor index usage
Use database tools to identify:
- Unused indexes (candidates for removal)
- Missing indexes (queries doing full scans)
- Index effectiveness

```sql
-- MySQL: Check index usage
SHOW INDEX FROM customers;
SELECT * FROM sys.schema_unused_indexes;

-- PostgreSQL: Check index usage
SELECT * FROM pg_stat_user_indexes 
WHERE idx_scan = 0;

-- SQL Server: Find missing indexes
SELECT * FROM sys.dm_db_missing_index_details;
```

#### 7. Consider index selectivity
Higher selectivity (more unique values) = better index performance

```sql
-- High selectivity: Good for index
CREATE INDEX idx_email ON users(email);  -- Mostly unique

-- Low selectivity: Waste of space
CREATE INDEX idx_gender ON users(gender);  -- Only 2-3 values
```

#### 8. Use covering indexes when possible
Include all columns needed by a query in the index to avoid accessing the table.

```sql
-- Query needs: name, email, phone
CREATE INDEX idx_covering 
ON customers(name) 
INCLUDE (email, phone);

-- Now this query only touches the index:
SELECT name, email, phone FROM customers WHERE name = 'John';
```

---

### H. Exam Tips

#### 1. Know types of indexes
- **Primary Key Index**: Unique, non-null, automatically created
- **Unique Index**: Enforces uniqueness, allows NULL
- **Clustered Index**: Physical order, one per table
- **Non-Clustered Index**: Logical order, multiple allowed
- **Composite Index**: Multiple columns, order matters
- **Full-Text Index**: Text searching, keyword queries

#### 2. Understand trade-offs
**Benefits:**
- Faster SELECT queries
- Efficient JOINs
- Quick sorting (ORDER BY)

**Costs:**
- Slower INSERT/UPDATE/DELETE operations
- Additional disk space
- Index maintenance overhead

#### 3. Be able to write CREATE/DROP INDEX queries

```sql
-- Create single column index
CREATE INDEX idx_name ON table(column);

-- Create unique index
CREATE UNIQUE INDEX idx_email ON users(email);

-- Create composite index
CREATE INDEX idx_multi ON table(col1, col2, col3);

-- Drop index
DROP INDEX idx_name;
```

#### 4. Understand B-Tree, Hash, and Bitmap indexing concepts
- **B-Tree**: General purpose, range queries, most common
- **Hash**: Exact matches only, very fast, no ranges
- **Bitmap**: Low-cardinality columns, data warehousing

#### 5. Real-world scenarios
- Optimizing **search queries** (WHERE clauses)
- Speeding up **JOINs** between tables
- Improving **ORDER BY** and **GROUP BY** performance
- Preventing duplicate entries with unique indexes
- Full-text search in document management systems

#### 6. Common Interview Questions
- "When would you NOT use an index?"
  → Small tables, low-selectivity columns, write-heavy tables
  
- "What's the difference between clustered and non-clustered indexes?"
  → Clustered determines physical order, only one per table
  
- "Why can indexes slow down INSERT operations?"
  → Each insert must update all indexes on the table
  
- "What is a composite index and when should you use it?"
  → Multiple columns indexed together, use for multi-column WHERE clauses

#### 7. Performance Analysis
Be familiar with EXPLAIN/EXPLAIN ANALYZE to:
- Identify whether index is used
- Spot full table scans
- Compare query plans
- Measure query costs

---

### I. Advanced Indexing Concepts

#### 1. Index Selectivity
Formula: Unique values / Total rows
- High selectivity (0.9-1.0): Excellent for indexing
- Low selectivity (0.0-0.1): Poor for indexing

#### 2. Index Cardinality
Number of unique values in indexed column
- High cardinality: Good for indexing (emails, IDs)
- Low cardinality: Bad for indexing (gender, boolean)

#### 3. Covering Indexes
Index contains all columns needed by query, avoiding table access entirely.

#### 4. Index-Only Scans
Query satisfied entirely from index without touching table data.

#### 5. Index Merge
Database combines multiple indexes to answer a single query.

```sql
-- Might use index merge on age and salary indexes
SELECT * FROM employees 
WHERE age > 30 OR salary > 80000;
```

#### 6. Partial Indexes (PostgreSQL)
Index only subset of rows matching a condition.

```sql
CREATE INDEX idx_active_orders 
ON orders(order_date) 
WHERE status = 'active';
```

---

### Summary

Database indexes are essential for query performance but come with trade-offs. Understanding index types, when to use them, and how to monitor their effectiveness is crucial for database optimization. Always balance read performance against write performance and storage costs when designing your indexing strategy.
''',
    codeSnippet: '''
-- ========================================
-- 1. Creating Basic Indexes
-- ========================================

-- Single column index
CREATE INDEX idx_lastname ON employees(last_name);

-- Composite index on multiple columns
CREATE INDEX idx_name ON employees(last_name, first_name);

-- Unique index (prevents duplicates)
CREATE UNIQUE INDEX idx_email ON employees(email);

-- ========================================
-- 2. Creating Indexes with Specific Options
-- ========================================

-- Index with descending order
CREATE INDEX idx_salary_desc ON employees(salary DESC);

-- Partial/Filtered Index (PostgreSQL)
CREATE INDEX idx_active_employees 
ON employees(name) 
WHERE status = 'active';

-- Covering index with included columns (SQL Server)
CREATE INDEX idx_emp_name 
ON employees(name) 
INCLUDE (salary, department);

-- Full-text index for text search
CREATE FULLTEXT INDEX idx_article_content 
ON articles(title, content);

-- ========================================
-- 3. Dropping Indexes
-- ========================================

-- Standard SQL
DROP INDEX idx_lastname;

-- MySQL
DROP INDEX idx_lastname ON employees;

-- SQL Server
DROP INDEX employees.idx_lastname;

-- PostgreSQL
DROP INDEX idx_lastname;

-- ========================================
-- 4. Queries That Benefit from Indexes
-- ========================================

-- Fast with index on emp_id (primary key)
SELECT * FROM employees WHERE emp_id = 101;

-- Fast with index on department
SELECT * FROM employees WHERE department = 'IT';

-- Fast with composite index on (department, salary)
SELECT * FROM employees 
WHERE department = 'IT' AND salary > 50000;

-- Fast with index on order_date
SELECT * FROM orders 
WHERE order_date BETWEEN '2025-01-01' AND '2025-12-31'
ORDER BY order_date;

-- Fast with index on (customer_id, order_date)
SELECT * FROM orders 
WHERE customer_id = 500 
ORDER BY order_date DESC 
LIMIT 10;

-- ========================================
-- 5. Queries That Won't Use Indexes Efficiently
-- ========================================

-- Function on indexed column prevents index use
SELECT * FROM employees 
WHERE YEAR(hire_date) = 2020;
-- Fix: WHERE hire_date BETWEEN '2020-01-01' AND '2020-12-31'

-- Leading wildcard prevents index use
SELECT * FROM customers WHERE name LIKE '%son';
-- Fix: WHERE name LIKE 'son%' (if pattern allows)

-- OR conditions may not use index efficiently
SELECT * FROM employees 
WHERE first_name = 'John' OR last_name = 'Smith';
-- May need index on both columns

-- Using NOT can prevent index use
SELECT * FROM employees WHERE department != 'IT';
-- Full table scan more likely

-- ========================================
-- 6. Analyzing Index Usage
-- ========================================

-- MySQL: Explain query execution plan
EXPLAIN SELECT * FROM employees WHERE emp_id = 101;

-- Detailed MySQL analysis
EXPLAIN FORMAT=JSON SELECT * FROM employees WHERE department = 'IT';

-- PostgreSQL: Explain with actual execution
EXPLAIN ANALYZE SELECT * FROM employees WHERE emp_id = 101;

-- SQL Server: Show execution plan
SET SHOWPLAN_ALL ON;
SELECT * FROM employees WHERE emp_id = 101;
SET SHOWPLAN_ALL OFF;

-- Check index statistics (MySQL)
SHOW INDEX FROM employees;

-- ========================================
-- 7. Full-Text Search Examples
-- ========================================

-- Create full-text index
CREATE FULLTEXT INDEX idx_blog_search 
ON blog_posts(title, content);

-- Natural language search
SELECT * FROM blog_posts 
WHERE MATCH(title, content) 
AGAINST('database optimization' IN NATURAL LANGUAGE MODE);

-- Boolean mode search
SELECT * FROM blog_posts 
WHERE MATCH(title, content) 
AGAINST('+mysql -postgresql' IN BOOLEAN MODE);

-- With relevance score
SELECT title, 
       MATCH(title, content) AGAINST('database') AS relevance
FROM blog_posts
WHERE MATCH(title, content) AGAINST('database')
ORDER BY relevance DESC;

-- ========================================
-- 8. Composite Index Examples
-- ========================================

-- Create composite index
CREATE INDEX idx_search 
ON products(category, price, stock_quantity);

-- Queries that can use this index efficiently:

-- Uses full index (category, price, stock)
SELECT * FROM products 
WHERE category = 'Electronics' 
  AND price < 1000 
  AND stock_quantity > 0;

-- Uses index on (category, price)
SELECT * FROM products 
WHERE category = 'Electronics' 
  AND price < 1000;

-- Uses index on (category) only
SELECT * FROM products 
WHERE category = 'Electronics';

-- CANNOT use index efficiently (doesn't start with category)
SELECT * FROM products 
WHERE price < 1000;

-- ========================================
-- 9. Index Maintenance
-- ========================================

-- Rebuild fragmented indexes (MySQL)
OPTIMIZE TABLE employees;
ANALYZE TABLE employees;

-- Rebuild all indexes (SQL Server)
ALTER INDEX ALL ON employees REBUILD;

-- Rebuild specific index (SQL Server)
ALTER INDEX idx_name ON employees REBUILD;

-- Reindex table (PostgreSQL)
REINDEX TABLE employees;

-- Update index statistics (MySQL)
ANALYZE TABLE employees;

-- ========================================
-- 10. Finding Missing and Unused Indexes
-- ========================================

-- MySQL: Find tables without indexes
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'your_database' 
  AND table_name NOT IN (
    SELECT DISTINCT table_name 
    FROM information_schema.statistics 
    WHERE table_schema = 'your_database'
  );

-- PostgreSQL: Find unused indexes
SELECT schemaname, tablename, indexname, idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexname NOT LIKE 'pg_%'
ORDER BY idx_scan;

-- SQL Server: Find missing indexes
SELECT 
    migs.avg_total_user_cost * (migs.avg_user_impact / 100.0) * (migs.user_seeks + migs.user_scans) AS improvement_measure,
    mid.statement AS table_name,
    mid.equality_columns,
    mid.inequality_columns,
    mid.included_columns
FROM sys.dm_db_missing_index_details AS mid
JOIN sys.dm_db_missing_index_groups AS mig ON mid.index_handle = mig.index_handle
JOIN sys.dm_db_missing_index_group_stats AS migs ON mig.index_group_handle = migs.group_handle
ORDER BY improvement_measure DESC;

-- ========================================
-- 11. Before/After Index Performance
-- ========================================

-- Without index (slow on large table)
SELECT * FROM orders 
WHERE customer_id = 1000 
  AND order_date > '2025-01-01';
-- Execution time: 2.5 seconds (full table scan)

-- Create index
CREATE INDEX idx_customer_date 
ON orders(customer_id, order_date);

-- With index (fast)
SELECT * FROM orders 
WHERE customer_id = 1000 
  AND order_date > '2025-01-01';
-- Execution time: 0.02 seconds (index scan)

-- ========================================
-- 12. Index Size and Statistics
-- ========================================

-- Check index size (MySQL)
SELECT 
    table_name,
    index_name,
    ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) AS size_mb
FROM mysql.innodb_index_stats
WHERE database_name = 'your_database'
  AND stat_name = 'size'
ORDER BY size_mb DESC;

-- PostgreSQL: Index size
SELECT 
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
ORDER BY pg_relation_size(indexrelid) DESC;
''',
    revisionPoints: [
      'A database index is a special data structure that speeds up data retrieval by acting like a table of contents',
      'Primary key indexes are automatically created and ensure uniqueness and fast access',
      'Unique indexes prevent duplicate values in indexed columns',
      'Clustered index determines physical storage order of table data (only one per table)',
      'Non-clustered indexes create separate structures with pointers to data (multiple allowed per table)',
      'Composite indexes span multiple columns and are most effective when queries filter on those columns in order',
      'Full-text indexes are optimized for searching keywords in large text columns',
      'B-Tree indexes are most common, efficient for range queries and ordered retrieval with O(log n) complexity',
      'Hash indexes use hash functions for exact matches (O(1)) but cannot handle range queries',
      'Bitmap indexes use bit arrays for low-cardinality columns and are efficient in data warehousing',
      'Indexes speed up SELECT, JOIN, WHERE, ORDER BY, and GROUP BY operations',
      'Indexes slow down INSERT, UPDATE, and DELETE operations because all indexes must be updated',
      'Each index consumes additional disk space',
      'Avoid indexing small tables, low-selectivity columns (few unique values), and write-heavy tables',
      'Composite index on (A, B, C) can be used for queries filtering on A, (A,B), or (A,B,C) but not efficiently for B or C alone',
      'Functions on indexed columns prevent index usage: YEAR(date) instead use BETWEEN',
      'Leading wildcards prevent index usage: LIKE "%text" instead use LIKE "text%"',
      'Use EXPLAIN or EXPLAIN ANALYZE to check if queries use indexes efficiently',
      'Index selectivity = unique values / total rows; higher selectivity means better index performance',
      'Covering indexes include all columns needed by query, avoiding table access (index-only scan)',
      'Regularly rebuild indexes to remove fragmentation and update statistics',
      'Monitor index usage to identify unused indexes (waste space) and missing indexes (slow queries)',
    ],
    quizQuestions: [
      Question(
        question: 'What is the main purpose of indexing in a database?',
        options: ['To save disk space', 'To improve query performance', 'To encrypt data', 'To normalize tables'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which of the following is NOT a common index type?',
        options: ['B-tree index', 'Hash index', 'Sequence index', 'Bitmap index'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which operation is typically NOT improved by an index?',
        options: ['SELECT with WHERE clause', 'ORDER BY on indexed column', 'INSERT operation', 'Joining tables on indexed columns'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a covering index?',
        options: [
          'An index that includes all columns from the table',
          'An index that contains all columns referenced in a query',
          'An index created on every column of a table',
          'An index that spans multiple tables'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'When should you generally avoid creating an index?',
        options: [
          'On small tables with few rows',
          'On columns used in WHERE clauses',
          'On columns used for joining tables',
          'On columns used in ORDER BY clauses'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'What is the difference between a clustered and non-clustered index?',
        options: [
          'Clustered is faster than non-clustered',
          'Clustered determines physical row order, non-clustered creates separate structure',
          'Clustered allows duplicates, non-clustered does not',
          'There is no difference'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which type of index is most efficient for exact match queries but cannot handle range queries?',
        options: ['B-tree index', 'Hash index', 'Bitmap index', 'Full-text index'],
        correctIndex: 1,
      ),
      Question(
        question: 'Given a composite index on (name, age, city), which query can use the index most efficiently?',
        options: [
          'WHERE age = 25 AND city = "NYC"',
          'WHERE city = "NYC"',
          'WHERE name = "John" AND age = 25',
          'WHERE age = 25'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Why would the query "SELECT * FROM users WHERE YEAR(birthdate) = 1990" not use an index on birthdate?',
        options: [
          'YEAR function is too complex',
          'Functions on indexed columns prevent index usage',
          'The index is corrupted',
          'YEAR function is not supported in SQL'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is index selectivity?',
        options: [
          'The speed of index access',
          'The size of the index in bytes',
          'The ratio of unique values to total rows',
          'The number of indexes on a table'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which SQL command is used to check if a query uses an index?',
        options: ['SHOW INDEX', 'EXPLAIN', 'INDEX USAGE', 'CHECK INDEX'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a disadvantage of having too many indexes on a table?',
        options: [
          'Queries become slower',
          'INSERT, UPDATE, and DELETE operations become slower',
          'The table becomes read-only',
          'Primary key stops working'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'db_design',
    title: '6. Database Design & Modeling',
    explanation: '''## Database Design & Modeling

### A. Introduction

**Definition:**

**Database Design** is the process of **structuring a database** to store, organize, and manage data efficiently. **Database Modeling** involves creating **abstract representations of data and its relationships** using **models and diagrams**.

**Importance:**

- Ensures **data integrity, minimal redundancy, and efficient queries**.
- Essential for **scalability, maintainability, and performance**.
- Forms the **foundation for application development** and **reporting systems**.
- Proper design prevents costly restructuring later in the project lifecycle.
- Facilitates communication between developers, DBAs, and stakeholders.

**Key Objectives:**

- Organize data logically and efficiently
- Minimize data redundancy
- Ensure data consistency and integrity
- Support current and future business requirements
- Optimize query performance
- Enable easy maintenance and updates

---

### B. Database Design Phases

#### 1. Requirement Analysis

**Purpose:** Understand **user requirements, business rules, and data needs**.

**Activities:**
- Interview stakeholders and end-users
- Identify **entities, attributes, and relationships**
- Document business rules and constraints
- Determine data volumes and growth projections
- Analyze current systems and processes

**Questions to Ask:**
- What information needs to be stored?
- Who will use the database and how?
- What reports and queries are needed?
- What are the performance requirements?
- What are the security and compliance needs?

**Deliverables:**
- Requirements document
- List of entities and attributes
- Business rules specification
- Use case descriptions

#### 2. Conceptual Design

**Purpose:** Create high-level **Entity-Relationship (ER) diagrams** independent of any DBMS.

**Activities:**
- Create **Entity-Relationship (ER) diagrams**
- Define **entities, attributes, relationships, and constraints**
- Identify entity types and their attributes
- Determine relationships and cardinalities
- Define domains for attributes

**Key Concepts:**
- **Entity:** Object or concept with independent existence
- **Attributes:** Properties that describe entities
- **Relationships:** Associations between entities
- **Cardinality:** Number of entity instances (1:1, 1:N, M:N)
- **Participation:** Total vs. Partial participation

**Deliverables:**
- ER diagrams
- Entity and relationship descriptions
- Attribute specifications
- Cardinality and participation constraints

#### 3. Logical Design

**Purpose:** Convert conceptual model to **relational schema** suitable for implementation.

**Activities:**
- Convert conceptual model to **relational schema**
- Define **tables, primary keys, foreign keys**
- Apply **normalization** to eliminate redundancy
- Specify data types for columns
- Define integrity constraints

**Transformation Rules:**
- Each entity → Table
- Each attribute → Column
- Each 1:1 relationship → Foreign key or merged table
- Each 1:N relationship → Foreign key in "many" side
- Each M:N relationship → Junction/Bridge table

**Deliverables:**
- Table schemas with columns and data types
- Primary key and foreign key definitions
- Normalization documentation
- Constraint specifications

#### 4. Physical Design

**Purpose:** Implement **database on a specific DBMS** with optimization.

**Activities:**
- Implement **database on a specific DBMS** (MySQL, PostgreSQL, Oracle, etc.)
- Optimize **storage, indexing, partitioning**
- Design **query performance** strategies
- Define file organizations and access paths
- Plan for backup and recovery

**Considerations:**
- Index creation strategies
- Partitioning for large tables
- Denormalization for read-heavy systems
- Storage allocation
- Security and access control
- Backup and disaster recovery plans

**Deliverables:**
- Physical schema with DBMS-specific features
- Index definitions
- Partitioning strategies
- Performance tuning documentation
- Security and backup plans

---

### C. Entity-Relationship (ER) Modeling

**Components:**

#### 1. Entity
Object or concept with independent existence (e.g., Student, Course, Employee).

**Types:**
- **Strong Entity:** Exists independently (has its own primary key)
- **Weak Entity:** Depends on another entity for existence

**Notation:**
- Rectangle represents entity
- Entity name in singular form

#### 2. Attributes
Properties of entities (e.g., student_id, name, email).

**Types:**
- **Simple:** Cannot be divided (e.g., age)
- **Composite:** Can be divided (e.g., full_name → first_name, last_name)
- **Single-valued:** One value per entity (e.g., date_of_birth)
- **Multi-valued:** Multiple values (e.g., phone_numbers)
- **Derived:** Calculated from other attributes (e.g., age from date_of_birth)

**Notation:**
- Oval represents attribute
- Underlined for primary key attributes
- Dashed oval for derived attributes
- Double oval for multi-valued attributes

#### 3. Primary Key
Unique identifier for an entity (e.g., student_id).

**Properties:**
- Must be unique for each entity instance
- Cannot be NULL
- Should be stable (not change over time)
- Preferably simple (single attribute)

#### 4. Relationship
Association between entities (e.g., Student **Enrolls in** Course).

**Degree:**
- **Unary (Recursive):** Entity related to itself (Employee manages Employee)
- **Binary:** Between two entities (most common)
- **Ternary:** Between three entities (Student-Course-Instructor)

**Notation:**
- Diamond represents relationship
- Lines connect entities to relationships

#### 5. Cardinality
Defines number of entity occurrences in a relationship.

**Types:**
- **One-to-One (1:1):** Each A relates to at most one B, and vice versa
  - Example: Person-Passport
- **One-to-Many (1:N):** Each A relates to many B, but each B relates to one A
  - Example: Department-Employee
- **Many-to-Many (M:N):** Each A relates to many B, and vice versa
  - Example: Student-Course

**Example ER Diagram:**

```
Student(student_id, name, email, date_of_birth)
Course(course_id, title, credits, department)
Enrolls(student_id, course_id, enrollment_date, grade)
```

**Relationships:**
- **Student Enrolls in Course:** Many-to-Many
  - Implemented via Enrolls junction table
  - student_id: FK to Student
  - course_id: FK to Course
  - Additional attributes: enrollment_date, grade

---

### D. Relational Schema Design

**Transformation from ER to Relational Schema:**

#### 1. Strong Entities → Tables

```sql
-- Entity: Student
CREATE TABLE Student (
    student_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(50) UNIQUE NOT NULL,
    date_of_birth DATE,
    enrollment_year INT
);
```

#### 2. Weak Entities → Tables with Composite Primary Key

```sql
-- Weak Entity: CourseSection (depends on Course)
CREATE TABLE CourseSection (
    course_id INT,
    section_number INT,
    instructor VARCHAR(50),
    schedule VARCHAR(100),
    PRIMARY KEY(course_id, section_number),
    FOREIGN KEY(course_id) REFERENCES Course(course_id) ON DELETE CASCADE
);
```

#### 3. Complete Example: Student-Course System

```sql
-- Strong Entity: Student
CREATE TABLE Student (
    student_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(50) UNIQUE NOT NULL,
    phone VARCHAR(15),
    enrollment_date DATE DEFAULT CURRENT_DATE
);

-- Strong Entity: Course
CREATE TABLE Course (
    course_id INT PRIMARY KEY,
    title VARCHAR(100) NOT NULL,
    credits INT CHECK (credits > 0 AND credits <= 6),
    department VARCHAR(50),
    level VARCHAR(20) CHECK (level IN ('Beginner', 'Intermediate', 'Advanced'))
);

-- Many-to-Many Relationship: Enrolls
CREATE TABLE Enrolls (
    student_id INT,
    course_id INT,
    enrollment_date DATE DEFAULT CURRENT_DATE,
    grade CHAR(2) CHECK (grade IN ('A', 'B', 'C', 'D', 'F', 'I', 'W')),
    semester VARCHAR(20),
    PRIMARY KEY(student_id, course_id, semester),
    FOREIGN KEY(student_id) REFERENCES Student(student_id) ON DELETE CASCADE,
    FOREIGN KEY(course_id) REFERENCES Course(course_id) ON DELETE CASCADE
);

-- One-to-Many Relationship: Department has many Courses
CREATE TABLE Department (
    dept_id INT PRIMARY KEY,
    dept_name VARCHAR(50) NOT NULL,
    building VARCHAR(50),
    budget DECIMAL(12,2)
);

ALTER TABLE Course
ADD dept_id INT,
ADD FOREIGN KEY(dept_id) REFERENCES Department(dept_id);
```

**Notes:**

- **M:N relationships** require a **junction table** (Enrolls)
- Primary keys and foreign keys maintain **data integrity**
- **ON DELETE CASCADE:** Automatically delete related records
- **ON DELETE SET NULL:** Set FK to NULL when parent deleted
- **CHECK constraints:** Validate data at insertion/update time

---

### E. Normalization (Part of Design)

**Purpose:** Eliminates **data redundancy** and **update anomalies**.

#### Normal Forms:

| Normal Form | Requirement                                                  | Purpose                               |
| ----------- | ------------------------------------------------------------ | ------------------------------------- |
| **1NF**     | No repeating groups; atomic attributes                       | Eliminate duplicate columns           |
| **2NF**     | 1NF + all non-key attributes depend on **whole primary key** | Eliminate partial dependencies        |
| **3NF**     | 2NF + no transitive dependencies                             | Eliminate transitive dependencies     |
| **BCNF**    | Every determinant is a candidate key                         | Stronger form of 3NF                  |
| **4NF**     | No multi-valued dependencies                                 | Eliminate independent multi-valued    |
| **5NF**     | No join dependencies                                         | Eliminate redundancy from joining     |

#### Example Normalization Process:

**Unnormalized (0NF):**
```
StudentCourses(student_id, name, courses)
1, John, "Math, Physics"
2, Alice, "Chemistry"
```

**First Normal Form (1NF):**
```
StudentCourses(student_id, name, course)
1, John, Math
1, John, Physics
2, Alice, Chemistry
```

**Second Normal Form (2NF):**
```
Student(student_id, name)
1, John
2, Alice

Enrollment(student_id, course)
1, Math
1, Physics
2, Chemistry
```

**Third Normal Form (3NF):**
```
Student(student_id, name)
Course(course_id, course_name)
Enrollment(student_id, course_id)
```

---

### F. Keys & Constraints

| Key Type             | Description                                                | Example                                   |
| -------------------- | ---------------------------------------------------------- | ----------------------------------------- |
| **Primary Key (PK)** | Uniquely identifies a row                                  | student_id in Student table               |
| **Foreign Key (FK)** | Maintains referential integrity between tables             | dept_id in Employee referencing Department |
| **Unique Key**       | Ensures column values are unique (allows NULL)             | email in Student table                    |
| **Candidate Key**    | Potential PKs; one chosen as primary                       | student_id or email                       |
| **Composite Key**    | PK made of multiple columns                                | (student_id, course_id) in Enrolls        |
| **Alternate Key**    | Candidate keys not chosen as primary                       | email if student_id is PK                 |
| **Super Key**        | Any combination of columns that uniquely identifies a row  | (student_id, name, email)                 |

#### Constraint Types:

```sql
-- PRIMARY KEY
CREATE TABLE Employee (
    emp_id INT PRIMARY KEY,
    ...
);

-- FOREIGN KEY with referential actions
CREATE TABLE Order (
    order_id INT PRIMARY KEY,
    customer_id INT,
    FOREIGN KEY(customer_id) REFERENCES Customer(customer_id)
        ON DELETE CASCADE
        ON UPDATE CASCADE
);

-- UNIQUE
CREATE TABLE User (
    user_id INT PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(100) UNIQUE
);

-- CHECK
CREATE TABLE Product (
    product_id INT PRIMARY KEY,
    price DECIMAL(10,2) CHECK (price > 0),
    stock_quantity INT CHECK (stock_quantity >= 0),
    category VARCHAR(50) CHECK (category IN ('Electronics', 'Clothing', 'Food'))
);

-- NOT NULL
CREATE TABLE Customer (
    customer_id INT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) NOT NULL
);

-- DEFAULT
CREATE TABLE Order (
    order_id INT PRIMARY KEY,
    order_date DATE DEFAULT CURRENT_DATE,
    status VARCHAR(20) DEFAULT 'Pending'
);
```

---

### G. Advanced Modeling Concepts

#### 1. ERD Notations

**Crow's Foot Notation (most popular):**
- Circle: Zero
- Single line: One
- Crow's foot (three lines): Many
- Example: Department —|<— Employee (One-to-Many)

**UML Notation:**
- Multiplicity expressed as numbers
- Example: Department 1..* —— 0..* Employee

**Chen's Notation:**
- Original ER notation
- Uses 1, N, M for cardinality

#### 2. Subtypes & Supertypes (Inheritance)

**Example: Employee hierarchy**

```sql
-- Supertype
CREATE TABLE Employee (
    emp_id INT PRIMARY KEY,
    name VARCHAR(50),
    hire_date DATE,
    employee_type VARCHAR(20) CHECK (employee_type IN ('Manager', 'Engineer', 'Sales'))
);

-- Subtype: Manager
CREATE TABLE Manager (
    emp_id INT PRIMARY KEY,
    department VARCHAR(50),
    num_reports INT,
    FOREIGN KEY(emp_id) REFERENCES Employee(emp_id)
);

-- Subtype: Engineer
CREATE TABLE Engineer (
    emp_id INT PRIMARY KEY,
    programming_languages VARCHAR(200),
    certifications VARCHAR(200),
    FOREIGN KEY(emp_id) REFERENCES Employee(emp_id)
);
```

**Specialization Strategies:**
- **Overlapping:** Entity can belong to multiple subtypes
- **Disjoint:** Entity belongs to only one subtype
- **Total:** Every supertype instance must be in a subtype
- **Partial:** Supertype instances may not be in any subtype

#### 3. Constraints Beyond Keys

```sql
-- CHECK constraints with complex conditions
CREATE TABLE BankAccount (
    account_id INT PRIMARY KEY,
    account_type VARCHAR(20) CHECK (account_type IN ('Savings', 'Checking', 'Credit')),
    balance DECIMAL(15,2),
    min_balance DECIMAL(15,2),
    CONSTRAINT chk_balance CHECK (
        (account_type = 'Savings' AND balance >= min_balance) OR
        (account_type != 'Savings')
    )
);

-- UNIQUE constraint on multiple columns
CREATE TABLE Course Offering (
    course_id INT,
    semester VARCHAR(20),
    year INT,
    section_number INT,
    UNIQUE(course_id, semester, year, section_number)
);
```

#### 4. Indexing Strategy

Plan indexes during design phase for:
- Primary keys (automatic)
- Foreign keys (for joins)
- Frequently searched columns (WHERE clauses)
- Columns used in ORDER BY
- Columns used in GROUP BY

#### 5. Data Types

**Use appropriate types for storage and performance:**

```sql
-- Numeric
INT, BIGINT, DECIMAL(p,s), FLOAT, DOUBLE

-- String
CHAR(n), VARCHAR(n), TEXT

-- Date/Time
DATE, TIME, DATETIME, TIMESTAMP

-- Boolean
BOOLEAN, TINYINT(1)

-- Binary
BLOB, VARBINARY

-- JSON (modern databases)
JSON
```

---

### H. Best Practices

#### 1. Start with clear requirement analysis
- Avoid costly redesign later
- Document all requirements
- Validate with stakeholders early

#### 2. Normalize data
Balance **normalization for data integrity** with **denormalization for read-heavy applications**.

**When to Normalize:**
- Transactional systems (OLTP)
- Data integrity is critical
- Frequent updates

**When to Denormalize:**
- Read-heavy applications
- Reporting and analytics (OLAP)
- Performance is critical

#### 3. Use meaningful table and column names

**Good Naming Conventions:**
```sql
-- Clear, descriptive names
Customer, Order, OrderItem, Product

-- Use singular for table names (debatable, be consistent)
Employee vs Employees

-- Use underscores for readability
customer_address, order_total, date_of_birth

-- Avoid reserved keywords
-- Bad: user, select, order
-- Good: app_user, customer_order
```

#### 4. Define proper keys and constraints
- Always define PRIMARY KEY
- Use FOREIGN KEY for referential integrity
- Add UNIQUE where needed
- Use CHECK constraints for validation
- Set NOT NULL for mandatory fields

#### 5. Document ER diagrams and schema
- Create and maintain ER diagrams
- Document business rules
- Explain design decisions
- Keep documentation up-to-date

#### 6. Plan for future growth
- Consider **partitioning** for large tables
- Plan **indexing strategy**
- Design for scalability
- Leave room for new relationships
- Use extensible designs

#### 7. Review with stakeholders
- Ensure model aligns with business rules
- Validate with actual users
- Get feedback early and often
- Prototype key queries

#### 8. Additional Best Practices
- Use consistent naming conventions
- Avoid over-designing (YAGNI principle)
- Consider temporal data (effective dates)
- Plan for auditing and logging
- Design with security in mind
- Test with realistic data volumes

---

### I. Exam Tips

#### 1. Be able to draw ER diagrams
- Identify entities from requirements
- Determine relationships and cardinalities
- Distinguish between 1:1, 1:N, and M:N relationships
- Show primary keys and attributes clearly

**Practice Scenarios:**
- Online bookstore (Customer, Book, Order, Author)
- Hospital management (Patient, Doctor, Appointment, Department)
- University system (Student, Course, Professor, Enrollment)

#### 2. Understand M:N relationship implementation
- Always requires a junction/bridge table
- Junction table has composite primary key
- Contains foreign keys to both entities
- Can have additional attributes (enrollment_date, grade)

**Example:**
```
Student ←→ Enrolls ←→ Course
(M:N implemented via junction table)
```

#### 3. Know normalization rules
- Be able to convert unnormalized tables to 1NF, 2NF, 3NF
- Identify partial dependencies (2NF)
- Identify transitive dependencies (3NF)
- Explain benefits: eliminate redundancy, prevent anomalies

#### 4. Explain keys with examples
- **Primary Key:** Unique, non-null identifier
- **Foreign Key:** References primary key in another table
- **Unique Key:** Ensures uniqueness, allows NULL
- **Composite Key:** Multiple columns together form key
- **Candidate Key:** Potential primary keys

#### 5. Real-world scenarios
Common exam questions:
- Design a database for an online shopping system
- Model a library management system
- Create schema for a social media platform
- Design a booking/reservation system

**Approach:**
1. Identify entities from description
2. List attributes for each entity
3. Determine relationships and cardinalities
4. Draw ER diagram
5. Convert to relational schema
6. Apply normalization
7. Define constraints

#### 6. Common Interview Questions
- "How do you handle many-to-many relationships?"
  → Junction table with composite key
  
- "What's the difference between 2NF and 3NF?"
  → 2NF removes partial dependencies, 3NF removes transitive
  
- "When would you denormalize a database?"
  → For read-heavy systems, reporting, performance optimization
  
- "How do you design for scalability?"
  → Partitioning, indexing, denormalization, caching

---

### Summary

Database design and modeling is a critical skill that combines theory (ER modeling, normalization) with practice (schema creation, constraint definition). A well-designed database ensures data integrity, optimal performance, and easy maintenance. Always start with thorough requirements analysis, create clear ER diagrams, apply normalization appropriately, and document your design decisions for future reference.
''',
    codeSnippet: '''
-- ========================================
-- 1. Complete Student-Course Database Design
-- ========================================

-- Create Department table (One-to-Many with Course)
CREATE TABLE Department (
    dept_id INT PRIMARY KEY,
    dept_name VARCHAR(50) NOT NULL UNIQUE,
    building VARCHAR(50),
    budget DECIMAL(12,2) CHECK (budget > 0),
    established_year INT
);

-- Create Student table (Strong Entity)
CREATE TABLE Student (
    student_id INT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    phone VARCHAR(15),
    date_of_birth DATE,
    enrollment_date DATE DEFAULT CURRENT_DATE,
    major_dept_id INT,
    FOREIGN KEY(major_dept_id) REFERENCES Department(dept_id)
);

-- Create Instructor table (Strong Entity)
CREATE TABLE Instructor (
    instructor_id INT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    dept_id INT,
    hire_date DATE,
    salary DECIMAL(10,2),
    FOREIGN KEY(dept_id) REFERENCES Department(dept_id)
);

-- Create Course table (Strong Entity)
CREATE TABLE Course (
    course_id INT PRIMARY KEY,
    course_code VARCHAR(10) UNIQUE NOT NULL,
    title VARCHAR(100) NOT NULL,
    credits INT CHECK (credits BETWEEN 1 AND 6),
    dept_id INT,
    level VARCHAR(20) CHECK (level IN ('Beginner', 'Intermediate', 'Advanced')),
    description TEXT,
    FOREIGN KEY(dept_id) REFERENCES Department(dept_id)
);

-- Create CourseOffering table (Weak Entity depending on Course)
CREATE TABLE CourseOffering (
    offering_id INT PRIMARY KEY,
    course_id INT NOT NULL,
    instructor_id INT,
    semester VARCHAR(20) NOT NULL,
    year INT NOT NULL,
    section_number INT NOT NULL,
    max_enrollment INT CHECK (max_enrollment > 0),
    room VARCHAR(20),
    schedule VARCHAR(100),
    UNIQUE(course_id, semester, year, section_number),
    FOREIGN KEY(course_id) REFERENCES Course(course_id) ON DELETE CASCADE,
    FOREIGN KEY(instructor_id) REFERENCES Instructor(instructor_id) ON DELETE SET NULL
);

-- Create Enrollment table (Many-to-Many junction table)
CREATE TABLE Enrollment (
    enrollment_id INT PRIMARY KEY AUTO_INCREMENT,
    student_id INT NOT NULL,
    offering_id INT NOT NULL,
    enrollment_date DATE DEFAULT CURRENT_DATE,
    grade CHAR(2) CHECK (grade IN ('A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D', 'F', 'I', 'W')),
    status VARCHAR(20) DEFAULT 'Active' CHECK (status IN ('Active', 'Completed', 'Withdrawn', 'Failed')),
    UNIQUE(student_id, offering_id),
    FOREIGN KEY(student_id) REFERENCES Student(student_id) ON DELETE CASCADE,
    FOREIGN KEY(offering_id) REFERENCES CourseOffering(offering_id) ON DELETE CASCADE
);

-- Create Prerequisites table (Self-referencing Many-to-Many)
CREATE TABLE Prerequisite (
    course_id INT,
    prerequisite_course_id INT,
    PRIMARY KEY(course_id, prerequisite_course_id),
    FOREIGN KEY(course_id) REFERENCES Course(course_id) ON DELETE CASCADE,
    FOREIGN KEY(prerequisite_course_id) REFERENCES Course(course_id) ON DELETE CASCADE
);

-- ========================================
-- 2. Insert Sample Data
-- ========================================

-- Insert Departments
INSERT INTO Department VALUES 
(1, 'Computer Science', 'Tech Building', 500000.00, 1995),
(2, 'Mathematics', 'Science Hall', 350000.00, 1990),
(3, 'Physics', 'Research Center', 450000.00, 1992);

-- Insert Instructors
INSERT INTO Instructor VALUES
(101, 'John', 'Smith', 'john.smith@university.edu', 1, '2010-08-15', 85000.00),
(102, 'Mary', 'Johnson', 'mary.j@university.edu', 1, '2015-01-10', 78000.00),
(103, 'Robert', 'Williams', 'r.williams@university.edu', 2, '2012-06-20', 82000.00);

-- Insert Courses
INSERT INTO Course VALUES
(201, 'CS101', 'Introduction to Programming', 4, 1, 'Beginner', 'Learn fundamentals of programming'),
(202, 'CS201', 'Data Structures', 4, 1, 'Intermediate', 'Study of data structures and algorithms'),
(203, 'CS301', 'Database Systems', 3, 1, 'Advanced', 'Relational databases and SQL'),
(204, 'MATH101', 'Calculus I', 4, 2, 'Beginner', 'Differential calculus');

-- Insert Prerequisites
INSERT INTO Prerequisite VALUES
(202, 201),  -- CS201 requires CS101
(203, 202);  -- CS301 requires CS201

-- Insert Students
INSERT INTO Student VALUES
(1001, 'Alice', 'Brown', 'alice.b@student.edu', '555-0101', '2003-05-15', '2021-09-01', 1),
(1002, 'Bob', 'Davis', 'bob.d@student.edu', '555-0102', '2002-08-22', '2020-09-01', 1),
(1003, 'Carol', 'Wilson', 'carol.w@student.edu', '555-0103', '2003-03-10', '2021-09-01', 2);

-- Insert Course Offerings
INSERT INTO CourseOffering VALUES
(3001, 201, 101, 'Fall', 2024, 1, 30, 'TB-101', 'MWF 9:00-10:00'),
(3002, 202, 102, 'Fall', 2024, 1, 25, 'TB-102', 'TTh 10:30-12:00'),
(3003, 203, 101, 'Spring', 2025, 1, 20, 'TB-201', 'MWF 14:00-15:30');

-- Insert Enrollments
INSERT INTO Enrollment (student_id, offering_id, enrollment_date, grade, status) VALUES
(1001, 3001, '2024-08-25', 'A', 'Completed'),
(1001, 3002, '2024-08-25', 'B+', 'Completed'),
(1002, 3001, '2024-08-26', 'B', 'Completed'),
(1003, 3001, '2024-08-27', 'A-', 'Completed');

-- ========================================
-- 3. Queries Demonstrating Relationships
-- ========================================

-- Find all students enrolled in a specific course
SELECT s.student_id, s.first_name, s.last_name, e.grade
FROM Student s
JOIN Enrollment e ON s.student_id = e.student_id
JOIN CourseOffering co ON e.offering_id = co.offering_id
JOIN Course c ON co.course_id = c.course_id
WHERE c.course_code = 'CS101';

-- Find all courses taught by an instructor
SELECT c.course_code, c.title, co.semester, co.year, co.section_number
FROM Course c
JOIN CourseOffering co ON c.course_id = co.course_id
JOIN Instructor i ON co.instructor_id = i.instructor_id
WHERE i.email = 'john.smith@university.edu';

-- Find prerequisites for a course
SELECT c1.course_code AS course, c2.course_code AS prerequisite, c2.title
FROM Course c1
JOIN Prerequisite p ON c1.course_id = p.course_id
JOIN Course c2 ON p.prerequisite_course_id = c2.course_id
WHERE c1.course_code = 'CS301';

-- Calculate average grade per course
SELECT c.course_code, c.title, AVG(
    CASE e.grade
        WHEN 'A' THEN 4.0
        WHEN 'A-' THEN 3.7
        WHEN 'B+' THEN 3.3
        WHEN 'B' THEN 3.0
        WHEN 'B-' THEN 2.7
        WHEN 'C+' THEN 2.3
        WHEN 'C' THEN 2.0
        WHEN 'C-' THEN 1.7
        WHEN 'D' THEN 1.0
        WHEN 'F' THEN 0.0
    END
) AS avg_gpa
FROM Course c
JOIN CourseOffering co ON c.course_id = co.course_id
JOIN Enrollment e ON co.offering_id = e.offering_id
WHERE e.grade IS NOT NULL AND e.grade NOT IN ('I', 'W')
GROUP BY c.course_id, c.course_code, c.title;

-- ========================================
-- 4. Normalization Example
-- ========================================

-- Unnormalized (0NF) - Bad Design
CREATE TABLE StudentCourses_Unnormalized (
    student_id INT,
    student_name VARCHAR(100),
    courses VARCHAR(500),  -- "Math, Physics, Chemistry"
    grades VARCHAR(100)    -- "A, B, A"
);

-- First Normal Form (1NF) - Remove repeating groups
CREATE TABLE StudentCourses_1NF (
    student_id INT,
    student_name VARCHAR(100),
    course VARCHAR(100),
    grade CHAR(2)
);

-- Second Normal Form (2NF) - Remove partial dependencies
CREATE TABLE Students_2NF (
    student_id INT PRIMARY KEY,
    student_name VARCHAR(100)
);

CREATE TABLE StudentCourses_2NF (
    student_id INT,
    course VARCHAR(100),
    grade CHAR(2),
    PRIMARY KEY(student_id, course)
);

-- Third Normal Form (3NF) - Remove transitive dependencies
CREATE TABLE Students_3NF (
    student_id INT PRIMARY KEY,
    student_name VARCHAR(100)
);

CREATE TABLE Courses_3NF (
    course_id INT PRIMARY KEY,
    course_name VARCHAR(100)
);

CREATE TABLE Enrollments_3NF (
    student_id INT,
    course_id INT,
    grade CHAR(2),
    PRIMARY KEY(student_id, course_id),
    FOREIGN KEY(student_id) REFERENCES Students_3NF(student_id),
    FOREIGN KEY(course_id) REFERENCES Courses_3NF(course_id)
);

-- ========================================
-- 5. Inheritance Example (Supertype/Subtype)
-- ========================================

-- Supertype: Employee
CREATE TABLE Employee (
    emp_id INT PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    hire_date DATE,
    employee_type VARCHAR(20) CHECK (employee_type IN ('Faculty', 'Staff', 'Administrator'))
);

-- Subtype: Faculty (Instructors)
CREATE TABLE Faculty (
    emp_id INT PRIMARY KEY,
    dept_id INT,
    rank VARCHAR(30) CHECK (rank IN ('Lecturer', 'Assistant Professor', 'Associate Professor', 'Professor')),
    tenure_status BOOLEAN,
    research_area VARCHAR(100),
    FOREIGN KEY(emp_id) REFERENCES Employee(emp_id) ON DELETE CASCADE,
    FOREIGN KEY(dept_id) REFERENCES Department(dept_id)
);

-- Subtype: Staff
CREATE TABLE Staff (
    emp_id INT PRIMARY KEY,
    dept_id INT,
    job_title VARCHAR(50),
    supervisor_id INT,
    FOREIGN KEY(emp_id) REFERENCES Employee(emp_id) ON DELETE CASCADE,
    FOREIGN KEY(dept_id) REFERENCES Department(dept_id),
    FOREIGN KEY(supervisor_id) REFERENCES Employee(emp_id)
);

-- ========================================
-- 6. Additional Constraints Examples
-- ========================================

-- Complex CHECK constraints
CREATE TABLE GradeBook (
    grade_id INT PRIMARY KEY,
    student_id INT,
    course_id INT,
    midterm_score DECIMAL(5,2) CHECK (midterm_score BETWEEN 0 AND 100),
    final_score DECIMAL(5,2) CHECK (final_score BETWEEN 0 AND 100),
    total_score DECIMAL(5,2),
    letter_grade CHAR(2),
    CONSTRAINT chk_total CHECK (total_score = (midterm_score * 0.4 + final_score * 0.6)),
    CONSTRAINT chk_grade CHECK (
        (letter_grade = 'A' AND total_score >= 90) OR
        (letter_grade = 'B' AND total_score >= 80 AND total_score < 90) OR
        (letter_grade = 'C' AND total_score >= 70 AND total_score < 80) OR
        (letter_grade = 'D' AND total_score >= 60 AND total_score < 70) OR
        (letter_grade = 'F' AND total_score < 60)
    )
);

-- Temporal data with effective dates
CREATE TABLE StudentMajor (
    student_id INT,
    dept_id INT,
    effective_date DATE NOT NULL,
    end_date DATE,
    is_current BOOLEAN DEFAULT TRUE,
    PRIMARY KEY(student_id, effective_date),
    FOREIGN KEY(student_id) REFERENCES Student(student_id),
    FOREIGN KEY(dept_id) REFERENCES Department(dept_id),
    CHECK (end_date IS NULL OR end_date > effective_date)
);
''',
    revisionPoints: [
      'Database design is the process of structuring a database to store, organize, and manage data efficiently',
      'Four phases of database design: Requirement Analysis, Conceptual Design, Logical Design, and Physical Design',
      'Requirement analysis identifies entities, attributes, relationships, and business rules',
      'Conceptual design creates Entity-Relationship (ER) diagrams independent of any DBMS',
      'Logical design converts ER model to relational schema with tables, keys, and normalization',
      'Physical design implements database on specific DBMS with indexing, partitioning, and optimization',
      'ER model components: Entity (object), Attribute (property), Primary Key (unique identifier), Relationship (association), Cardinality (1:1, 1:N, M:N)',
      'Strong entities exist independently with their own primary keys',
      'Weak entities depend on other entities for existence',
      'Attribute types: Simple, Composite, Single-valued, Multi-valued, Derived',
      'Relationship degrees: Unary (recursive), Binary (most common), Ternary (three entities)',
      'Cardinality types: One-to-One (1:1), One-to-Many (1:N), Many-to-Many (M:N)',
      'Many-to-Many relationships require junction/bridge tables with composite primary keys',
      'Transformation rules: Entity → Table, Attribute → Column, 1:1 → FK or merge, 1:N → FK in many side, M:N → Junction table',
      'Normalization eliminates redundancy: 1NF (atomic values), 2NF (no partial dependencies), 3NF (no transitive dependencies)',
      'Primary Key uniquely identifies rows, cannot be NULL, should be stable',
      'Foreign Key maintains referential integrity between tables',
      'Composite Key is a primary key made of multiple columns',
      'ON DELETE CASCADE automatically deletes related records, ON DELETE SET NULL sets FK to NULL',
      'Supertype/Subtype implements inheritance: Employee → Faculty, Staff',
      'Use meaningful naming conventions and document ER diagrams and schemas',
      'Balance normalization for data integrity with denormalization for read-heavy systems',
    ],
    quizQuestions: [
      Question(
        question: 'What is the first step in database design?',
        options: ['Creating tables', 'Requirements analysis', 'Writing queries', 'Creating indexes'],
        correctIndex: 1,
      ),
      Question(
        question: 'In ER modeling, what is an entity?',
        options: [
          'A property of an object',
          'An object or concept with independent existence',
          'A relationship between tables',
          'A database constraint'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'How is a Many-to-Many relationship implemented in a relational database?',
        options: [
          'Using a foreign key in one table',
          'Using a junction/bridge table',
          'Merging the two tables into one',
          'Using a unique constraint'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does cardinality in ER modeling represent?',
        options: [
          'The number of attributes in an entity',
          'The size of a database table',
          'The number of entity occurrences in a relationship',
          'The depth of relationship hierarchy'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which normal form eliminates partial dependencies?',
        options: ['First Normal Form (1NF)', 'Second Normal Form (2NF)', 'Third Normal Form (3NF)', 'BCNF'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a weak entity in ER modeling?',
        options: [
          'An entity with few attributes',
          'An entity that depends on another entity for existence',
          'An entity with a small number of records',
          'An entity without relationships'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'In the transformation from ER to relational schema, what does a 1:N relationship become?',
        options: [
          'A junction table',
          'A foreign key in the "one" side',
          'A foreign key in the "many" side',
          'Two separate tables with no connection'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a composite key?',
        options: [
          'A key made of multiple tables',
          'A primary key made of multiple columns',
          'A foreign key referencing multiple tables',
          'A key that can be NULL'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which phase of database design creates ER diagrams?',
        options: ['Requirement Analysis', 'Conceptual Design', 'Logical Design', 'Physical Design'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does ON DELETE CASCADE mean in a foreign key constraint?',
        options: [
          'Prevents deletion of parent records',
          'Automatically deletes related child records when parent is deleted',
          'Sets foreign key to NULL when parent is deleted',
          'Creates a backup before deletion'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'When should you consider denormalization?',
        options: [
          'Always, to simplify the database',
          'For read-heavy applications where performance is critical',
          'Never, it violates database principles',
          'Only for small databases'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a derived attribute in ER modeling?',
        options: [
          'An attribute inherited from a supertype',
          'An attribute calculated from other attributes',
          'An attribute stored in multiple tables',
          'An attribute that is a foreign key'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'advanced_sql',
    title: '7. Advanced SQL & Query Optimization',
    explanation: '''## Advanced SQL & Query Optimization

### A. Introduction

**Definition:**

**Advanced SQL** involves complex queries beyond simple `SELECT` statements, including **joins, subqueries, window functions, and set operations**.

**Query Optimization** ensures **SQL queries execute efficiently**, minimizing **execution time and resource usage**.

**Importance:**

- Critical for **large databases and high-traffic applications**.
- Improves **application performance**, **reduces server load**, and **enhances scalability**.
- Essential skill for database developers, data analysts, and backend engineers.
- Can dramatically reduce query execution time from minutes to seconds.
- Reduces infrastructure costs by using resources more efficiently.

**Key Objectives:**

- Write efficient, optimized SQL queries
- Minimize database load and resource consumption
- Improve response times for end-users
- Handle large datasets effectively
- Scale applications to support growing data volumes

---

### B. Advanced SQL Concepts

#### 1. Joins – Combine data from multiple tables

Joins are fundamental for querying relational databases where data is normalized across multiple tables.

| Join Type              | Description                                                      | Use Case                              |
| ---------------------- | ---------------------------------------------------------------- | ------------------------------------- |
| **INNER JOIN**         | Returns rows with matching values in both tables                 | Most common, find matching records    |
| **LEFT (OUTER) JOIN**  | Returns all rows from left table, matching rows from right table | Include all left records              |
| **RIGHT (OUTER) JOIN** | Returns all rows from right table, matching rows from left table | Include all right records             |
| **FULL OUTER JOIN**    | Returns all rows when there is a match in either table           | Combine all records from both tables  |
| **CROSS JOIN**         | Cartesian product of two tables                                  | Generate all combinations             |
| **SELF JOIN**          | Join table to itself                                             | Hierarchical data, comparisons        |

**Example: INNER JOIN**

```sql
-- Find students enrolled in courses
SELECT s.name, c.title, e.grade
FROM Student s
INNER JOIN Enrolls e ON s.student_id = e.student_id
INNER JOIN Course c ON e.course_id = c.course_id
WHERE e.grade IS NOT NULL;
```

**Example: LEFT JOIN**

```sql
-- Find all students and their enrollments (including students with no enrollments)
SELECT s.student_id, s.name, c.title
FROM Student s
LEFT JOIN Enrolls e ON s.student_id = e.student_id
LEFT JOIN Course c ON e.course_id = c.course_id;
```

**Example: SELF JOIN**

```sql
-- Find employees and their managers
SELECT e1.name AS employee, e2.name AS manager
FROM Employee e1
LEFT JOIN Employee e2 ON e1.manager_id = e2.employee_id;
```

**Example: CROSS JOIN**

```sql
-- Generate all possible student-course combinations
SELECT s.name, c.title
FROM Student s
CROSS JOIN Course c;
```

---

#### 2. Subqueries – Query inside another query

Subqueries can appear in SELECT, FROM, WHERE, or HAVING clauses.

**Types:**

- **Scalar subquery:** Returns single value
- **Row subquery:** Returns single row
- **Table subquery:** Returns multiple rows/columns
- **Correlated subquery:** References outer query
- **Non-correlated subquery:** Independent of outer query

**Example: Subquery in WHERE clause**

```sql
-- Find students enrolled in courses with more than 3 credits
SELECT name, email
FROM Student
WHERE student_id IN (
    SELECT student_id
    FROM Enrolls e
    JOIN Course c ON e.course_id = c.course_id
    WHERE c.credits > 3
);
```

**Example: Correlated subquery**

```sql
-- Find students who scored above the average in their course
SELECT s.name, e.grade, c.title
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id
WHERE e.grade > (
    SELECT AVG(grade)
    FROM Enrolls e2
    WHERE e2.course_id = e.course_id
);
```

**Example: Subquery in SELECT**

```sql
-- Show each student with their total number of enrollments
SELECT s.name,
       (SELECT COUNT(*)
        FROM Enrolls e
        WHERE e.student_id = s.student_id) AS total_enrollments
FROM Student s;
```

---

#### 3. Set Operations – Combine result sets

Set operations combine results from two or more SELECT statements.

| Operation          | Description                               | Removes Duplicates |
| ------------------ | ----------------------------------------- | ------------------ |
| **UNION**          | Combine two queries, removes duplicates   | Yes                |
| **UNION ALL**      | Combine two queries, keeps duplicates     | No                 |
| **INTERSECT**      | Returns common rows                       | Yes                |
| **EXCEPT / MINUS** | Returns rows in first query not in second | Yes                |

**Example: UNION**

```sql
-- Combine active and archived students
SELECT student_id, name FROM ActiveStudents
UNION
SELECT student_id, name FROM ArchivedStudents;
```

**Example: INTERSECT**

```sql
-- Find students enrolled in both Math and Physics
SELECT student_id FROM Enrolls
WHERE course_id = (SELECT course_id FROM Course WHERE title = 'Mathematics')
INTERSECT
SELECT student_id FROM Enrolls
WHERE course_id = (SELECT course_id FROM Course WHERE title = 'Physics');
```

**Example: EXCEPT (or MINUS)**

```sql
-- Find students NOT enrolled in any course
SELECT student_id, name FROM Student
EXCEPT
SELECT DISTINCT s.student_id, s.name
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id;
```

---

#### 4. Window Functions – Perform calculations across rows

Window functions perform calculations across a set of rows related to the current row, without collapsing rows like GROUP BY.

**Common Window Functions:**

| Function          | Description                                    |
| ----------------- | ---------------------------------------------- |
| **ROW_NUMBER()**  | Assigns unique sequential number               |
| **RANK()**        | Assigns rank with gaps for ties                |
| **DENSE_RANK()**  | Assigns rank without gaps                      |
| **NTILE(n)**      | Divides rows into n groups                     |
| **LAG()**         | Access previous row value                      |
| **LEAD()**        | Access next row value                          |
| **SUM() OVER**    | Running/cumulative sum                         |
| **AVG() OVER**    | Moving average                                 |
| **FIRST_VALUE()** | First value in window                          |
| **LAST_VALUE()**  | Last value in window                           |

**Example: Ranking students by grade**

```sql
SELECT name, course_id, grade,
       RANK() OVER(PARTITION BY course_id ORDER BY grade DESC) AS rank,
       DENSE_RANK() OVER(PARTITION BY course_id ORDER BY grade DESC) AS dense_rank
FROM Enrolls e
JOIN Student s ON e.student_id = s.student_id;
```

**Example: Running total**

```sql
SELECT order_date, order_amount,
       SUM(order_amount) OVER(ORDER BY order_date) AS running_total
FROM Orders;
```

**Example: Moving average**

```sql
SELECT date, sales,
       AVG(sales) OVER(ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_7days
FROM DailySales;
```

**Example: LAG and LEAD**

```sql
-- Compare current month sales with previous month
SELECT month, sales,
       LAG(sales, 1) OVER(ORDER BY month) AS prev_month_sales,
       sales - LAG(sales, 1) OVER(ORDER BY month) AS sales_change
FROM MonthlySales;
```

---

#### 5. Common Table Expressions (CTEs)

CTEs improve query readability and allow recursive queries.

**Example: Simple CTE**

```sql
WITH HighPerformers AS (
    SELECT s.student_id, s.name, AVG(e.grade) AS avg_grade
    FROM Student s
    JOIN Enrolls e ON s.student_id = e.student_id
    GROUP BY s.student_id, s.name
    HAVING AVG(e.grade) >= 3.5
)
SELECT * FROM HighPerformers
ORDER BY avg_grade DESC;
```

**Example: Multiple CTEs**

```sql
WITH 
    StudentGrades AS (
        SELECT student_id, AVG(grade) AS avg_grade
        FROM Enrolls
        GROUP BY student_id
    ),
    TopStudents AS (
        SELECT student_id
        FROM StudentGrades
        WHERE avg_grade >= 3.5
    )
SELECT s.name, sg.avg_grade
FROM Student s
JOIN TopStudents ts ON s.student_id = ts.student_id
JOIN StudentGrades sg ON s.student_id = sg.student_id;
```

**Example: Recursive CTE (Employee hierarchy)**

```sql
WITH RECURSIVE EmployeeHierarchy AS (
    -- Base case: top-level managers
    SELECT employee_id, name, manager_id, 1 AS level
    FROM Employee
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive case: employees reporting to previous level
    SELECT e.employee_id, e.name, e.manager_id, eh.level + 1
    FROM Employee e
    JOIN EmployeeHierarchy eh ON e.manager_id = eh.employee_id
)
SELECT * FROM EmployeeHierarchy
ORDER BY level, name;
```

---

#### 6. Aggregate Functions with GROUPING

```sql
-- Find average grade per course
SELECT c.title, AVG(e.grade) AS avg_grade, COUNT(*) AS num_students
FROM Course c
JOIN Enrolls e ON c.course_id = e.course_id
GROUP BY c.course_id, c.title
HAVING AVG(e.grade) >= 3.0
ORDER BY avg_grade DESC;
```

---

### C. Query Optimization Techniques

#### 1. Indexing – Create indexes on frequently queried columns

Indexes speed up data retrieval but slow down inserts/updates.

```sql
-- Create index on frequently searched column
CREATE INDEX idx_student_email ON Student(email);

-- Composite index for multi-column searches
CREATE INDEX idx_enrolls_student_course ON Enrolls(student_id, course_id);

-- Index on foreign keys (improves joins)
CREATE INDEX idx_enrolls_student ON Enrolls(student_id);
CREATE INDEX idx_enrolls_course ON Enrolls(course_id);
```

**When to Index:**
- Columns frequently used in WHERE clauses
- Foreign key columns (for joins)
- Columns used in ORDER BY
- Columns used in GROUP BY

**When NOT to Index:**
- Small tables (full scan is faster)
- Columns with low cardinality (few unique values)
- Tables with frequent inserts/updates

---

#### 2. Use EXPLAIN / EXPLAIN PLAN

Analyze **query execution path** to identify bottlenecks.

```sql
-- MySQL / PostgreSQL
EXPLAIN SELECT * FROM Enrolls WHERE grade = 'A';

-- With more details
EXPLAIN ANALYZE SELECT s.name, c.title
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id;
```

**Key Metrics to Look For:**
- **Execution time**
- **Rows scanned** (lower is better)
- **Index usage** (avoid full table scans)
- **Join type** (prefer nested loop for small tables, hash join for large)

---

#### 3. Avoid SELECT *

Select only **required columns** to reduce I/O and network traffic.

```sql
-- Bad: Retrieves all columns
SELECT * FROM Student WHERE student_id = 100;

-- Good: Retrieves only needed columns
SELECT name, email FROM Student WHERE student_id = 100;
```

---

#### 4. Use Joins Instead of Subqueries

Joins are often **faster** than correlated subqueries because they avoid repeated execution.

```sql
-- Slower: Correlated subquery
SELECT name
FROM Student s
WHERE (
    SELECT COUNT(*)
    FROM Enrolls e
    WHERE e.student_id = s.student_id
) > 3;

-- Faster: Join with GROUP BY
SELECT s.name
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
GROUP BY s.student_id, s.name
HAVING COUNT(*) > 3;
```

---

#### 5. Limit Use of Functions in WHERE Clause

Functions on indexed columns can **disable index usage**.

```sql
-- Bad: Index on 'name' not used
SELECT * FROM Student WHERE UPPER(name) = 'ALICE';

-- Good: Preprocess value, index can be used
SELECT * FROM Student WHERE name = 'Alice';

-- Alternative: Create functional index
CREATE INDEX idx_student_name_upper ON Student(UPPER(name));
```

---

#### 6. Use EXISTS Instead of IN for Subqueries

More efficient for **large datasets** because EXISTS stops at first match.

```sql
-- Slower with large subquery results
SELECT name FROM Student s
WHERE s.student_id IN (
    SELECT student_id FROM Enrolls WHERE grade = 'A'
);

-- Faster: EXISTS stops at first match
SELECT name FROM Student s
WHERE EXISTS (
    SELECT 1 FROM Enrolls e
    WHERE e.student_id = s.student_id AND grade = 'A'
);
```

---

#### 7. Limit Result Sets

Use LIMIT/TOP to retrieve only needed rows.

```sql
-- Retrieve only top 10 students
SELECT name, gpa FROM Student
ORDER BY gpa DESC
LIMIT 10;
```

---

#### 8. Use UNION ALL Instead of UNION

UNION removes duplicates (expensive), UNION ALL doesn't.

```sql
-- Slower: Removes duplicates
SELECT name FROM ActiveStudents
UNION
SELECT name FROM ArchivedStudents;

-- Faster: Keeps duplicates
SELECT name FROM ActiveStudents
UNION ALL
SELECT name FROM ArchivedStudents;
```

---

#### 9. Optimize JOINs

- **Filter early:** Apply WHERE conditions before joining
- **Join order matters:** Join smaller tables first
- **Use appropriate join type**

```sql
-- Optimized: Filter before joining
SELECT s.name, c.title
FROM (SELECT * FROM Student WHERE enrollment_year = 2024) s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id;
```

---

#### 10. Partitioning and Table Design

Split large tables for **faster query access**.

```sql
-- Range partitioning by year
CREATE TABLE Orders (
    order_id INT,
    order_date DATE,
    amount DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

---

#### 11. Caching Frequent Queries

Use **materialized views** for complex, frequently-run queries.

```sql
-- Create materialized view
CREATE MATERIALIZED VIEW StudentGPA AS
SELECT s.student_id, s.name, AVG(e.grade) AS gpa
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
GROUP BY s.student_id, s.name;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW StudentGPA;
```

---

#### 12. Denormalization for Read-Heavy Systems

For reporting/analytics, consider denormalizing data.

```sql
-- Denormalized table for faster reporting
CREATE TABLE StudentCourseReport AS
SELECT s.student_id, s.name, s.email,
       c.course_id, c.title, c.credits,
       e.grade, e.enrollment_date
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id;
```

---

### D. Examples of Optimized Queries

#### 1. Ranking Top Students per Course

```sql
-- Find top 3 students in each course
SELECT course_id, name, grade
FROM (
    SELECT e.course_id, s.name, e.grade,
           ROW_NUMBER() OVER(PARTITION BY e.course_id ORDER BY e.grade DESC) AS rn
    FROM Enrolls e
    JOIN Student s ON e.student_id = s.student_id
    WHERE e.grade IS NOT NULL
) sub
WHERE rn <= 3;
```

#### 2. Join with Filtered Table

```sql
-- Efficiently find students in high-credit courses
SELECT s.name, c.title, c.credits
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id
WHERE c.credits > 3
ORDER BY c.credits DESC, s.name;
```

#### 3. Complex Analytics Query

```sql
-- Student performance report with rankings
WITH StudentStats AS (
    SELECT s.student_id, s.name,
           COUNT(*) AS total_courses,
           AVG(e.grade) AS avg_grade,
           SUM(c.credits) AS total_credits
    FROM Student s
    JOIN Enrolls e ON s.student_id = e.student_id
    JOIN Course c ON e.course_id = c.course_id
    WHERE e.grade IS NOT NULL
    GROUP BY s.student_id, s.name
)
SELECT student_id, name, total_courses, avg_grade, total_credits,
       RANK() OVER(ORDER BY avg_grade DESC) AS grade_rank,
       RANK() OVER(ORDER BY total_credits DESC) AS credit_rank
FROM StudentStats
WHERE total_courses >= 3
ORDER BY avg_grade DESC;
```

#### 4. Time-Series Analysis with Window Functions

```sql
-- Calculate month-over-month growth
WITH MonthlySales AS (
    SELECT DATE_TRUNC('month', order_date) AS month,
           SUM(amount) AS total_sales
    FROM Orders
    GROUP BY DATE_TRUNC('month', order_date)
)
SELECT month, total_sales,
       LAG(total_sales) OVER(ORDER BY month) AS prev_month,
       total_sales - LAG(total_sales) OVER(ORDER BY month) AS growth,
       ROUND(100.0 * (total_sales - LAG(total_sales) OVER(ORDER BY month)) / 
             LAG(total_sales) OVER(ORDER BY month), 2) AS growth_pct
FROM MonthlySales
ORDER BY month;
```

---

### E. Best Practices

#### 1. Analyze query execution plans before optimizing
- Use EXPLAIN to understand current performance
- Identify bottlenecks (table scans, missing indexes)
- Measure before and after optimization

#### 2. Use indexes wisely
- Too many indexes slow down inserts/updates
- Index columns used in WHERE, JOIN, ORDER BY
- Regularly maintain indexes (rebuild, reorganize)

#### 3. Avoid unnecessary subqueries for large datasets
- Use joins instead of correlated subqueries
- Use EXISTS instead of IN for large result sets

#### 4. Select only required columns
- Avoid SELECT * in production code
- Reduces I/O, network traffic, and memory usage

#### 5. Use joins and window functions efficiently
- Window functions avoid expensive self-joins
- Choose appropriate join types
- Filter data early in the query

#### 6. Consider partitioning for very large tables
- Partition by date, range, or hash
- Improves query performance and maintenance
- Enables parallel processing

#### 7. Monitor query performance using DBMS tools
- MySQL Workbench, SQL Server Profiler
- PostgreSQL pg_stat_statements
- Query execution time logs
- Slow query logs

#### 8. Additional Best Practices
- Use prepared statements to prevent SQL injection
- Batch inserts/updates when possible
- Use connection pooling
- Cache frequent query results
- Consider read replicas for read-heavy workloads
- Regular database maintenance (VACUUM, ANALYZE)
- Keep statistics up to date

---

### F. Exam Tips

#### 1. Know advanced joins, subqueries, set operations, and window functions
- Practice writing INNER, LEFT, RIGHT, FULL OUTER joins
- Understand when to use each join type
- Master window functions: RANK(), ROW_NUMBER(), LAG(), LEAD()
- Know the difference between UNION and UNION ALL

#### 2. Understand query optimization strategies and trade-offs
- Indexing pros/cons: faster reads, slower writes
- Normalization vs denormalization trade-offs
- When to use materialized views
- Partitioning strategies

#### 3. Be ready to explain EXPLAIN PLAN output
- Identify full table scans vs index scans
- Understand execution time and row counts
- Recognize inefficient query patterns

#### 4. Real-world scenario questions
Common exam scenarios:
- "Optimize this slow query"
- "Find top N records per group" (use window functions)
- "Calculate running totals" (use SUM() OVER)
- "Rank employees by department" (use RANK() with PARTITION BY)
- "Why is this query slow?" (analyze EXPLAIN plan)

#### 5. Practice Common Patterns
- Pagination: LIMIT and OFFSET
- Top-N queries: ORDER BY with LIMIT
- Ranking within groups: PARTITION BY
- Time-series analysis: LAG/LEAD
- Recursive queries: Employee hierarchies, bill of materials

#### 6. Key Concepts to Remember
- Correlated subqueries execute once per outer row (expensive)
- Window functions don't reduce rows like GROUP BY
- EXISTS is faster than IN for large datasets
- Indexes on foreign keys improve join performance
- Functions in WHERE clause can prevent index usage

---

### Summary

Advanced SQL and query optimization are essential skills for working with relational databases efficiently. Mastering joins, subqueries, window functions, and CTEs enables complex data analysis. Understanding optimization techniques like indexing, query rewriting, and execution plan analysis ensures queries perform well at scale. Always profile queries before optimization, select only needed columns, use appropriate indexes, and monitor performance in production systems.
''',
    codeSnippet: '''
-- ========================================
-- 1. Advanced JOINS Examples
-- ========================================

-- INNER JOIN: Students enrolled in courses
SELECT s.student_id, s.name, c.title, e.grade
FROM Student s
INNER JOIN Enrolls e ON s.student_id = e.student_id
INNER JOIN Course c ON e.course_id = c.course_id
WHERE e.grade IS NOT NULL
ORDER BY s.name, c.title;

-- LEFT JOIN: All students and their enrollments (including students with no enrollments)
SELECT s.student_id, s.name, COALESCE(c.title, 'Not Enrolled') AS course
FROM Student s
LEFT JOIN Enrolls e ON s.student_id = e.student_id
LEFT JOIN Course c ON e.course_id = c.course_id
ORDER BY s.name;

-- RIGHT JOIN: All courses and enrolled students
SELECT c.course_id, c.title, COUNT(e.student_id) AS num_students
FROM Enrolls e
RIGHT JOIN Course c ON e.course_id = c.course_id
GROUP BY c.course_id, c.title
ORDER BY num_students DESC;

-- SELF JOIN: Find pairs of students enrolled in the same course
SELECT s1.name AS student1, s2.name AS student2, c.title AS course
FROM Enrolls e1
JOIN Enrolls e2 ON e1.course_id = e2.course_id AND e1.student_id < e2.student_id
JOIN Student s1 ON e1.student_id = s1.student_id
JOIN Student s2 ON e2.student_id = s2.student_id
JOIN Course c ON e1.course_id = c.course_id
ORDER BY c.title, s1.name;

-- CROSS JOIN: Generate all possible student-course combinations
SELECT s.name, c.title
FROM Student s
CROSS JOIN Course c
WHERE c.credits >= 3
LIMIT 20;

-- ========================================
-- 2. Subqueries Examples
-- ========================================

-- Subquery in WHERE: Students in high-credit courses
SELECT name, email
FROM Student
WHERE student_id IN (
    SELECT DISTINCT e.student_id
    FROM Enrolls e
    JOIN Course c ON e.course_id = c.course_id
    WHERE c.credits > 3
);

-- Correlated Subquery: Students who scored above course average
SELECT s.name, c.title, e.grade,
       (SELECT AVG(grade) FROM Enrolls WHERE course_id = e.course_id) AS course_avg
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id
WHERE e.grade > (
    SELECT AVG(grade)
    FROM Enrolls e2
    WHERE e2.course_id = e.course_id
);

-- Subquery in SELECT: Student with enrollment count
SELECT s.student_id, s.name,
       (SELECT COUNT(*)
        FROM Enrolls e
        WHERE e.student_id = s.student_id) AS num_courses,
       (SELECT AVG(grade)
        FROM Enrolls e
        WHERE e.student_id = s.student_id) AS avg_grade
FROM Student s
ORDER BY num_courses DESC;

-- Subquery in FROM (Derived Table)
SELECT dept, AVG(avg_credits) AS dept_avg_credits
FROM (
    SELECT c.department AS dept, c.course_id, AVG(c.credits) AS avg_credits
    FROM Course c
    GROUP BY c.department, c.course_id
) course_stats
GROUP BY dept;

-- ========================================
-- 3. Set Operations
-- ========================================

-- UNION: Combine current and graduated students
SELECT student_id, name, 'Current' AS status FROM CurrentStudents
UNION
SELECT student_id, name, 'Graduated' AS status FROM GraduatedStudents
ORDER BY name;

-- UNION ALL: Faster when duplicates are acceptable
SELECT course_id FROM Enrolls WHERE grade = 'A'
UNION ALL
SELECT course_id FROM Enrolls WHERE grade = 'B'
ORDER BY course_id;

-- INTERSECT: Students enrolled in both courses
SELECT student_id FROM Enrolls WHERE course_id = 101
INTERSECT
SELECT student_id FROM Enrolls WHERE course_id = 102;

-- EXCEPT: Students NOT enrolled in any course
SELECT student_id FROM Student
EXCEPT
SELECT DISTINCT student_id FROM Enrolls;

-- ========================================
-- 4. Window Functions
-- ========================================

-- ROW_NUMBER: Assign unique row numbers
SELECT name, grade, course_id,
       ROW_NUMBER() OVER(PARTITION BY course_id ORDER BY grade DESC) AS row_num
FROM Enrolls e
JOIN Student s ON e.student_id = s.student_id;

-- RANK and DENSE_RANK: Ranking with ties
SELECT name, course_id, grade,
       RANK() OVER(PARTITION BY course_id ORDER BY grade DESC) AS rank,
       DENSE_RANK() OVER(PARTITION BY course_id ORDER BY grade DESC) AS dense_rank
FROM Enrolls e
JOIN Student s ON e.student_id = s.student_id
ORDER BY course_id, grade DESC;

-- Top 3 students per course using window function
SELECT *
FROM (
    SELECT s.name, c.title, e.grade,
           ROW_NUMBER() OVER(PARTITION BY e.course_id ORDER BY e.grade DESC) AS rn
    FROM Enrolls e
    JOIN Student s ON e.student_id = s.student_id
    JOIN Course c ON e.course_id = c.course_id
) ranked
WHERE rn <= 3;

-- Running Total (Cumulative Sum)
SELECT order_date, amount,
       SUM(amount) OVER(ORDER BY order_date) AS running_total,
       AVG(amount) OVER(ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_7days
FROM Orders;

-- LAG and LEAD: Access previous and next rows
SELECT month, sales,
       LAG(sales, 1) OVER(ORDER BY month) AS prev_month_sales,
       LEAD(sales, 1) OVER(ORDER BY month) AS next_month_sales,
       sales - LAG(sales, 1) OVER(ORDER BY month) AS month_over_month_change
FROM MonthlySales
ORDER BY month;

-- NTILE: Divide students into quartiles by GPA
SELECT name, gpa,
       NTILE(4) OVER(ORDER BY gpa DESC) AS quartile
FROM Student
ORDER BY gpa DESC;

-- FIRST_VALUE and LAST_VALUE
SELECT name, grade, course_id,
       FIRST_VALUE(name) OVER(PARTITION BY course_id ORDER BY grade DESC) AS top_student,
       LAST_VALUE(name) OVER(PARTITION BY course_id ORDER BY grade DESC 
                             ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS lowest_student
FROM Enrolls e
JOIN Student s ON e.student_id = s.student_id;

-- ========================================
-- 5. Common Table Expressions (CTEs)
-- ========================================

-- Simple CTE: High-performing students
WITH HighPerformers AS (
    SELECT s.student_id, s.name, AVG(e.grade) AS avg_grade
    FROM Student s
    JOIN Enrolls e ON s.student_id = e.student_id
    GROUP BY s.student_id, s.name
    HAVING AVG(e.grade) >= 3.5
)
SELECT * FROM HighPerformers
ORDER BY avg_grade DESC;

-- Multiple CTEs: Student analysis
WITH 
    StudentGrades AS (
        SELECT student_id, AVG(grade) AS avg_grade, COUNT(*) AS num_courses
        FROM Enrolls
        GROUP BY student_id
    ),
    TopStudents AS (
        SELECT student_id
        FROM StudentGrades
        WHERE avg_grade >= 3.5 AND num_courses >= 3
    )
SELECT s.name, sg.avg_grade, sg.num_courses
FROM Student s
JOIN TopStudents ts ON s.student_id = ts.student_id
JOIN StudentGrades sg ON s.student_id = sg.student_id
ORDER BY sg.avg_grade DESC;

-- Recursive CTE: Employee hierarchy
WITH RECURSIVE EmployeeHierarchy AS (
    -- Base case: Top-level managers
    SELECT employee_id, name, manager_id, 1 AS level, 
           CAST(name AS VARCHAR(1000)) AS path
    FROM Employee
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive case
    SELECT e.employee_id, e.name, e.manager_id, eh.level + 1,
           CAST(eh.path || ' -> ' || e.name AS VARCHAR(1000))
    FROM Employee e
    JOIN EmployeeHierarchy eh ON e.manager_id = eh.employee_id
    WHERE eh.level < 10  -- Prevent infinite recursion
)
SELECT level, name, path
FROM EmployeeHierarchy
ORDER BY level, name;

-- ========================================
-- 6. Query Optimization Examples
-- ========================================

-- Create indexes for optimization
CREATE INDEX idx_student_email ON Student(email);
CREATE INDEX idx_enrolls_student ON Enrolls(student_id);
CREATE INDEX idx_enrolls_course ON Enrolls(course_id);
CREATE INDEX idx_course_credits ON Course(credits);
CREATE INDEX idx_enrolls_grade ON Enrolls(grade);

-- Composite index for multi-column searches
CREATE INDEX idx_enrolls_student_course_grade ON Enrolls(student_id, course_id, grade);

-- Analyze query execution plan
EXPLAIN ANALYZE
SELECT s.name, c.title, e.grade
FROM Student s
JOIN Enrolls e ON s.student_id = e.student_id
JOIN Course c ON e.course_id = c.course_id
WHERE c.credits > 3
ORDER BY e.grade DESC;

-- Optimized: Use EXISTS instead of IN
SELECT s.name
FROM Student s
WHERE EXISTS (
    SELECT 1
    FROM Enrolls e
    WHERE e.student_id = s.student_id AND e.grade = 'A'
);

-- Optimized: Avoid functions in WHERE clause
-- Bad: SELECT * FROM Student WHERE UPPER(name) = 'ALICE';
-- Good:
SELECT * FROM Student WHERE name = 'Alice';

-- Optimized: Use JOIN instead of correlated subquery
-- Slower version with correlated subquery:
-- SELECT name FROM Student s
-- WHERE (SELECT COUNT(*) FROM Enrolls e WHERE e.student_id = s.student_id) > 3;

-- Faster version with JOIN:
SELECT s.name
FROM Student s
JOIN (
    SELECT student_id, COUNT(*) AS course_count
    FROM Enrolls
    GROUP BY student_id
    HAVING COUNT(*) > 3
) e ON s.student_id = e.student_id;

-- ========================================
-- 7. Advanced Analytics Query
-- ========================================

-- Comprehensive student performance report
WITH StudentStats AS (
    SELECT 
        s.student_id,
        s.name,
        COUNT(DISTINCT e.course_id) AS total_courses,
        AVG(e.grade) AS avg_grade,
        MIN(e.grade) AS min_grade,
        MAX(e.grade) AS max_grade,
        SUM(c.credits) AS total_credits
    FROM Student s
    JOIN Enrolls e ON s.student_id = e.student_id
    JOIN Course c ON e.course_id = c.course_id
    WHERE e.grade IS NOT NULL
    GROUP BY s.student_id, s.name
)
SELECT 
    student_id,
    name,
    total_courses,
    ROUND(avg_grade, 2) AS avg_grade,
    min_grade,
    max_grade,
    total_credits,
    RANK() OVER(ORDER BY avg_grade DESC) AS overall_rank,
    NTILE(4) OVER(ORDER BY avg_grade DESC) AS performance_quartile,
    CASE 
        WHEN avg_grade >= 3.7 THEN 'Excellent'
        WHEN avg_grade >= 3.0 THEN 'Good'
        WHEN avg_grade >= 2.0 THEN 'Average'
        ELSE 'Needs Improvement'
    END AS performance_category
FROM StudentStats
WHERE total_courses >= 3
ORDER BY avg_grade DESC;

-- ========================================
-- 8. Materialized View for Performance
-- ========================================

-- Create materialized view for frequently accessed data
CREATE MATERIALIZED VIEW StudentGPASummary AS
SELECT 
    s.student_id,
    s.name,
    s.email,
    COUNT(DISTINCT e.course_id) AS courses_taken,
    AVG(e.grade) AS gpa,
    SUM(c.credits) AS total_credits,
    MAX(e.enrollment_date) AS last_enrollment
FROM Student s
LEFT JOIN Enrolls e ON s.student_id = e.student_id
LEFT JOIN Course c ON e.course_id = c.course_id
GROUP BY s.student_id, s.name, s.email;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW StudentGPASummary;

-- Query materialized view (much faster)
SELECT * FROM StudentGPASummary
WHERE gpa >= 3.5
ORDER BY gpa DESC;

-- ========================================
-- 9. Partitioning Example
-- ========================================

-- Partition large Orders table by year
CREATE TABLE Orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE NOT NULL,
    amount DECIMAL(10,2),
    status VARCHAR(20)
)
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Query benefits from partition pruning
SELECT * FROM Orders
WHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';
-- Only scans p2024 partition
''',
    revisionPoints: [
      'Advanced SQL includes joins, subqueries, window functions, set operations, and CTEs',
      'Query optimization minimizes execution time and resource usage, critical for large databases',
      'INNER JOIN returns matching rows, LEFT JOIN includes all left table rows, FULL OUTER JOIN includes all rows from both tables',
      'SELF JOIN joins a table to itself, useful for hierarchical data and comparisons',
      'Subqueries can be scalar (single value), row (single row), table (multiple rows/columns), correlated, or non-correlated',
      'Correlated subqueries reference the outer query and execute once per outer row',
      'Set operations: UNION (removes duplicates), UNION ALL (keeps duplicates), INTERSECT (common rows), EXCEPT (rows in first not in second)',
      'Window functions perform calculations across rows without collapsing like GROUP BY',
      'Common window functions: ROW_NUMBER(), RANK(), DENSE_RANK(), NTILE(), LAG(), LEAD(), SUM() OVER, AVG() OVER',
      'RANK() creates gaps for ties, DENSE_RANK() has no gaps, ROW_NUMBER() assigns unique sequential numbers',
      'LAG() accesses previous row value, LEAD() accesses next row value',
      'CTEs improve readability and allow recursive queries for hierarchical data',
      'Recursive CTEs have base case and recursive case, useful for employee hierarchies and tree structures',
      'Indexing speeds up reads but slows down writes; index columns used in WHERE, JOIN, ORDER BY, GROUP BY',
      'EXPLAIN/EXPLAIN ANALYZE shows query execution plan, identifies full table scans and missing indexes',
      'Avoid SELECT * in production; select only required columns to reduce I/O and network traffic',
      'Use JOINs instead of correlated subqueries for better performance',
      'Functions in WHERE clause can disable index usage; preprocess values instead',
      'EXISTS is faster than IN for large datasets because it stops at first match',
      'UNION ALL is faster than UNION because it does not remove duplicates',
      'Partitioning splits large tables by range, list, or hash for faster queries and maintenance',
      'Materialized views cache complex query results for read-heavy workloads',
      'Denormalization improves read performance in analytics systems at cost of redundancy',
      'Filter data early in queries, join smaller tables first, use appropriate join types',
      'Regular index maintenance includes rebuilding, reorganizing, and updating statistics',
    ],
    quizQuestions: [
      Question(
        question: 'Which join type returns all rows from the left table and matching rows from the right table?',
        options: ['INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'CROSS JOIN'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a correlated subquery?',
        options: [
          'A subquery that returns multiple rows',
          'A subquery that references the outer query',
          'A subquery in the SELECT clause',
          'A subquery that uses UNION'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which set operation combines results and removes duplicates?',
        options: ['UNION ALL', 'UNION', 'INTERSECT', 'EXCEPT'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main difference between RANK() and DENSE_RANK()?',
        options: [
          'RANK() is faster',
          'RANK() creates gaps for ties, DENSE_RANK() does not',
          'DENSE_RANK() requires PARTITION BY',
          'They are identical'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which window function accesses the previous row value?',
        options: ['LEAD()', 'LAG()', 'ROW_NUMBER()', 'FIRST_VALUE()'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does the EXPLAIN command do in SQL?',
        options: [
          'Explains table structure',
          'Shows query execution plan',
          'Creates documentation',
          'Validates syntax'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Why should you avoid SELECT * in production queries?',
        options: [
          'It causes syntax errors',
          'It retrieves all columns, increasing I/O and network traffic',
          'It only works in development',
          'It requires special permissions'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is generally faster for large datasets?',
        options: ['IN with subquery', 'EXISTS with subquery', 'Both are equal', 'Neither works with large data'],
        correctIndex: 1,
      ),
      Question(
        question: 'What happens when you use a function on an indexed column in WHERE clause?',
        options: [
          'Query runs faster',
          'Index may not be used, slowing down the query',
          'Syntax error occurs',
          'Index is automatically updated'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a materialized view?',
        options: [
          'A temporary table',
          'A cached result of a query stored as a table',
          'A view that cannot be queried',
          'An index type'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of partitioning a large table?',
        options: [
          'To reduce storage space',
          'To split data into smaller, manageable pieces for faster queries',
          'To create backups automatically',
          'To encrypt data'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does CTE stand for in SQL?',
        options: ['Common Table Expression', 'Calculated Table Entity', 'Conditional Table Extraction', 'Composite Table Element'],
        correctIndex: 0,
      ),
    ],
  ),
  Topic(
    id: 'concurrency_control',
    title: '8. Concurrency Control & Locking',
    explanation: '''## Concurrency Control & Locking

### A. Introduction

**Definition:**

**Concurrency control** ensures **correct results** when **multiple transactions access the database simultaneously**.

**Locking** is a mechanism to **prevent conflicts** and maintain **data consistency and integrity** during concurrent operations.

**Importance:**

- Multi-user databases require **concurrent access** for performance and scalability.
- Prevents **data anomalies** like lost updates, dirty reads, non-repeatable reads, and phantom reads.
- Ensures **ACID properties**, especially **Isolation**.
- Essential for banking systems, e-commerce platforms, and any multi-user application.
- Balances consistency with performance trade-offs.

**Key Objectives:**

- Allow multiple users to access database simultaneously
- Maintain data consistency and integrity
- Prevent conflicts between concurrent transactions
- Ensure serializability of transactions
- Optimize performance while maintaining correctness

---

### B. Concurrency Problems

Without proper concurrency control, several problems can occur:

#### 1. Lost Update

Two transactions update the same row, and one update is lost.

**Example:**

```
Timeline:
T1: Read balance = \$100
T2: Read balance = \$100
T1: balance = balance + \$50 → \$150
T2: balance = balance + \$30 → \$130 (overwrites T1's update)
Result: T1's \$50 deposit is lost!
```

**Real-World Impact:** Customer deposits money, but it doesn't reflect in their account.

---

#### 2. Dirty Read

Transaction reads uncommitted changes of another transaction that may later rollback.

**Example:**

```
T1: UPDATE accounts SET balance = 200 WHERE id = 1; (not committed)
T2: SELECT balance FROM accounts WHERE id = 1; → 200
T1: ROLLBACK; (balance reverts to original)
Result: T2 read data that never existed!
```

**Real-World Impact:** Reports show incorrect balances, leading to wrong business decisions.

---

#### 3. Non-Repeatable Read

Same query returns different results within a single transaction.

**Example:**

```
T1: SELECT balance FROM accounts WHERE id = 1; → \$100
T2: UPDATE accounts SET balance = 200 WHERE id = 1; COMMIT;
T1: SELECT balance FROM accounts WHERE id = 1; → \$200
Result: T1 sees different values for the same row!
```

**Real-World Impact:** Inconsistent data during complex calculations or audits.

---

#### 4. Phantom Read

New rows appear in repeated query results within a transaction.

**Example:**

```
T1: SELECT COUNT(*) FROM orders WHERE status = 'Pending'; → 5
T2: INSERT INTO orders VALUES (..., 'Pending'); COMMIT;
T1: SELECT COUNT(*) FROM orders WHERE status = 'Pending'; → 6
Result: T1 sees a new row (phantom) that didn't exist before!
```

**Real-World Impact:** Aggregate calculations become inconsistent within a transaction.

---

### C. Locking Mechanisms

#### 1. Types of Locks

| Lock Type              | Description                                               | Usage                        |
| ---------------------- | --------------------------------------------------------- | ---------------------------- |
| **Shared Lock (S)**    | Read lock; multiple transactions can read but not write.  | SELECT statements            |
| **Exclusive Lock (X)** | Write lock; only one transaction can read/write.          | INSERT, UPDATE, DELETE       |
| **Update Lock (U)**    | Temporary lock during update process to prevent deadlock. | UPDATE operations            |
| **Intent Lock**        | Indicates intention to acquire lock at finer granularity  | Table-level intent           |

**Compatibility Matrix:**

```
          Shared   Exclusive   Update
Shared      ✓         ✗          ✓
Exclusive   ✗         ✗          ✗
Update      ✓         ✗          ✗
```

**Example:**
- Transaction 1 holds Shared lock → Transaction 2 can acquire Shared lock (both read)
- Transaction 1 holds Exclusive lock → Transaction 2 must wait (write in progress)

---

#### 2. Granularity of Locks

| Level           | Description                                            | Pros                      | Cons                     |
| --------------- | ------------------------------------------------------ | ------------------------- | ------------------------ |
| **Row-level**   | Locks a single row; allows maximum concurrency.        | High concurrency          | More overhead            |
| **Page-level**  | Locks a page of rows; balance between row/table level. | Moderate concurrency      | May lock unneeded rows   |
| **Table-level** | Locks entire table; simpler but reduces concurrency.   | Simple, low overhead      | Low concurrency          |
| **Database**    | Locks entire database                                  | For maintenance only      | No concurrency           |

**Choosing Granularity:**
- **Row-level:** OLTP systems with many concurrent small transactions
- **Table-level:** Batch processing, data warehouse loads
- **Page-level:** Middle ground for moderate concurrency

---

### D. Locking Protocols

#### 1. Two-Phase Locking (2PL)

Ensures **serializability** of transactions (equivalent to some serial execution).

**Phases:**

1. **Growing Phase:** 
   - Acquire all required locks
   - No release allowed
   - Transaction can obtain new locks

2. **Shrinking Phase:** 
   - Release locks
   - No new locks allowed
   - Once any lock is released, no new locks can be acquired

**Example Timeline:**

```
T1: BEGIN
T1: LOCK(A) - shared          [Growing Phase]
T1: LOCK(B) - shared          [Growing Phase]
T1: READ(A)
T1: READ(B)
T1: UNLOCK(A)                 [Shrinking Phase begins]
T1: UNLOCK(B)                 [Shrinking Phase]
T1: COMMIT
```

**Limitation:** May not prevent cascading rollbacks.

---

#### 2. Strict Two-Phase Locking (Strict 2PL)

**Rule:** Locks held **until transaction commits or rolls back**.

**Advantages:**
- Prevents **cascading rollbacks**
- Simpler to implement
- Most commercial databases use this

**Example:**

```
T1: BEGIN
T1: LOCK(A) - exclusive
T1: UPDATE A
T1: LOCK(B) - exclusive
T1: UPDATE B
T1: COMMIT                    [All locks released here]
```

---

#### 3. Deadlock

**Definition:** Two or more transactions wait for each other's locks indefinitely.

**Example:**

```
T1: LOCK(A) - exclusive
T2: LOCK(B) - exclusive
T1: Wait for LOCK(B) - blocked by T2
T2: Wait for LOCK(A) - blocked by T1
Result: Deadlock! Both wait forever.
```

**Deadlock Detection:**
- Database maintains **wait-for graph**
- Cycle in graph indicates deadlock
- Abort one transaction to break cycle

**Deadlock Prevention Techniques:**

1. **Timeout:**
   - Abort transaction after waiting too long
   - Simple but may abort unnecessarily

2. **Wait-Die (Non-preemptive):**
   - Older transaction (smaller timestamp) waits
   - Younger transaction dies (rolls back)
   - Rule: If TS(Ti) < TS(Tj), Ti waits; else Ti dies

3. **Wound-Wait (Preemptive):**
   - Older transaction wounds younger (forces rollback)
   - Younger transaction waits for older
   - Rule: If TS(Ti) < TS(Tj), Tj wounds (rollback); else Ti waits

4. **Lock Ordering:**
   - Acquire locks in predefined order
   - All transactions follow same order
   - Prevents circular wait

**Deadlock Resolution:**
- Choose victim transaction (usually youngest or lowest cost)
- Rollback victim transaction
- Restart victim after delay

---

### E. SQL Locking Examples

#### 1. Row-level Lock with SELECT FOR UPDATE

```sql
-- Transaction 1: Lock specific row for update
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001 FOR UPDATE;
-- Row is locked; other transactions cannot modify it
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1001;
COMMIT;
-- Lock released
```

**Use Case:** Banking transfer to prevent lost updates.

---

#### 2. Table-level Lock

```sql
-- Exclusive table lock
LOCK TABLE accounts IN EXCLUSIVE MODE;
-- Entire table is locked; no other transactions can read/write
UPDATE accounts SET balance = balance * 1.05;
-- Release lock (implicit at transaction end)
COMMIT;
```

**Use Case:** Batch interest calculation for all accounts.

---

#### 3. Shared Lock for Consistent Read

```sql
-- MySQL: Shared lock
BEGIN;
SELECT * FROM inventory WHERE product_id = 500 LOCK IN SHARE MODE;
-- Other transactions can read but not modify
-- Perform calculations...
COMMIT;
```

---

#### 4. Pessimistic vs Optimistic Locking

**Pessimistic Locking:** Lock data before reading/updating.

```sql
-- Pessimistic: Lock immediately
BEGIN;
SELECT * FROM products WHERE id = 100 FOR UPDATE;
UPDATE products SET stock = stock - 1 WHERE id = 100;
COMMIT;
```

**Optimistic Locking:** Check version before updating.

```sql
-- Optimistic: Use version number
-- Step 1: Read data with version
SELECT id, stock, version FROM products WHERE id = 100;
-- Result: stock = 50, version = 5

-- Step 2: Update with version check
UPDATE products 
SET stock = 49, version = 6
WHERE id = 100 AND version = 5;
-- If rows affected = 0, someone else updated; retry
```

**Comparison:**

| Pessimistic                 | Optimistic                        |
| --------------------------- | --------------------------------- |
| Locks immediately           | No locks, check before update     |
| Prevents conflicts          | Detects conflicts                 |
| Better for high contention  | Better for low contention         |
| May cause blocking          | May cause retries                 |

---

#### 5. Explicit vs Implicit Locks

**Implicit Locks:** Database automatically locks during DML operations.

```sql
-- Implicit lock
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1;
-- Database automatically acquires exclusive lock on the row
```

**Explicit Locks:** Developer manually controls locking.

```sql
-- Explicit lock
LOCK TABLE accounts IN SHARE MODE;
SELECT SUM(balance) FROM accounts;
-- Release lock
COMMIT;
```

---

### F. Isolation Levels & Locking

Isolation levels control how transactions see changes made by other transactions.

| Isolation Level      | Locks Applied                           | Prevents                                  | Allows              |
| -------------------- | --------------------------------------- | ----------------------------------------- | ------------------- |
| **Read Uncommitted** | None                                    | Nothing                                   | All anomalies       |
| **Read Committed**   | Shared lock on read; exclusive on write | Dirty reads                               | Non-repeatable, phantom |
| **Repeatable Read**  | Shared locks on read until commit       | Dirty reads, non-repeatable reads         | Phantom reads       |
| **Serializable**     | Range locks                             | All anomalies                             | None                |

#### Detailed Explanation:

**1. READ UNCOMMITTED (Lowest isolation)**

```sql
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
BEGIN;
SELECT * FROM accounts WHERE account_id = 1;
-- Can read uncommitted changes from other transactions
COMMIT;
```

- **No locks** on read
- **Fastest** but **least safe**
- Allows dirty reads
- Use only for approximate queries (reports, analytics)

---

**2. READ COMMITTED (Default in most databases)**

```sql
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
BEGIN;
SELECT * FROM accounts WHERE account_id = 1;
-- Reads only committed data
-- Releases shared lock immediately after read
COMMIT;
```

- **Shared lock** during read, released immediately
- **Exclusive lock** during write
- Prevents dirty reads
- Most common isolation level

---

**3. REPEATABLE READ**

```sql
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN;
SELECT * FROM accounts WHERE account_id = 1;  -- Result: balance = 100
-- Other transactions cannot modify this row
SELECT * FROM accounts WHERE account_id = 1;  -- Result: balance = 100 (same)
COMMIT;
```

- **Shared locks** held until commit
- Prevents non-repeatable reads
- Still allows phantom reads

---

**4. SERIALIZABLE (Highest isolation)**

```sql
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
BEGIN;
SELECT * FROM accounts WHERE balance > 1000;
-- Range lock prevents inserts/updates in this range
SELECT * FROM accounts WHERE balance > 1000;
-- Same result, no phantoms
COMMIT;
```

- **Range locks** or **table locks**
- Prevents all anomalies
- **Slowest** but **safest**
- Equivalent to serial execution

---

**Trade-offs:**

```
┌─────────────────────┬──────────────┬─────────────┐
│ Isolation Level     │ Consistency  │ Performance │
├─────────────────────┼──────────────┼─────────────┤
│ READ UNCOMMITTED    │ Low          │ High        │
│ READ COMMITTED      │ Medium       │ Medium-High │
│ REPEATABLE READ     │ High         │ Medium      │
│ SERIALIZABLE        │ Very High    │ Low         │
└─────────────────────┴──────────────┴─────────────┘
```

---

### G. Best Practices

#### 1. Keep transactions short
Reduces lock contention and improves concurrency.

```sql
-- Bad: Long transaction
BEGIN;
SELECT * FROM large_table;  -- Takes 10 seconds
-- ... complex processing ...
UPDATE accounts SET balance = balance + 100;
COMMIT;

-- Good: Short transaction
-- Do complex processing outside transaction
BEGIN;
UPDATE accounts SET balance = balance + 100;
COMMIT;
```

#### 2. Use appropriate isolation level
- **READ COMMITTED:** Most applications
- **REPEATABLE READ:** Financial calculations
- **SERIALIZABLE:** Critical transactions (rare)

#### 3. Prefer row-level locks over table-level
Allows maximum concurrency for OLTP systems.

#### 4. Avoid unnecessary locks on large datasets
Use batch processing or partition data.

#### 5. Monitor deadlocks and resolve promptly
- Use DBMS tools to detect deadlocks
- Analyze wait-for graphs
- Adjust lock ordering if needed

#### 6. Combine locking with optimistic concurrency control
Use optimistic locking for read-heavy systems to reduce blocking.

#### 7. Use proper indexing
Indexes reduce lock duration by speeding up queries.

#### 8. Implement retry logic
Handle deadlocks gracefully with exponential backoff.

```python
# Example retry logic
max_retries = 3
for attempt in range(max_retries):
    try:
        execute_transaction()
        break
    except DeadlockException:
        if attempt == max_retries - 1:
            raise
        time.sleep(2 ** attempt)  # Exponential backoff
```

---

### H. Exam Tips

#### 1. Be able to explain concurrency problems with examples
- **Lost Update:** Two deposits, one lost
- **Dirty Read:** Reading uncommitted data that rolls back
- **Non-Repeatable Read:** Same query, different results
- **Phantom Read:** New rows appear mid-transaction

#### 2. Know types of locks, granularity, and 2PL protocols
- **Locks:** Shared (read), Exclusive (write), Update (transition)
- **Granularity:** Row, Page, Table
- **2PL:** Growing phase (acquire), Shrinking phase (release)
- **Strict 2PL:** Release all locks at commit/rollback

#### 3. Explain deadlocks, prevention, and detection mechanisms
- **Deadlock:** Circular wait for locks
- **Prevention:** Timeout, Wait-Die, Wound-Wait, Lock ordering
- **Detection:** Wait-for graph, cycle detection
- **Resolution:** Abort victim transaction

#### 4. Understand isolation levels and their impact on locking
- **READ UNCOMMITTED:** No locks, all anomalies
- **READ COMMITTED:** Prevents dirty reads
- **REPEATABLE READ:** Prevents non-repeatable reads
- **SERIALIZABLE:** Prevents all anomalies

#### 5. Real-world scenario questions
Common exam scenarios:
- **Banking:** "Why did the deposit disappear?" (Lost Update)
- **Inventory:** "Stock went negative despite checks" (Race condition)
- **Reports:** "Numbers don't add up" (Non-repeatable reads)
- **Deadlock:** "Two transactions waiting forever" (Circular dependency)

#### 6. Practice Questions
- "Which isolation level prevents phantom reads?" (Serializable)
- "What is the difference between pessimistic and optimistic locking?"
- "How does Strict 2PL prevent cascading rollbacks?"
- "Explain a deadlock scenario with two transactions"

---

### Summary

Concurrency control and locking are essential for maintaining data integrity in multi-user databases. Understanding the types of locks, locking protocols like Two-Phase Locking, and isolation levels helps balance consistency with performance. Proper use of locks prevents concurrency problems like lost updates, dirty reads, and deadlocks. Always keep transactions short, use appropriate isolation levels, and monitor for deadlocks in production systems.
''',
    codeSnippet: '''
-- ========================================
-- 1. Concurrency Problems Demonstration
-- ========================================

-- Lost Update Problem
-- Transaction 1
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1001;  -- balance = 100
-- Simulate processing time
UPDATE accounts SET balance = 100 + 50 WHERE account_id = 1001;  -- Set to 150
COMMIT;

-- Transaction 2 (running concurrently)
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1001;  -- Also reads 100
-- Simulate processing time
UPDATE accounts SET balance = 100 + 30 WHERE account_id = 1001;  -- Set to 130, overwrites T1!
COMMIT;
-- Result: T1's update is lost!

-- ========================================
-- 2. Dirty Read Problem
-- ========================================

-- Transaction 1
BEGIN TRANSACTION;
UPDATE accounts SET balance = 200 WHERE account_id = 1001;
-- T2 reads uncommitted change here (if READ UNCOMMITTED)
-- Something goes wrong...
ROLLBACK;  -- Changes reverted

-- Transaction 2 (READ UNCOMMITTED level)
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1001;  -- Reads 200 (dirty data!)
-- T1 rolls back, but T2 already used wrong value
COMMIT;

-- ========================================
-- 3. Non-Repeatable Read Problem
-- ========================================

-- Transaction 1
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 1001;  -- Result: 100
-- T2 commits an update here
SELECT balance FROM accounts WHERE account_id = 1001;  -- Result: 200 (different!)
COMMIT;

-- Transaction 2
BEGIN TRANSACTION;
UPDATE accounts SET balance = 200 WHERE account_id = 1001;
COMMIT;

-- ========================================
-- 4. Phantom Read Problem
-- ========================================

-- Transaction 1
BEGIN TRANSACTION;
SELECT COUNT(*) FROM orders WHERE status = 'Pending';  -- Result: 5
-- T2 inserts new row here
SELECT COUNT(*) FROM orders WHERE status = 'Pending';  -- Result: 6 (phantom!)
COMMIT;

-- Transaction 2
BEGIN TRANSACTION;
INSERT INTO orders (order_id, status) VALUES (1006, 'Pending');
COMMIT;

-- ========================================
-- 5. Locking Examples
-- ========================================

-- Row-level Shared Lock (MySQL)
BEGIN;
SELECT * FROM accounts WHERE account_id = 1001 LOCK IN SHARE MODE;
-- Other transactions can also acquire shared lock (read)
-- But cannot acquire exclusive lock (write)
COMMIT;

-- Row-level Exclusive Lock (FOR UPDATE)
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001 FOR UPDATE;
-- Row is locked exclusively; no other transaction can read/write
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1001;
COMMIT;

-- Table-level Lock
LOCK TABLE accounts IN EXCLUSIVE MODE;
UPDATE accounts SET balance = balance * 1.05;  -- 5% interest for all
UNLOCK TABLES;

-- Table-level Shared Lock
LOCK TABLE accounts IN SHARE MODE;
SELECT SUM(balance) FROM accounts;  -- Read-only operations
UNLOCK TABLES;

-- ========================================
-- 6. Banking Transfer with Locking
-- ========================================

-- Safe Money Transfer (prevents lost updates)
BEGIN TRANSACTION;

-- Lock both accounts
SELECT balance FROM accounts WHERE account_id = 1001 FOR UPDATE;
SELECT balance FROM accounts WHERE account_id = 1002 FOR UPDATE;

-- Check sufficient funds
DECLARE @balance DECIMAL(10,2);
SELECT @balance = balance FROM accounts WHERE account_id = 1001;

IF @balance >= 500
BEGIN
    -- Debit from source
    UPDATE accounts SET balance = balance - 500 WHERE account_id = 1001;
    
    -- Credit to destination
    UPDATE accounts SET balance = balance + 500 WHERE account_id = 1002;
    
    COMMIT;
END
ELSE
BEGIN
    ROLLBACK;
END;

-- ========================================
-- 7. Deadlock Example
-- ========================================

-- Transaction 1
BEGIN TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1001;  -- Lock A
-- Wait a moment...
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1002;  -- Wait for B (held by T2)
COMMIT;

-- Transaction 2 (runs concurrently)
BEGIN TRANSACTION;
UPDATE accounts SET balance = balance - 50 WHERE account_id = 1002;   -- Lock B
-- Wait a moment...
UPDATE accounts SET balance = balance + 50 WHERE account_id = 1001;   -- Wait for A (held by T1)
COMMIT;

-- Result: DEADLOCK! Both transactions wait for each other.
-- Database will detect and abort one transaction.

-- ========================================
-- 8. Deadlock Prevention with Lock Ordering
-- ========================================

-- Solution: Always lock accounts in ascending order of account_id

-- Transaction 1
BEGIN TRANSACTION;
-- Lock in order: 1001 first, then 1002
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1001;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1002;
COMMIT;

-- Transaction 2
BEGIN TRANSACTION;
-- Also lock in order: 1001 first, then 1002
UPDATE accounts SET balance = balance + 50 WHERE account_id = 1001;
UPDATE accounts SET balance = balance - 50 WHERE account_id = 1002;
COMMIT;

-- No deadlock! T2 waits for T1 to release 1001, then proceeds.

-- ========================================
-- 9. Isolation Levels
-- ========================================

-- READ UNCOMMITTED (allows dirty reads)
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001;
-- Can read uncommitted changes from other transactions
COMMIT;

-- READ COMMITTED (default, prevents dirty reads)
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001;
-- Reads only committed data
-- Shared lock released immediately after read
COMMIT;

-- REPEATABLE READ (prevents non-repeatable reads)
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001;  -- Acquires shared lock
-- Shared lock held until commit
SELECT * FROM accounts WHERE account_id = 1001;  -- Same result guaranteed
COMMIT;

-- SERIALIZABLE (prevents phantom reads)
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE balance > 1000;
-- Range lock prevents inserts/updates in this range
SELECT * FROM accounts WHERE balance > 1000;  -- No new rows (phantoms)
COMMIT;

-- ========================================
-- 10. Pessimistic Locking
-- ========================================

-- Example: E-commerce inventory management
BEGIN TRANSACTION;

-- Lock product row
SELECT stock FROM products WHERE product_id = 500 FOR UPDATE;

-- Check stock availability
DECLARE @stock INT;
SELECT @stock = stock FROM products WHERE product_id = 500;

IF @stock > 0
BEGIN
    -- Reduce stock
    UPDATE products SET stock = stock - 1 WHERE product_id = 500;
    
    -- Create order
    INSERT INTO orders (product_id, quantity, status) 
    VALUES (500, 1, 'Confirmed');
    
    COMMIT;
END
ELSE
BEGIN
    ROLLBACK;
    -- Throw error: Out of stock
END;

-- ========================================
-- 11. Optimistic Locking with Version
-- ========================================

-- Table with version column
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    name VARCHAR(100),
    stock INT,
    version INT DEFAULT 0
);

-- Step 1: Read product with current version
SELECT product_id, stock, version 
FROM products 
WHERE product_id = 500;
-- Result: stock = 10, version = 5

-- Step 2: Update with version check
UPDATE products
SET stock = stock - 1,
    version = version + 1
WHERE product_id = 500 AND version = 5;

-- Check rows affected
IF @@ROWCOUNT = 0
BEGIN
    -- Someone else updated; conflict detected
    ROLLBACK;
    -- Retry transaction
END
ELSE
BEGIN
    COMMIT;
END;

-- ========================================
-- 12. Lock Timeout Configuration
-- ========================================

-- Set lock timeout (MySQL)
SET innodb_lock_wait_timeout = 10;  -- Wait max 10 seconds

-- Set lock timeout (SQL Server)
SET LOCK_TIMEOUT 10000;  -- 10 seconds in milliseconds

-- Transaction with timeout
BEGIN TRANSACTION;
SELECT * FROM accounts WHERE account_id = 1001 FOR UPDATE;
-- If lock cannot be acquired within timeout, transaction fails
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1001;
COMMIT;

-- ========================================
-- 13. Monitoring Locks (MySQL)
-- ========================================

-- View current locks
SELECT * FROM information_schema.INNODB_LOCKS;

-- View lock waits
SELECT * FROM information_schema.INNODB_LOCK_WAITS;

-- View transactions
SELECT * FROM information_schema.INNODB_TRX;

-- Kill blocking transaction
KILL <thread_id>;

-- ========================================
-- 14. Two-Phase Locking Example
-- ========================================

-- Transaction following Strict 2PL
BEGIN TRANSACTION;

-- Growing Phase: Acquire locks
SELECT * FROM accounts WHERE account_id = 1001 FOR UPDATE;  -- Lock A
SELECT * FROM accounts WHERE account_id = 1002 FOR UPDATE;  -- Lock B

-- Perform operations
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1001;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 1002;

-- Shrinking Phase: Release all locks at once
COMMIT;  -- All locks released here (Strict 2PL)

-- ========================================
-- 15. Handling Deadlocks in Application
-- ========================================

-- Retry logic pseudocode
/*
max_retries = 3
for attempt in range(max_retries):
    try:
        BEGIN TRANSACTION
        -- Your SQL operations
        UPDATE accounts SET balance = balance - 100 WHERE account_id = 1001
        UPDATE accounts SET balance = balance + 100 WHERE account_id = 1002
        COMMIT
        break
    catch DeadlockException:
        ROLLBACK
        if attempt == max_retries - 1:
            raise exception
        sleep(2^attempt)  -- Exponential backoff: 1s, 2s, 4s
*/
''',
    revisionPoints: [
      'Concurrency control ensures correct results when multiple transactions access the database simultaneously',
      'Locking prevents conflicts and maintains data consistency and integrity during concurrent operations',
      'Lost Update occurs when two transactions update the same row and one update is lost',
      'Dirty Read occurs when a transaction reads uncommitted changes that may later rollback',
      'Non-Repeatable Read occurs when the same query returns different results within a transaction',
      'Phantom Read occurs when new rows appear in repeated query results within a transaction',
      'Shared Lock (S) allows multiple transactions to read but not write',
      'Exclusive Lock (X) allows only one transaction to read/write, blocking all others',
      'Update Lock (U) is a temporary lock during updates to prevent deadlock',
      'Row-level locks allow maximum concurrency but have higher overhead',
      'Table-level locks are simpler with lower overhead but reduce concurrency',
      'Two-Phase Locking (2PL) has Growing Phase (acquire locks) and Shrinking Phase (release locks)',
      'Strict 2PL holds all locks until transaction commits or rolls back, preventing cascading rollbacks',
      'Deadlock occurs when two or more transactions wait for each other\'s locks indefinitely',
      'Deadlock prevention techniques include timeout, Wait-Die, Wound-Wait, and lock ordering',
      'READ UNCOMMITTED allows all anomalies with no locks for maximum performance',
      'READ COMMITTED prevents dirty reads with shared locks released immediately after read',
      'REPEATABLE READ prevents non-repeatable reads by holding shared locks until commit',
      'SERIALIZABLE prevents all anomalies using range locks, equivalent to serial execution',
      'Pessimistic locking locks data immediately before operations',
      'Optimistic locking checks version numbers before updating, better for low contention',
      'Keep transactions short to reduce lock contention and improve concurrency',
      'Use appropriate isolation level: READ COMMITTED for most apps, SERIALIZABLE for critical operations',
      'Implement retry logic with exponential backoff to handle deadlocks gracefully',
    ],
    quizQuestions: [
      Question(
        question: 'Which concurrency problem occurs when two transactions update the same row and one update is lost?',
        options: ['Dirty Read', 'Lost Update', 'Phantom Read', 'Non-Repeatable Read'],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of lock allows multiple transactions to read but not write?',
        options: ['Exclusive Lock', 'Shared Lock', 'Update Lock', 'Intent Lock'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which lock granularity provides maximum concurrency?',
        options: ['Database-level', 'Table-level', 'Page-level', 'Row-level'],
        correctIndex: 3,
      ),
      Question(
        question: 'In Two-Phase Locking, what happens during the Growing Phase?',
        options: [
          'Locks are released',
          'Locks are acquired, no releases allowed',
          'Both acquisition and release',
          'No locking occurs'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a deadlock in database systems?',
        options: [
          'A very slow query',
          'Two or more transactions waiting for each other\'s locks indefinitely',
          'A lock that never expires',
          'A transaction that runs forever'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which isolation level prevents dirty reads?',
        options: ['READ UNCOMMITTED', 'READ COMMITTED', 'REPEATABLE READ', 'SERIALIZABLE'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which isolation level prevents phantom reads?',
        options: ['READ UNCOMMITTED', 'READ COMMITTED', 'REPEATABLE READ', 'SERIALIZABLE'],
        correctIndex: 3,
      ),
      Question(
        question: 'What is the main difference between pessimistic and optimistic locking?',
        options: [
          'Pessimistic is faster',
          'Pessimistic locks immediately, optimistic checks before updating',
          'Optimistic uses more memory',
          'They are the same'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does SELECT ... FOR UPDATE do?',
        options: [
          'Updates all rows',
          'Acquires an exclusive lock on selected rows',
          'Selects rows for deletion',
          'Creates a backup'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which deadlock prevention technique aborts the transaction after waiting too long?',
        options: ['Wait-Die', 'Wound-Wait', 'Timeout', 'Lock ordering'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is Strict Two-Phase Locking?',
        options: [
          'Locks acquired in two steps',
          'All locks released at commit/rollback',
          'Two transactions running simultaneously',
          'Locks held for exactly two seconds'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Why should transactions be kept short?',
        options: [
          'To save memory',
          'To reduce lock contention and improve concurrency',
          'To avoid syntax errors',
          'Database requirement'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'db_security',
    title: '9. Database Security & Access Control',
    explanation: '''## Database Security & Access Control

### A. Introduction

**Definition:**

**Database Security** refers to protecting the database from **unauthorized access, misuse, or attacks**.

**Access Control** is the mechanism to **grant or restrict permissions** to users based on their roles and responsibilities.

**Importance:**

- Protects **sensitive data** such as personal information, financial data, healthcare records, and intellectual property.
- Ensures **data integrity and confidentiality** by preventing unauthorized modifications.
- Complies with **legal regulations** like GDPR (EU), HIPAA (Healthcare), PCI DSS (Payment Cards), SOX (Financial).
- Prevents data breaches that can cause financial loss, reputation damage, and legal penalties.
- Essential for maintaining customer trust and business continuity.

**Key Objectives:**

- Prevent unauthorized access to database
- Protect data confidentiality and integrity
- Ensure accountability through auditing
- Comply with regulatory requirements
- Detect and respond to security threats
- Minimize impact of security breaches

---

### B. Types of Database Security

#### 1. Physical Security

Protect servers from **unauthorized physical access**.

**Measures:**
- **Locked server rooms** with restricted access
- **CCTV monitoring** of data center facilities
- **Access logs** for tracking physical entry
- **Environmental controls** (temperature, humidity, fire suppression)
- **Biometric authentication** for critical areas
- **Secure disposal** of old hardware (data wiping, destruction)

**Example:** Data center with badge access, security guards, and surveillance cameras.

---

#### 2. Network Security

Prevent **unauthorized network access** to database servers.

**Measures:**
- **Firewalls:** Block unauthorized network traffic
- **VPNs:** Secure remote access through encrypted tunnels
- **SSL/TLS encryption:** Protect data in transit
- **Network segmentation:** Isolate database servers in separate network zones
- **Intrusion Detection Systems (IDS):** Monitor for suspicious network activity
- **IP whitelisting:** Allow connections only from trusted IP addresses

**Example Configuration:**

```
Application Server (DMZ) → Firewall → Database Server (Private Network)
Only specific IPs allowed to connect to database port 3306/5432
```

---

#### 3. Authentication

Verify the identity of users before granting access.

**Methods:**

**a) Username/Password:**
- Traditional method
- Should enforce password policies (length, complexity, expiration)

**b) Multi-Factor Authentication (MFA):**
- Something you know (password)
- Something you have (token, phone)
- Something you are (biometrics)
- Significantly reduces risk of unauthorized access

**c) OAuth / SSO Integration:**
- Single Sign-On (SSO) for enterprise applications
- OAuth 2.0 for third-party authentication
- Centralized authentication management

**d) Certificate-Based Authentication:**
- Use SSL/TLS client certificates
- Stronger than passwords for automated systems

**e) Kerberos Authentication:**
- Ticket-based authentication protocol
- Common in enterprise environments (Active Directory)

---

#### 4. Authorization & Access Control

Restrict what authenticated users can **read, write, update, or delete**.

**Models:**

**a) Role-Based Access Control (RBAC):**
- Users assigned to roles
- Roles granted specific permissions
- Simplifies management for large organizations

**Example:**
```
Role: Data Analyst
Permissions: SELECT on all tables, no INSERT/UPDATE/DELETE

Role: Application User
Permissions: SELECT, INSERT, UPDATE on specific tables

Role: DBA
Permissions: Full access to all objects
```

**b) Discretionary Access Control (DAC):**
- Object owner controls access
- Can grant/revoke permissions to other users
- More flexible but harder to manage

**c) Mandatory Access Control (MAC):**
- System-enforced access rules
- Users cannot change permissions
- Used in high-security environments

---

#### 5. Encryption

Protect data **at rest** and **in transit**.

**a) Data at Rest:**
- **Transparent Data Encryption (TDE):** Database-level encryption
- **File system encryption:** Encrypt database files on disk
- **Column-level encryption:** Encrypt specific sensitive columns
- **Algorithm:** AES-256 commonly used

**b) Data in Transit:**
- **SSL/TLS:** Encrypt connections between client and database
- **IPSec:** Network-layer encryption
- **VPN tunnels:** Encrypted connections over public networks

**c) Backup Encryption:**
- Encrypt backup files to protect offline data
- Use strong encryption keys and secure key management

---

#### 6. Auditing & Monitoring

Track user activities and changes for security analysis.

**Components:**

**a) Audit Logs:**
- Record login attempts (successful and failed)
- Track SQL queries executed
- Monitor data modifications (INSERT, UPDATE, DELETE)
- Record privilege changes (GRANT, REVOKE)

**b) Database Activity Monitoring (DAM):**
- Real-time monitoring of database operations
- Detect suspicious activities and anomalies
- Alert on policy violations

**c) Log Analysis:**
- Regular review of audit logs
- Automated alerting for security events
- Forensic analysis after incidents

**Example Audit Events:**
- Failed login attempts from unknown IPs
- Access to sensitive tables outside business hours
- Mass data exports or deletions
- Privilege escalation attempts

---

### C. Access Control in SQL

#### 1. Creating Users

**MySQL:**

```sql
-- Create user with password
CREATE USER 'alice'@'localhost' IDENTIFIED BY 'SecurePass123!';

-- Create user with specific host
CREATE USER 'bob'@'192.168.1.%' IDENTIFIED BY 'BobPass456!';

-- Create user accessible from any host
CREATE USER 'charlie'@'%' IDENTIFIED BY 'CharliePass789!';
```

**PostgreSQL:**

```sql
-- Create user with password
CREATE USER alice WITH PASSWORD 'SecurePass123!';

-- Create user with additional options
CREATE USER bob WITH 
    PASSWORD 'BobPass456!'
    VALID UNTIL '2026-12-31'
    CONNECTION LIMIT 10;

-- Create superuser
CREATE USER admin WITH SUPERUSER PASSWORD 'AdminPass!';
```

**SQL Server:**

```sql
-- Create SQL Server login
CREATE LOGIN alice WITH PASSWORD = 'SecurePass123!';

-- Create database user
CREATE USER alice FOR LOGIN alice;
```

---

#### 2. Granting Privileges

**MySQL:**

```sql
-- Grant SELECT on specific database
GRANT SELECT ON school.* TO 'alice'@'localhost';

-- Grant multiple privileges
GRANT SELECT, INSERT, UPDATE ON school.students TO 'alice'@'localhost';

-- Grant all privileges on database
GRANT ALL PRIVILEGES ON school.* TO 'bob'@'localhost';

-- Grant specific column access
GRANT SELECT (student_id, name, email) ON school.students TO 'alice'@'localhost';

-- Grant with GRANT OPTION (allows user to grant to others)
GRANT SELECT ON school.* TO 'charlie'@'localhost' WITH GRANT OPTION;
```

**PostgreSQL:**

```sql
-- Grant SELECT on table
GRANT SELECT ON TABLE students TO alice;

-- Grant multiple privileges
GRANT SELECT, INSERT, UPDATE ON TABLE students TO alice;

-- Grant on all tables in schema
GRANT SELECT ON ALL TABLES IN SCHEMA public TO alice;

-- Grant on specific columns
GRANT SELECT (student_id, name, email) ON students TO alice;

-- Grant sequence privileges (for auto-increment)
GRANT USAGE, SELECT ON SEQUENCE students_id_seq TO alice;
```

**Common Privilege Types:**

| Privilege    | Description                                |
| ------------ | ------------------------------------------ |
| **SELECT**   | Read data from tables                      |
| **INSERT**   | Add new rows to tables                     |
| **UPDATE**   | Modify existing data                       |
| **DELETE**   | Remove rows from tables                    |
| **CREATE**   | Create new databases or tables             |
| **DROP**     | Delete databases or tables                 |
| **ALTER**    | Modify table structure                     |
| **INDEX**    | Create or drop indexes                     |
| **EXECUTE**  | Run stored procedures and functions        |
| **GRANT**    | Grant privileges to other users            |

---

#### 3. Revoking Privileges

**MySQL:**

```sql
-- Revoke INSERT privilege
REVOKE INSERT ON school.* FROM 'alice'@'localhost';

-- Revoke all privileges
REVOKE ALL PRIVILEGES ON school.* FROM 'bob'@'localhost';

-- Revoke GRANT OPTION
REVOKE GRANT OPTION ON school.* FROM 'charlie'@'localhost';
```

**PostgreSQL:**

```sql
-- Revoke INSERT privilege
REVOKE INSERT ON TABLE students FROM alice;

-- Revoke all privileges
REVOKE ALL PRIVILEGES ON TABLE students FROM alice;

-- Revoke from all tables
REVOKE SELECT ON ALL TABLES IN SCHEMA public FROM alice;
```

---

#### 4. Roles & Role-Based Access Control (RBAC)

Roles **simplify user permission management** for multiple users with similar access needs.

**PostgreSQL:**

```sql
-- Create roles
CREATE ROLE data_analyst;
CREATE ROLE app_user;
CREATE ROLE dba_admin;

-- Grant privileges to roles
GRANT SELECT ON ALL TABLES IN SCHEMA public TO data_analyst;
GRANT SELECT, INSERT, UPDATE, DELETE ON students, courses TO app_user;
GRANT ALL PRIVILEGES ON DATABASE school TO dba_admin;

-- Assign roles to users
GRANT data_analyst TO alice;
GRANT app_user TO bob;
GRANT dba_admin TO charlie;

-- Create role with login capability
CREATE ROLE analyst_role WITH LOGIN PASSWORD 'AnalystPass!';
GRANT SELECT ON ALL TABLES IN SCHEMA public TO analyst_role;
```

**MySQL:**

```sql
-- Create roles (MySQL 8.0+)
CREATE ROLE 'data_analyst';
CREATE ROLE 'app_user';

-- Grant privileges to roles
GRANT SELECT ON school.* TO 'data_analyst';
GRANT SELECT, INSERT, UPDATE ON school.students TO 'app_user';

-- Assign roles to users
GRANT 'data_analyst' TO 'alice'@'localhost';
GRANT 'app_user' TO 'bob'@'localhost';

-- Activate roles
SET DEFAULT ROLE ALL TO 'alice'@'localhost';
```

**Benefits of RBAC:**
- Easier to manage permissions for groups
- Consistent access control across users
- Simplified onboarding/offboarding
- Better compliance with security policies

---

#### 5. Viewing Privileges

**MySQL:**

```sql
-- Show grants for current user
SHOW GRANTS;

-- Show grants for specific user
SHOW GRANTS FOR 'alice'@'localhost';
```

**PostgreSQL:**

```sql
-- List all users
\\du

-- View table privileges
SELECT grantee, privilege_type 
FROM information_schema.table_privileges 
WHERE table_name = 'students';
```

---

### D. Advanced Security Measures

#### 1. Row-Level Security (RLS)

Control access **at row level**, not just table level.

**PostgreSQL Example:**

```sql
-- Enable row-level security
ALTER TABLE employees ENABLE ROW LEVEL SECURITY;

-- Create policy: users can only see their own records
CREATE POLICY employee_policy
ON employees
FOR SELECT
USING (username = current_user);

-- Policy for managers to see their team
CREATE POLICY manager_policy
ON employees
FOR SELECT
USING (department = (SELECT department FROM users WHERE username = current_user));

-- Policy for INSERT operations
CREATE POLICY employee_insert_policy
ON employees
FOR INSERT
WITH CHECK (username = current_user);
```

**Use Cases:**
- Multi-tenant applications (each tenant sees only their data)
- Employee records (users see only their own data)
- Regional data access (users see only their region's data)

---

#### 2. Column-Level Security

Hide sensitive columns from unauthorized users.

**Example:**

```sql
-- Grant access to specific columns only
GRANT SELECT (employee_id, name, email, department) 
ON employees TO analyst;
-- Analyst cannot see salary, ssn columns

-- Create view with limited columns
CREATE VIEW employee_public AS
SELECT employee_id, name, email, department
FROM employees;

GRANT SELECT ON employee_public TO analyst;
```

**Dynamic Data Masking (SQL Server):**

```sql
-- Mask SSN column
ALTER TABLE employees
ALTER COLUMN ssn ADD MASKED WITH (FUNCTION = 'partial(0,"XXX-XX-",4)');
-- Result: XXX-XX-1234 instead of 123-45-1234

-- Mask email
ALTER TABLE employees
ALTER COLUMN email ADD MASKED WITH (FUNCTION = 'email()');
-- Result: aXXX@XXXX.com instead of alice@example.com
```

---

#### 3. Database Activity Monitoring (DAM)

Real-time monitoring of queries and suspicious activity.

**Features:**
- Monitor all SQL queries in real-time
- Detect anomalies (unusual queries, data access patterns)
- Alert on policy violations
- Generate compliance reports

**Example Alerts:**
- Query accessing more than 10,000 rows
- Data export during non-business hours
- Access to sensitive tables by unauthorized users
- SQL injection attempts

---

#### 4. SQL Injection Prevention

SQL injection is one of the most common and dangerous database attacks.

**Vulnerable Code (UNSAFE):**

```sql
-- String concatenation - NEVER DO THIS!
String query = "SELECT * FROM users WHERE username='" + user + "' AND password='" + pass + "'";

-- Attacker input: user = "admin' OR '1'='1"
-- Resulting query: SELECT * FROM users WHERE username='admin' OR '1'='1' AND password='...'
-- This bypasses authentication!
```

**Safe Code (Using Prepared Statements):**

```java
// Java example
PreparedStatement ps = con.prepareStatement(
    "SELECT * FROM users WHERE username = ? AND password = ?"
);
ps.setString(1, user);
ps.setString(2, pass);
ResultSet rs = ps.executeQuery();
```

```python
# Python example
cursor.execute(
    "SELECT * FROM users WHERE username = %s AND password = %s",
    (user, pass)
)
```

**Additional Prevention Measures:**
- **Input validation:** Whitelist allowed characters
- **Least privilege:** Database user has minimal permissions
- **Stored procedures:** Encapsulate logic
- **Web Application Firewall (WAF):** Filter malicious requests

---

#### 5. Database Encryption

**Transparent Data Encryption (TDE):**

```sql
-- MySQL
ALTER INSTANCE ROTATE INNODB MASTER KEY;

-- SQL Server
CREATE DATABASE ENCRYPTION KEY
WITH ALGORITHM = AES_256
ENCRYPTION BY SERVER CERTIFICATE MyServerCert;

ALTER DATABASE mydb
SET ENCRYPTION ON;
```

**Column-Level Encryption:**

```sql
-- Encrypt specific sensitive columns
INSERT INTO customers (name, credit_card)
VALUES ('Alice', AES_ENCRYPT('4111-1111-1111-1111', 'encryption_key'));

-- Decrypt when querying
SELECT name, AES_DECRYPT(credit_card, 'encryption_key') AS credit_card
FROM customers;
```

---

### E. Security Best Practices

#### 1. Use strong passwords and Multi-Factor Authentication (MFA)

**Password Policy:**
- Minimum 12 characters
- Mix of uppercase, lowercase, numbers, special characters
- No dictionary words or common patterns
- Regular password rotation (60-90 days)
- Password history to prevent reuse

**MFA Implementation:**
- Time-based One-Time Passwords (TOTP)
- SMS or email codes
- Hardware tokens
- Biometric authentication

---

#### 2. Grant least privileges

**Principle of Least Privilege:** Users should have only the minimum access needed for their job.

**Example:**
```sql
-- Bad: Granting excessive privileges
GRANT ALL PRIVILEGES ON *.* TO 'app_user'@'%';

-- Good: Granting only necessary privileges
GRANT SELECT, INSERT, UPDATE ON app_db.orders TO 'app_user'@'localhost';
GRANT SELECT ON app_db.products TO 'app_user'@'localhost';
```

---

#### 3. Regularly audit and monitor database activity

**Activities to Monitor:**
- Login attempts (successful and failed)
- Privilege escalation
- Data access patterns
- Schema changes
- Large data exports

**Tools:**
- Database audit logs
- SIEM (Security Information and Event Management) systems
- Database Activity Monitoring (DAM) solutions

---

#### 4. Encrypt sensitive data

**Encryption Checklist:**
- ✅ Data at rest (TDE, file-level encryption)
- ✅ Data in transit (SSL/TLS connections)
- ✅ Backups (encrypted backup files)
- ✅ Sensitive columns (SSN, credit cards, passwords)

---

#### 5. Avoid hardcoding credentials in applications

**Bad Practice:**

```java
// NEVER do this!
String dbUrl = "jdbc:mysql://localhost:3306/mydb";
String user = "root";
String password = "MyPassword123";
```

**Best Practices:**
- Use environment variables
- Configuration files with restricted permissions
- Secret management systems (HashiCorp Vault, AWS Secrets Manager)
- Connection pooling with encrypted credentials

---

#### 6. Keep DBMS updated with security patches

- Subscribe to security mailing lists
- Test patches in staging environment
- Apply critical security updates promptly
- Monitor vendor security advisories

---

#### 7. Backup data securely

**Backup Security:**
- Encrypt backup files
- Store backups in secure, offsite locations
- Restrict access to backup files
- Test restore procedures regularly
- Protect against ransomware (immutable backups)

---

#### 8. Additional Best Practices

- **Disable unused features** and services
- **Remove default accounts** (root, sa, admin with default passwords)
- **Use separate accounts** for applications and administration
- **Implement database firewall** rules
- **Regular security assessments** and penetration testing
- **Employee training** on security best practices
- **Incident response plan** for security breaches

---

### F. Exam Tips

#### 1. Be able to explain authentication, authorization, and encryption

**Authentication:** Verifying identity (who you are)
- Methods: Password, MFA, certificates, biometrics

**Authorization:** Controlling access (what you can do)
- Models: RBAC, DAC, MAC
- SQL: GRANT, REVOKE commands

**Encryption:** Protecting data confidentiality
- At rest: TDE, file encryption
- In transit: SSL/TLS

---

#### 2. Know SQL commands for user creation, GRANT, REVOKE, and roles

**Key Commands:**
- `CREATE USER`: Create new database user
- `GRANT`: Give privileges to users/roles
- `REVOKE`: Remove privileges from users/roles
- `CREATE ROLE`: Define role with specific permissions
- `SHOW GRANTS`: View user privileges

**Practice Scenarios:**
- Create analyst user with read-only access
- Set up role for application users
- Revoke DELETE privilege from user
- Grant column-level access

---

#### 3. Understand row-level and column-level security concepts

**Row-Level Security (RLS):**
- Users see only subset of rows based on policy
- Useful for multi-tenant applications
- PostgreSQL: CREATE POLICY

**Column-Level Security:**
- Hide sensitive columns from users
- Grant SELECT on specific columns only
- Dynamic data masking

---

#### 4. Be ready to explain SQL injection and prevention

**SQL Injection:**
- Attack that injects malicious SQL code
- Can bypass authentication, steal data, modify/delete data

**Prevention:**
- **Prepared statements** (parameterized queries)
- **Input validation**
- **Least privilege** for database accounts
- **Stored procedures**
- **Web Application Firewall**

**Example Question:**
"Why is string concatenation dangerous for SQL queries?"
Answer: Allows attacker to inject malicious SQL code; use prepared statements instead.

---

#### 5. Real-world scenario questions

Common exam scenarios:

**Scenario 1: Employee Database**
"Design access control for HR system where employees see only their own records, managers see their team, and HR sees all."
Solution: Row-level security policies based on user role and department.

**Scenario 2: Financial Application**
"How would you protect credit card numbers in the database?"
Solution: Column-level encryption, access restrictions, dynamic data masking, PCI DSS compliance.

**Scenario 3: Compliance Requirement**
"Your application must comply with GDPR. What security measures are needed?"
Solution: Encryption, access controls, audit logs, data minimization, right to deletion.

**Scenario 4: Security Breach**
"An unauthorized user gained access to the database. How do you investigate?"
Solution: Review audit logs, analyze login attempts, check privilege escalations, assess data accessed, implement additional security measures.

---

### Summary

Database security is a multi-layered approach involving physical security, network security, authentication, authorization, encryption, and monitoring. Access control through SQL GRANT/REVOKE commands and role-based management ensures users have appropriate permissions. Advanced measures like row-level security, column-level encryption, and SQL injection prevention protect against sophisticated attacks. Regular auditing, strong passwords, least privilege principle, and compliance with security best practices are essential for maintaining a secure database environment.
''',
    codeSnippet: '''
-- ========================================
-- 1. User Management
-- ========================================

-- MySQL: Create users
CREATE USER 'alice'@'localhost' IDENTIFIED BY 'SecurePass123!';
CREATE USER 'bob'@'192.168.1.%' IDENTIFIED BY 'BobPass456!';
CREATE USER 'charlie'@'%' IDENTIFIED BY 'CharliePass789!';

-- PostgreSQL: Create users
CREATE USER alice WITH PASSWORD 'SecurePass123!';
CREATE USER bob WITH PASSWORD 'BobPass456!' VALID UNTIL '2026-12-31';
CREATE USER charlie WITH SUPERUSER PASSWORD 'CharliePass!';

-- SQL Server: Create login and user
CREATE LOGIN alice WITH PASSWORD = 'SecurePass123!';
CREATE USER alice FOR LOGIN alice;

-- Change password
ALTER USER alice WITH PASSWORD 'NewSecurePass456!';

-- Drop user
DROP USER bob;

-- ========================================
-- 2. Granting Privileges
-- ========================================

-- MySQL: Grant SELECT on database
GRANT SELECT ON school.* TO 'alice'@'localhost';

-- Grant multiple privileges
GRANT SELECT, INSERT, UPDATE ON school.students TO 'alice'@'localhost';

-- Grant all privileges
GRANT ALL PRIVILEGES ON school.* TO 'bob'@'localhost';

-- Grant specific columns
GRANT SELECT (student_id, name, email) ON school.students TO 'alice'@'localhost';

-- Grant with GRANT OPTION
GRANT SELECT ON school.* TO 'charlie'@'localhost' WITH GRANT OPTION;

-- PostgreSQL: Grant privileges
GRANT SELECT ON TABLE students TO alice;
GRANT SELECT, INSERT, UPDATE ON TABLE students TO bob;
GRANT ALL PRIVILEGES ON TABLE students TO charlie;

-- Grant on all tables in schema
GRANT SELECT ON ALL TABLES IN SCHEMA public TO alice;

-- Grant sequence privileges
GRANT USAGE, SELECT ON SEQUENCE students_id_seq TO alice;

-- ========================================
-- 3. Revoking Privileges
-- ========================================

-- MySQL: Revoke privileges
REVOKE INSERT ON school.* FROM 'alice'@'localhost';
REVOKE ALL PRIVILEGES ON school.* FROM 'bob'@'localhost';
REVOKE GRANT OPTION ON school.* FROM 'charlie'@'localhost';

-- PostgreSQL: Revoke privileges
REVOKE INSERT ON TABLE students FROM alice;
REVOKE ALL PRIVILEGES ON TABLE students FROM bob;

-- ========================================
-- 4. Role-Based Access Control (RBAC)
-- ========================================

-- PostgreSQL: Create roles
CREATE ROLE data_analyst;
CREATE ROLE app_user;
CREATE ROLE dba_admin;

-- Grant privileges to roles
GRANT SELECT ON ALL TABLES IN SCHEMA public TO data_analyst;
GRANT SELECT, INSERT, UPDATE, DELETE ON students, courses TO app_user;
GRANT ALL PRIVILEGES ON DATABASE school TO dba_admin;

-- Assign roles to users
GRANT data_analyst TO alice;
GRANT app_user TO bob;
GRANT dba_admin TO charlie;

-- MySQL 8.0+: Create roles
CREATE ROLE 'data_analyst';
CREATE ROLE 'app_user';
CREATE ROLE 'dba_admin';

-- Grant privileges to roles
GRANT SELECT ON school.* TO 'data_analyst';
GRANT SELECT, INSERT, UPDATE ON school.* TO 'app_user';
GRANT ALL PRIVILEGES ON school.* TO 'dba_admin';

-- Assign roles to users
GRANT 'data_analyst' TO 'alice'@'localhost';
GRANT 'app_user' TO 'bob'@'localhost';

-- Set default role
SET DEFAULT ROLE ALL TO 'alice'@'localhost';

-- ========================================
-- 5. Viewing Privileges
-- ========================================

-- MySQL: Show grants
SHOW GRANTS;
SHOW GRANTS FOR 'alice'@'localhost';
SHOW GRANTS FOR 'data_analyst';

-- PostgreSQL: View privileges
\\du  -- List all users and roles

SELECT grantee, privilege_type, table_name
FROM information_schema.table_privileges 
WHERE table_schema = 'public';

-- ========================================
-- 6. Row-Level Security (PostgreSQL)
-- ========================================

-- Enable row-level security
ALTER TABLE employees ENABLE ROW LEVEL SECURITY;

-- Policy: users see only their own records
CREATE POLICY employee_self_policy
ON employees
FOR ALL
USING (username = current_user);

-- Policy: managers see their team
CREATE POLICY manager_policy
ON employees
FOR SELECT
USING (
    department = (
        SELECT department 
        FROM managers 
        WHERE manager_name = current_user
    )
);

-- Policy for HR to see all
CREATE POLICY hr_admin_policy
ON employees
FOR ALL
USING (current_user = 'hr_admin');

-- Policy with CHECK for INSERT
CREATE POLICY employee_insert_policy
ON employees
FOR INSERT
WITH CHECK (username = current_user AND hire_date >= CURRENT_DATE);

-- Drop policy
DROP POLICY employee_self_policy ON employees;

-- Disable row-level security
ALTER TABLE employees DISABLE ROW LEVEL SECURITY;

-- ========================================
-- 7. Column-Level Security
-- ========================================

-- Grant access to specific columns only
GRANT SELECT (employee_id, name, email, department) 
ON employees TO analyst;

-- Create view with limited columns
CREATE VIEW employee_public AS
SELECT employee_id, name, email, department, hire_date
FROM employees;

GRANT SELECT ON employee_public TO public;

-- ========================================
-- 8. SQL Injection Prevention
-- ========================================

-- UNSAFE: String concatenation (NEVER DO THIS!)
-- String query = "SELECT * FROM users WHERE username='" + user + "'";

-- SAFE: Prepared statement (Java example)
/*
PreparedStatement ps = con.prepareStatement(
    "SELECT * FROM users WHERE username = ? AND password = ?"
);
ps.setString(1, username);
ps.setString(2, password);
ResultSet rs = ps.executeQuery();
*/

-- SAFE: Parameterized query (Python example)
/*
cursor.execute(
    "SELECT * FROM users WHERE username = %s AND password = %s",
    (username, password)
)
*/

-- Using stored procedures for security
CREATE PROCEDURE AuthenticateUser(
    IN p_username VARCHAR(50),
    IN p_password VARCHAR(255)
)
BEGIN
    SELECT user_id, name, role
    FROM users
    WHERE username = p_username
    AND password_hash = SHA2(p_password, 256)
    AND is_active = TRUE;
END;

-- Call stored procedure
CALL AuthenticateUser('alice', 'password123');

-- ========================================
-- 9. Data Encryption
-- ========================================

-- Column-level encryption (MySQL)
-- Encrypt data
INSERT INTO customers (name, credit_card_encrypted)
VALUES ('Alice', AES_ENCRYPT('4111-1111-1111-1111', 'secret_key'));

-- Decrypt data
SELECT name, 
       CAST(AES_DECRYPT(credit_card_encrypted, 'secret_key') AS CHAR) AS credit_card
FROM customers;

-- PostgreSQL: pgcrypto extension
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- Encrypt with pgcrypto
INSERT INTO customers (name, ssn_encrypted)
VALUES ('Bob', pgp_sym_encrypt('123-45-6789', 'encryption_key'));

-- Decrypt with pgcrypto
SELECT name,
       pgp_sym_decrypt(ssn_encrypted, 'encryption_key') AS ssn
FROM customers;

-- Hash passwords (one-way)
INSERT INTO users (username, password_hash)
VALUES ('alice', SHA2('password123', 256));

-- Verify password
SELECT * FROM users
WHERE username = 'alice'
AND password_hash = SHA2('input_password', 256);

-- ========================================
-- 10. SSL/TLS Connection Setup
-- ========================================

-- MySQL: Require SSL for user
CREATE USER 'secure_user'@'%' 
IDENTIFIED BY 'password'
REQUIRE SSL;

-- Grant with SSL requirement
GRANT ALL ON mydb.* TO 'secure_user'@'%' REQUIRE SSL;

-- Check SSL status
SHOW STATUS LIKE 'Ssl_cipher';
SHOW VARIABLES LIKE '%ssl%';

-- ========================================
-- 11. Audit Logging
-- ========================================

-- MySQL: Enable general query log
SET GLOBAL general_log = 'ON';
SET GLOBAL general_log_file = '/var/log/mysql/general.log';

-- Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;

-- PostgreSQL: Enable logging
-- In postgresql.conf:
-- log_statement = 'all'
-- log_connections = on
-- log_disconnections = on

-- View recent connections (PostgreSQL)
SELECT usename, application_name, client_addr, backend_start
FROM pg_stat_activity;

-- ========================================
-- 12. Security Best Practices Examples
-- ========================================

-- Principle of Least Privilege
-- Bad: Too much access
GRANT ALL PRIVILEGES ON *.* TO 'app_user'@'%';

-- Good: Minimal necessary access
GRANT SELECT, INSERT, UPDATE ON app_db.orders TO 'app_user'@'localhost';
GRANT SELECT ON app_db.products TO 'app_user'@'localhost';

-- Remove default test databases
DROP DATABASE IF EXISTS test;

-- Remove anonymous users
DELETE FROM mysql.user WHERE User = '';

-- Disable remote root login
DELETE FROM mysql.user WHERE User = 'root' AND Host NOT IN ('localhost', '127.0.0.1');

-- Flush privileges after changes
FLUSH PRIVILEGES;

-- ========================================
-- 13. Multi-Tenant Security
-- ========================================

-- Create table with tenant_id
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    tenant_id INT NOT NULL,
    customer_name VARCHAR(100),
    order_total DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Row-level security for multi-tenant (PostgreSQL)
ALTER TABLE orders ENABLE ROW LEVEL SECURITY;

CREATE POLICY tenant_isolation_policy
ON orders
FOR ALL
USING (tenant_id = current_setting('app.current_tenant')::INT);

-- Application sets tenant context
SET app.current_tenant = '123';

-- User can only see their tenant's data
SELECT * FROM orders;  -- Automatically filtered by tenant_id

-- ========================================
-- 14. Password Policy (MySQL 8.0+)
-- ========================================

-- Set password validation plugin
INSTALL PLUGIN validate_password SONAME 'validate_password.so';

-- Configure password policy
SET GLOBAL validate_password.length = 12;
SET GLOBAL validate_password.mixed_case_count = 1;
SET GLOBAL validate_password.number_count = 1;
SET GLOBAL validate_password.special_char_count = 1;

-- Set password expiration
ALTER USER 'alice'@'localhost' PASSWORD EXPIRE INTERVAL 90 DAY;

-- ========================================
-- 15. Monitoring Security Events
-- ========================================

-- Track failed login attempts (custom table)
CREATE TABLE login_attempts (
    attempt_id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50),
    ip_address VARCHAR(45),
    attempt_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    success BOOLEAN
);

-- Log failed attempt
INSERT INTO login_attempts (username, ip_address, success)
VALUES ('alice', '192.168.1.100', FALSE);

-- Check for suspicious activity
SELECT username, ip_address, COUNT(*) AS failed_attempts
FROM login_attempts
WHERE success = FALSE
AND attempt_time > NOW() - INTERVAL 1 HOUR
GROUP BY username, ip_address
HAVING COUNT(*) >= 5;
''',
    revisionPoints: [
      'Database security protects data from unauthorized access, misuse, or attacks',
      'Access control grants or restricts permissions based on user roles and responsibilities',
      'Physical security includes locked server rooms, CCTV, access logs, and environmental controls',
      'Network security uses firewalls, VPNs, SSL/TLS encryption, and network segmentation',
      'Authentication verifies user identity using passwords, MFA, OAuth/SSO, certificates, or Kerberos',
      'Authorization controls what authenticated users can read, write, update, or delete',
      'Role-Based Access Control (RBAC) assigns users to roles with specific permissions',
      'Encryption protects data at rest (TDE, file encryption) and in transit (SSL/TLS)',
      'Auditing and monitoring track user activities, detect anomalies, and maintain logs for security analysis',
      'CREATE USER command creates database users with passwords and host restrictions',
      'GRANT command gives privileges (SELECT, INSERT, UPDATE, DELETE) to users or roles',
      'REVOKE command removes privileges from users or roles',
      'Roles simplify permission management by grouping privileges for multiple users',
      'Row-Level Security (RLS) controls access at row level using policies',
      'Column-Level Security hides sensitive columns through limited grants or views',
      'SQL injection attacks inject malicious SQL code through user inputs',
      'Prepared statements (parameterized queries) prevent SQL injection by separating SQL logic from data',
      'Principle of least privilege: users should have only minimum access needed for their job',
      'Password policies enforce strong passwords: minimum length, complexity, expiration, history',
      'Multi-Factor Authentication (MFA) significantly reduces unauthorized access risk',
      'Transparent Data Encryption (TDE) encrypts database files at rest',
      'SSL/TLS encrypts connections between clients and database servers',
      'Database Activity Monitoring (DAM) provides real-time monitoring and alerting',
      'Regular security audits review logs, access patterns, and privilege assignments',
      'Compliance requirements like GDPR, HIPAA, PCI DSS mandate specific security controls',
    ],
    quizQuestions: [
      Question(
        question: 'What is the principle of least privilege?',
        options: [
          'Give users maximum access for convenience',
          'Give users minimum access needed for their job',
          'Remove all privileges from everyone',
          'Share admin passwords with team'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which SQL command is used to give permissions to a database user?',
        options: ['CREATE', 'GRANT', 'ALLOW', 'PERMIT'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does authentication verify?',
        options: [
          'What actions user can perform',
          'User identity (who they are)',
          'Data encryption status',
          'Network security'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Row-Level Security (RLS)?',
        options: [
          'Encrypting database rows',
          'Controlling access at row level based on policies',
          'Deleting old rows',
          'Indexing rows for performance'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'How do prepared statements prevent SQL injection?',
        options: [
          'They encrypt the query',
          'They separate SQL logic from user data',
          'They disable user input',
          'They use stronger passwords'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of roles in RBAC?',
        options: [
          'To encrypt data',
          'To group privileges for multiple users with similar access needs',
          'To backup databases',
          'To monitor performance'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which encryption protects data stored on disk?',
        options: ['SSL/TLS', 'VPN', 'Transparent Data Encryption (TDE)', 'IPSec'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does the REVOKE command do?',
        options: [
          'Creates a new user',
          'Removes privileges from users or roles',
          'Encrypts data',
          'Backs up database'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Multi-Factor Authentication (MFA)?',
        options: [
          'Using multiple passwords',
          'Authentication requiring multiple forms of verification',
          'Multiple users sharing one account',
          'Logging in multiple times'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of database auditing?',
        options: [
          'To improve query performance',
          'To track user activities and detect security threats',
          'To create backups',
          'To encrypt data'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Column-Level Security?',
        options: [
          'Encrypting database columns',
          'Hiding sensitive columns from unauthorized users',
          'Indexing columns',
          'Sorting columns'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which regulation requires healthcare data protection?',
        options: ['GDPR', 'HIPAA', 'PCI DSS', 'SOX'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'backup_recovery',
    title: '10. Backup, Recovery & High Availability',
    explanation: '''## Backup, Recovery & High Availability

### A. Introduction

**Definitions:**

**Backup:** A **copy of data** stored separately to protect against loss or corruption.

**Recovery:** The **process of restoring** data from a backup after a failure.

**High Availability (HA):** Ensuring that the database system remains **accessible and operational** with minimal downtime.

**Importance:**

- Prevents data loss due to **hardware failure**, **human error**, **software bugs**, or **cyberattacks** (ransomware, malware).
- Maintains **business continuity** and **data reliability** for critical operations.
- Essential for **24/7 systems** like banking, e-commerce, healthcare databases, and cloud services.
- Reduces **financial impact** of downtime and data loss.
- Ensures compliance with regulatory requirements (GDPR, HIPAA, SOX).

**Key Objectives:**

- Minimize data loss (Recovery Point Objective - RPO)
- Minimize downtime (Recovery Time Objective - RTO)
- Ensure data integrity and consistency
- Provide disaster recovery capabilities
- Enable business continuity

**Cost of Downtime:**
- Financial losses (revenue, transactions)
- Customer trust and reputation damage
- Regulatory penalties
- Lost productivity

---

### B. Types of Backups

#### 1. Full Backup

**Complete copy** of the entire database including all tables, schemas, stored procedures, and objects.

**Characteristics:**
- **Backup Time:** Longest
- **Restore Time:** Fastest (single file)
- **Storage Space:** Largest
- **Recovery Complexity:** Simplest

**Example Commands:**

```bash
# MySQL: Full backup
mysqldump -u root -p school_db > full_backup_2025_10_18.sql

# PostgreSQL: Full backup
pg_dump -U postgres -d school_db -f full_backup.sql

# PostgreSQL: Custom format (compressed)
pg_dump -U postgres -Fc -d school_db -f full_backup.dump

# SQL Server
BACKUP DATABASE school_db TO DISK = 'C:\\Backups\\full_backup.bak';
```

**Pros:**
- Simple recovery process (one file)
- Complete data snapshot
- Easy to understand and manage

**Cons:**
- Time-consuming for large databases
- Large storage requirements
- High I/O and CPU usage during backup

**When to Use:**
- Weekly or monthly scheduled backups
- Before major updates or migrations
- Baseline for incremental/differential backups

---

#### 2. Incremental Backup

Stores only **changes made since the last backup** (full or incremental).

**Characteristics:**
- **Backup Time:** Fastest
- **Restore Time:** Slowest (requires multiple files)
- **Storage Space:** Smallest
- **Recovery Complexity:** Most complex

**Example:**

```
Day 1: Full Backup (100GB)
Day 2: Incremental (5GB changes since Day 1)
Day 3: Incremental (3GB changes since Day 2)
Day 4: Incremental (7GB changes since Day 3)

Recovery: Restore Day 1 + Day 2 + Day 3 + Day 4
```

**Pros:**
- Minimal backup time
- Saves storage space
- Low I/O impact

**Cons:**
- Complex recovery (requires all incremental backups)
- If one backup is corrupted, entire chain may fail
- Longer restore time

**When to Use:**
- Daily backups for active databases
- Limited storage environments
- Frequent backup schedule

---

#### 3. Differential Backup

Stores **changes since the last full backup** (not since last differential).

**Characteristics:**
- **Backup Time:** Moderate (increases over time)
- **Restore Time:** Moderate (full + latest differential)
- **Storage Space:** Moderate (grows until next full)
- **Recovery Complexity:** Simpler than incremental

**Example:**

```
Sunday: Full Backup (100GB)
Monday: Differential (5GB since Sunday)
Tuesday: Differential (10GB since Sunday)
Wednesday: Differential (18GB since Sunday)

Recovery: Restore Sunday + Wednesday only
```

**Pros:**
- Faster restore than incremental (only 2 files)
- Balance between full and incremental
- More reliable than incremental chain

**Cons:**
- Grows larger each day until next full backup
- More storage than incremental

**When to Use:**
- Medium-sized databases
- Balance between backup speed and recovery speed
- Weekly full + daily differential pattern

---

#### 4. Logical vs Physical Backup

**Logical Backup:**
- Exports **schema and data** in SQL/text format
- Tools: `mysqldump`, `pg_dump`, SQL Server BCP
- Platform-independent
- Human-readable

**Example:**

```bash
# MySQL logical backup
mysqldump -u root -p --all-databases > logical_backup.sql

# PostgreSQL logical backup with schema only
pg_dump -U postgres -s -d mydb > schema_only.sql

# PostgreSQL logical backup with data only
pg_dump -U postgres -a -d mydb > data_only.sql
```

**Physical Backup:**
- Copies **actual database files** from disk
- Tools: File system copy, Percona XtraBackup, pg_basebackup
- Platform-specific
- Binary format

**Example:**

```bash
# PostgreSQL physical backup
pg_basebackup -U postgres -D /backup/physical -Fp -Xs -P

# MySQL (stop database first)
cp -r /var/lib/mysql/mydb /backup/physical/
```

**Comparison:**

| Feature           | Logical Backup         | Physical Backup      |
| ----------------- | ---------------------- | -------------------- |
| Format            | SQL/Text               | Binary files         |
| Portability       | High                   | Low                  |
| Speed             | Slower                 | Faster               |
| Size              | Larger (text)          | Smaller (compressed) |
| Restore Options   | Flexible (selective)   | All-or-nothing       |
| Database Running  | Yes                    | Usually yes (hot)    |

---

### C. Recovery Methods

#### 1. Restore from Full Backup

**Simple restoration** from a complete backup file.

```bash
# MySQL restore
mysql -u root -p school_db < full_backup.sql

# PostgreSQL restore
psql -U postgres -d school_db -f full_backup.sql

# PostgreSQL custom format
pg_restore -U postgres -d school_db full_backup.dump

# SQL Server
RESTORE DATABASE school_db 
FROM DISK = 'C:\\Backups\\full_backup.bak'
WITH REPLACE;
```

---

#### 2. Point-in-Time Recovery (PITR)

Restore database to a **specific timestamp** before the failure occurred.

**PostgreSQL PITR Example:**

```bash
# Step 1: Configure continuous archiving
# In postgresql.conf:
# wal_level = replica
# archive_mode = on
# archive_command = 'cp %p /mnt/wal_archive/%f'

# Step 2: Take base backup
pg_basebackup -U postgres -D /backup/base -Fp -Xs -P

# Step 3: Create recovery.conf for PITR
restore_command = 'cp /mnt/wal_archive/%f %p'
recovery_target_time = '2025-10-18 10:30:00'
recovery_target_action = 'promote'

# Step 4: Start PostgreSQL (enters recovery mode)
```

**MySQL Binary Log Recovery:**

```bash
# Restore full backup first
mysql -u root -p < full_backup.sql

# Replay binary logs to specific time
mysqlbinlog --stop-datetime="2025-10-18 10:30:00" \\
  binlog.000001 binlog.000002 | mysql -u root -p
```

**Use Cases:**
- Accidental data deletion or update
- Recover from specific error point
- Roll back to before corruption

---

#### 3. Log-Based Recovery

Replays **transaction logs** after restoring backup to ensure data consistency.

**Process:**
1. Restore full backup
2. Apply transaction logs (WAL, redo logs)
3. Reach consistent state

**Ensures:**
- ACID transaction completion
- No partial transactions
- Database consistency

---

#### 4. Partial Recovery

Restores **specific tables or schemas** instead of entire database.

```bash
# MySQL: Restore specific table
mysql -u root -p school_db < students_table_backup.sql

# PostgreSQL: Restore specific table
pg_restore -U postgres -d school_db -t students full_backup.dump

# PostgreSQL: Restore specific schema
pg_restore -U postgres -d school_db -n public full_backup.dump
```

**When to Use:**
- Only specific table corrupted
- Faster than full restore
- Selective data recovery

---

### D. High Availability (HA) Concepts

**Definition:**

High Availability ensures **continuous database availability**, even during failures, with minimal downtime.

**Availability Metrics:**

```
Uptime %    Downtime/Year    Downtime/Month    Downtime/Week
99%         3.65 days        7.31 hours        1.68 hours
99.9%       8.77 hours       43.83 minutes     10.08 minutes
99.99%      52.60 minutes    4.38 minutes      1.01 minutes
99.999%     5.26 minutes     26.30 seconds     6.05 seconds
```

**Key HA Techniques:**

#### 1. Replication

Keeps **multiple copies (replicas)** of the database on different servers.

**Types:**

**a) Master-Slave (Primary-Replica) Replication:**

- **Master:** Handles all writes
- **Slaves:** Handle reads, replicate from master
- **Asynchronous** or **Synchronous**

**MySQL Master-Slave Setup:**

```sql
-- On Master
CREATE USER 'repl'@'%' IDENTIFIED BY 'password';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';
FLUSH PRIVILEGES;

-- Get master status
SHOW MASTER STATUS;
-- Note File and Position

-- On Slave
CHANGE MASTER TO
  MASTER_HOST='192.168.1.100',
  MASTER_USER='repl',
  MASTER_PASSWORD='password',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;

START SLAVE;

-- Check slave status
SHOW SLAVE STATUS\\G
```

**PostgreSQL Streaming Replication:**

```bash
# On Primary: postgresql.conf
wal_level = replica
max_wal_senders = 5
wal_keep_size = 1GB

# On Standby: Create recovery signal
touch standby.signal

# On Standby: postgresql.conf
primary_conninfo = 'host=192.168.1.100 port=5432 user=replicator password=pass'
```

**b) Multi-Master Replication:**

- **Multiple writable nodes**
- All nodes accept writes
- Conflict resolution needed

**Use Cases:**
- Distributed applications
- Global deployments
- High write throughput

**c) Replication Modes:**

| Mode            | Description                          | Pros                 | Cons                     |
| --------------- | ------------------------------------ | -------------------- | ------------------------ |
| Asynchronous    | Slave lags behind master             | Fast writes          | Potential data loss      |
| Synchronous     | Slave confirms before commit         | No data loss         | Slower writes            |
| Semi-Sync       | At least one slave confirms          | Balance              | Moderate performance     |

---

#### 2. Failover

**Automatic switch** to standby server when primary fails.

**Types:**

**a) Automatic Failover:**
- Detection of primary failure
- Promotion of standby to primary
- Redirection of application connections

**b) Manual Failover:**
- DBA manually switches to standby
- Used for planned maintenance

**Failover Process:**

```
1. Health Check: Monitor primary database
2. Failure Detection: Primary becomes unresponsive
3. Standby Promotion: Standby becomes new primary
4. Client Redirection: Applications connect to new primary
5. Old Primary Recovery: Rejoin as standby when fixed
```

**Tools:**
- PostgreSQL: **Patroni**, **repmgr**
- MySQL: **MHA (Master High Availability)**, **Orchestrator**
- SQL Server: **Always On Availability Groups**

---

#### 3. Load Balancing

**Distribute read/write queries** across multiple nodes for better performance.

**Strategies:**
- **Read Replicas:** Route reads to slaves, writes to master
- **Connection Pooling:** Distribute connections across servers
- **Geographic Distribution:** Route to nearest server

**Tools:**
- **HAProxy:** Load balancer for databases
- **ProxySQL:** MySQL-specific proxy
- **pgBouncer:** PostgreSQL connection pooler

---

#### 4. Clustering

**Multiple database instances** act as one logical unit for fault tolerance.

**Examples:**
- **MySQL NDB Cluster:** Shared-nothing architecture
- **Oracle RAC:** Shared storage cluster
- **PostgreSQL with Patroni:** Automated HA cluster

**Benefits:**
- Automatic failover
- No single point of failure
- Horizontal scalability

---

#### 5. Data Mirroring

**Real-time duplication** of data to another location for disaster recovery.

**Types:**
- **Synchronous Mirroring:** Real-time copy
- **Asynchronous Mirroring:** Delayed copy

**Use Cases:**
- Geographic redundancy
- Disaster recovery
- Compliance requirements

---

### E. Backup and Recovery Best Practices

#### 1. Follow the 3-2-1 Rule

- **3** copies of data (1 primary + 2 backups)
- **2** different storage media (disk, tape, cloud)
- **1** copy offsite (disaster recovery)

**Example:**
- Primary: Production database server
- Backup 1: Local backup server (disk)
- Backup 2: Cloud storage (AWS S3, Azure Blob)

---

#### 2. Automate Backups

Use **cron jobs** (Linux) or **Task Scheduler** (Windows) for scheduled backups.

**Example Cron Job:**

```bash
# Daily full backup at 2 AM
0 2 * * * /usr/bin/mysqldump -u backup_user -p'password' school_db > /backups/daily_\`date +\\%Y\\%m\\%d\`.sql

# Weekly full backup on Sunday at 1 AM
0 1 * * 0 /usr/bin/pg_dump -U postgres school_db -f /backups/weekly_\`date +\\%Y\\%m\\%d\`.dump

# Hourly incremental (custom script)
0 * * * * /scripts/incremental_backup.sh
```

---

#### 3. Verify Backups Regularly

**Test restoration** in a sandbox environment to ensure backups are valid.

**Verification Steps:**
1. Restore backup to test server
2. Run data integrity checks
3. Verify critical tables and records
4. Test application connectivity
5. Document restore time

**Automated Verification:**

```bash
#!/bin/bash
# backup_verification.sh

# Restore to test database
mysql -u root -p test_db < /backups/latest_backup.sql

# Run integrity checks
mysqlcheck -u root -p test_db --check --all-databases

# Email results
if [ \$? -eq 0 ]; then
    echo "Backup verification successful" | mail -s "Backup OK" admin@company.com
else
    echo "Backup verification FAILED" | mail -s "Backup FAILED" admin@company.com
fi
```

---

#### 4. Encrypt Backups

Protect backups with **encryption** to prevent data breaches.

**Encryption Methods:**

```bash
# MySQL: Encrypt during backup
mysqldump -u root -p mydb | openssl enc -aes-256-cbc -salt -out backup.sql.enc

# Decrypt and restore
openssl enc -aes-256-cbc -d -in backup.sql.enc | mysql -u root -p mydb

# PostgreSQL: Encrypt backup file
pg_dump -U postgres mydb -f backup.sql
gpg --symmetric --cipher-algo AES256 backup.sql

# Decrypt
gpg --decrypt backup.sql.gpg > backup.sql
```

---

#### 5. Version Control for Schema Changes

Use **migration tools** to manage schema changes.

**Tools:**
- **Liquibase:** Database-agnostic migrations
- **Flyway:** Version control for database
- **Alembic:** Python-based migrations
- **Django Migrations:** For Django applications

**Benefits:**
- Track schema history
- Rollback capability
- Reproducible deployments
- Team collaboration

---

#### 6. Disaster Recovery Plan (DRP)

**Documented procedures** to restore services after failures.

**DRP Components:**
- **Contact List:** Key personnel and their roles
- **RTO/RPO Targets:** Acceptable downtime and data loss
- **Recovery Procedures:** Step-by-step restoration steps
- **Communication Plan:** Internal and external notifications
- **Testing Schedule:** Regular DR drills

**Example RTO/RPO:**

| System Type       | RTO       | RPO        |
| ----------------- | --------- | ---------- |
| Critical (Bank)   | 1 hour    | 5 minutes  |
| Important (CRM)   | 4 hours   | 1 hour     |
| Standard (Reports)| 24 hours  | 24 hours   |

---

#### 7. Additional Best Practices

- **Retention Policy:** Define how long to keep backups (7 days, 30 days, 1 year)
- **Backup Monitoring:** Alert on failed backups
- **Compression:** Save storage space (gzip, zip)
- **Backup Documentation:** Maintain backup procedures and schedules
- **Off-Peak Backups:** Schedule during low-activity periods
- **Incremental Archives:** Keep multiple versions

---

### F. High Availability Architectures

| Architecture             | Description                             | Pros                          | Cons                        | Use Case                |
| ------------------------ | --------------------------------------- | ----------------------------- | --------------------------- | ----------------------- |
| **Master-Slave Replication** | One primary, multiple read replicas | High read scalability         | Single write point          | Read-heavy workloads    |
| **Multi-Master**         | Multiple writable nodes                 | High write scalability        | Conflict resolution needed  | Distributed systems     |
| **Shared Storage Cluster** | Common disk accessible by all nodes   | Fast failover                 | Storage bottleneck          | Enterprise databases    |
| **Active-Passive Failover** | One live, one standby                | Zero data loss (sync)         | Standby underutilized       | Critical 24/7 systems   |
| **Active-Active**        | Both nodes serve traffic                | Maximum resource utilization  | Complex configuration       | High-traffic systems    |

---

### G. Monitoring and Tools

#### Backup Tools

| Purpose                  | Tool                                              |
| ------------------------ | ------------------------------------------------- |
| **MySQL Backup**         | `mysqldump`, `Percona XtraBackup`, `MySQL Enterprise Backup` |
| **PostgreSQL Backup**    | `pg_dump`, `pg_basebackup`, `pgBackRest`, `Barman` |
| **SQL Server Backup**    | `BACKUP DATABASE`, `SQL Server Management Studio` |
| **Cross-Platform**       | `Bacula`, `Amanda`, `Duplicity`                   |
| **Cloud Backup**         | AWS RDS Snapshots, Azure Backup, Google Cloud SQL Backups |

#### Replication Monitoring

```sql
-- MySQL: Check slave status
SHOW SLAVE STATUS\\G

-- PostgreSQL: Check replication status
SELECT * FROM pg_stat_replication;

-- PostgreSQL: Check replication lag
SELECT now() - pg_last_xact_replay_timestamp() AS replication_lag;
```

#### High Availability Tools

| Tool              | Database   | Features                               |
| ----------------- | ---------- | -------------------------------------- |
| **Patroni**       | PostgreSQL | Automated failover, leader election    |
| **repmgr**        | PostgreSQL | Replication management, failover       |
| **MHA**           | MySQL      | Master failover automation             |
| **Orchestrator**  | MySQL      | Topology management, auto-recovery     |
| **Galera Cluster**| MySQL      | Multi-master synchronous replication   |

---

### H. Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO)

**RTO (Recovery Time Objective):**
- **Maximum acceptable downtime**
- How quickly must system be restored?
- Example: "Database must be online within 1 hour"

**RPO (Recovery Point Objective):**
- **Maximum acceptable data loss**
- How much data can be lost?
- Example: "No more than 15 minutes of transactions lost"

**Relationship:**

```
Lower RTO/RPO → Higher Cost
- More frequent backups
- Faster storage
- Redundant systems
- Synchronous replication
```

**Strategy Selection:**

| RTO         | RPO         | Strategy                                    |
| ----------- | ----------- | ------------------------------------------- |
| Hours       | Hours       | Daily full backups, restore from backup     |
| 1-2 hours   | 15-30 min   | Hourly incremental + log shipping           |
| Minutes     | Minutes     | Asynchronous replication + automated failover |
| Seconds     | Near-zero   | Synchronous replication + active-active     |

---

### I. Exam Tips

#### 1. Understand types of backups

- **Full:** Complete database copy
- **Incremental:** Changes since last backup
- **Differential:** Changes since last full backup
- **Logical:** SQL format (portable)
- **Physical:** File copy (faster)

**Practice Question:**
"What is the difference between incremental and differential backups?"

---

#### 2. Know backup and restore commands

**Key Commands to Remember:**

```bash
# MySQL
mysqldump -u root -p dbname > backup.sql
mysql -u root -p dbname < backup.sql

# PostgreSQL
pg_dump -U postgres dbname > backup.sql
pg_restore -U postgres -d dbname backup.dump

# Point-in-Time Recovery
mysqlbinlog --stop-datetime="2025-10-18 10:30:00" binlog.000001
```

---

#### 3. Explain Point-in-Time Recovery and transaction logs

**PITR:**
- Restore to specific timestamp before failure
- Uses transaction logs (WAL, binary logs)
- Allows recovery from specific error point

**How it works:**
1. Restore full backup
2. Replay transaction logs up to target time
3. Stop before the error occurred

---

#### 4. Remember replication types and failover concepts

**Replication:**
- **Master-Slave:** One writer, multiple readers
- **Multi-Master:** Multiple writers
- **Synchronous:** No data loss, slower
- **Asynchronous:** Potential data loss, faster

**Failover:**
- Automatic promotion of standby to primary
- Minimizes downtime
- Tools: Patroni, MHA, Orchestrator

---

#### 5. Know the 3-2-1 rule and HA tools

**3-2-1 Backup Rule:**
- 3 copies of data
- 2 different media types
- 1 offsite copy

**HA Tools:**
- PostgreSQL: Patroni, repmgr
- MySQL: MHA, Orchestrator, Galera
- Cross-platform: HAProxy, Keepalived

---

#### 6. Real-world scenarios

**Scenario 1: E-commerce Database**
"Design a backup strategy for a 24/7 e-commerce database with 1TB of data."

**Answer:**
- **Daily:** Full backup (off-peak hours 2 AM)
- **Hourly:** Incremental backups
- **Continuous:** Transaction log archiving for PITR
- **Replication:** Master-slave for read scaling
- **HA:** Automated failover with Patroni/MHA
- **DR:** Offsite cloud backups (AWS S3)
- **RTO:** 1 hour, **RPO:** 5 minutes

**Scenario 2: Data Loss Recovery**
"A developer accidentally deleted 1000 customer records at 10:30 AM. How do you recover?"

**Answer:**
1. Use Point-in-Time Recovery
2. Restore to 10:29 AM (before deletion)
3. Export deleted records from PITR database
4. Import records back to production
5. Verify data integrity

---

### Summary

Backup and recovery strategies are critical for protecting database data against loss and ensuring business continuity. Understanding different backup types (full, incremental, differential), recovery methods (full restore, PITR, log-based), and high availability techniques (replication, failover, clustering) is essential. Following best practices like the 3-2-1 rule, automating backups, verifying restores, and encrypting backup files ensures data protection. High availability architectures minimize downtime through redundancy and automated failover, meeting strict RTO and RPO requirements for mission-critical systems.
''',
    codeSnippet: '''
-- ========================================
-- 1. Full Backup Commands
-- ========================================

-- MySQL: Full database backup
-- mysqldump -u root -p school_db > full_backup_2025_10_18.sql

-- MySQL: All databases
-- mysqldump -u root -p --all-databases > all_databases.sql

-- MySQL: With compression
-- mysqldump -u root -p school_db | gzip > backup.sql.gz

-- PostgreSQL: Full backup (plain SQL)
-- pg_dump -U postgres -d school_db -f full_backup.sql

-- PostgreSQL: Custom format (compressed, parallel restore)
-- pg_dump -U postgres -Fc -d school_db -f full_backup.dump

-- PostgreSQL: Directory format (parallel backup/restore)
-- pg_dump -U postgres -Fd -d school_db -f backup_dir/

-- SQL Server: Full backup
BACKUP DATABASE school_db 
TO DISK = 'C:\\Backups\\full_backup.bak'
WITH FORMAT, COMPRESSION;

-- ========================================
-- 2. Restore from Full Backup
-- ========================================

-- MySQL: Restore database
-- mysql -u root -p school_db < full_backup.sql

-- MySQL: Restore from compressed backup
-- gunzip < backup.sql.gz | mysql -u root -p school_db

-- PostgreSQL: Restore from SQL file
-- psql -U postgres -d school_db -f full_backup.sql

-- PostgreSQL: Restore from custom format
-- pg_restore -U postgres -d school_db -v full_backup.dump

-- PostgreSQL: Parallel restore
-- pg_restore -U postgres -d school_db -j 4 backup_dir/

-- SQL Server: Restore database
RESTORE DATABASE school_db
FROM DISK = 'C:\\Backups\\full_backup.bak'
WITH REPLACE, RECOVERY;

-- ========================================
-- 3. Incremental/Differential Backup Setup
-- ========================================

-- MySQL: Enable binary logging for incremental backups
-- In my.cnf:
-- log-bin=mysql-bin
-- binlog_format=ROW
-- expire_logs_days=7

-- Show binary logs
SHOW BINARY LOGS;

-- Flush logs (start new binary log file)
FLUSH LOGS;

-- PostgreSQL: Continuous archiving setup
-- In postgresql.conf:
-- wal_level = replica
-- archive_mode = on
-- archive_command = 'cp %p /mnt/wal_archive/%f'
-- max_wal_senders = 5

-- Check WAL archiving status
SELECT * FROM pg_stat_archiver;

-- ========================================
-- 4. Point-in-Time Recovery (PITR)
-- ========================================

-- MySQL: PITR using binary logs
-- Step 1: Restore full backup
-- mysql -u root -p < full_backup.sql

-- Step 2: Apply binary logs up to specific time
-- mysqlbinlog --stop-datetime="2025-10-18 10:30:00" \\
--   /var/lib/mysql/mysql-bin.000001 \\
--   /var/lib/mysql/mysql-bin.000002 | \\
--   mysql -u root -p school_db

-- PostgreSQL: PITR configuration
-- Step 1: Take base backup
-- pg_basebackup -U postgres -D /backup/base -Fp -Xs -P

-- Step 2: Create recovery configuration
-- In postgresql.auto.conf or recovery.conf (older versions):
-- restore_command = 'cp /mnt/wal_archive/%f %p'
-- recovery_target_time = '2025-10-18 10:30:00'
-- recovery_target_action = 'promote'

-- Step 3: Create recovery signal file
-- touch /var/lib/postgresql/data/recovery.signal

-- Step 4: Start PostgreSQL (enters recovery mode automatically)

-- ========================================
-- 5. Partial Backup and Restore
-- ========================================

-- MySQL: Backup specific table
-- mysqldump -u root -p school_db students > students_backup.sql

-- MySQL: Backup multiple specific tables
-- mysqldump -u root -p school_db students courses > tables_backup.sql

-- MySQL: Restore specific table
-- mysql -u root -p school_db < students_backup.sql

-- PostgreSQL: Backup specific table
-- pg_dump -U postgres -d school_db -t students -f students_backup.sql

-- PostgreSQL: Restore specific table
-- pg_restore -U postgres -d school_db -t students full_backup.dump

-- PostgreSQL: Backup specific schema
-- pg_dump -U postgres -d school_db -n public -f public_schema.sql

-- ========================================
-- 6. Encrypted Backups
-- ========================================

-- MySQL: Encrypt backup with OpenSSL
-- mysqldump -u root -p school_db | \\
--   openssl enc -aes-256-cbc -salt -out backup.sql.enc

-- Decrypt and restore
-- openssl enc -aes-256-cbc -d -in backup.sql.enc | \\
--   mysql -u root -p school_db

-- PostgreSQL: Encrypt with GPG
-- pg_dump -U postgres school_db | \\
--   gpg --symmetric --cipher-algo AES256 -o backup.sql.gpg

-- Decrypt and restore
-- gpg --decrypt backup.sql.gpg | psql -U postgres -d school_db

-- ========================================
-- 7. MySQL Replication Setup (Master-Slave)
-- ========================================

-- On Master: Create replication user
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'replication_password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

-- On Master: Enable binary logging
-- In my.cnf:
-- server-id = 1
-- log-bin = mysql-bin
-- binlog_do_db = school_db

-- Get master status
SHOW MASTER STATUS;
-- Note: File (e.g., mysql-bin.000001) and Position (e.g., 154)

-- On Slave: Configure replication
-- In my.cnf:
-- server-id = 2
-- relay-log = relay-bin
-- read_only = 1

-- On Slave: Set up replication
CHANGE MASTER TO
  MASTER_HOST='192.168.1.100',
  MASTER_USER='repl_user',
  MASTER_PASSWORD='replication_password',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;

-- Start replication
START SLAVE;

-- Check slave status
SHOW SLAVE STATUS\\G

-- Monitor replication lag
SHOW SLAVE STATUS\\G
-- Look at: Seconds_Behind_Master

-- ========================================
-- 8. PostgreSQL Streaming Replication
-- ========================================

-- On Primary: Configure postgresql.conf
-- wal_level = replica
-- max_wal_senders = 5
-- wal_keep_size = 1GB
-- hot_standby = on

-- On Primary: Create replication user
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'repl_password';

-- On Primary: Configure pg_hba.conf
-- host replication replicator 192.168.1.0/24 md5

-- On Standby: Take base backup
-- pg_basebackup -h 192.168.1.100 -U replicator -D /var/lib/postgresql/data -Fp -Xs -P -R

-- On Standby: postgresql.conf is auto-configured with -R flag
-- primary_conninfo = 'host=192.168.1.100 port=5432 user=replicator password=repl_password'

-- On Standby: Check replication status
SELECT * FROM pg_stat_wal_receiver;

-- On Primary: Check connected standbys
SELECT * FROM pg_stat_replication;

-- Check replication lag
SELECT now() - pg_last_xact_replay_timestamp() AS replication_lag;

-- ========================================
-- 9. Automated Backup Scripts
-- ========================================

-- Bash script for MySQL daily backups
/*
#!/bin/bash
# mysql_backup.sh

BACKUP_DIR="/backups/mysql"
DATE=\$(date +%Y%m%d_%H%M%S)
DB_NAME="school_db"
DB_USER="backup_user"
DB_PASS="backup_password"

# Create backup directory
mkdir -p \$BACKUP_DIR

# Full backup
mysqldump -u\$DB_USER -p\$DB_PASS \$DB_NAME > \$BACKUP_DIR/backup_\$DATE.sql

# Compress backup
gzip \$BACKUP_DIR/backup_\$DATE.sql

# Delete backups older than 7 days
find \$BACKUP_DIR -name "backup_*.sql.gz" -mtime +7 -delete

# Log success
echo "\$DATE: Backup completed successfully" >> /var/log/mysql_backup.log
*/

-- Cron job for daily backup at 2 AM
-- 0 2 * * * /scripts/mysql_backup.sh

-- ========================================
-- 10. Backup Verification Script
-- ========================================

/*
#!/bin/bash
# verify_backup.sh

BACKUP_FILE="/backups/latest_backup.sql"
TEST_DB="test_restore_db"

# Drop test database if exists
mysql -u root -p -e "DROP DATABASE IF EXISTS \$TEST_DB;"

# Create test database
mysql -u root -p -e "CREATE DATABASE \$TEST_DB;"

# Restore backup to test database
mysql -u root -p \$TEST_DB < \$BACKUP_FILE

# Run integrity checks
mysqlcheck -u root -p \$TEST_DB --check --all-tables

if [ \$? -eq 0 ]; then
    echo "Backup verification PASSED"
    # Send success notification
else
    echo "Backup verification FAILED"
    # Send alert
fi

# Cleanup
mysql -u root -p -e "DROP DATABASE \$TEST_DB;"
*/

-- ========================================
-- 11. High Availability Monitoring
-- ========================================

-- MySQL: Check replication health
SELECT 
    VARIABLE_VALUE as Uptime
FROM performance_schema.global_status 
WHERE VARIABLE_NAME = 'Uptime';

SHOW PROCESSLIST;

-- PostgreSQL: Check cluster health
SELECT 
    application_name,
    client_addr,
    state,
    sync_state,
    replay_lag
FROM pg_stat_replication;

-- Check if database is in recovery (standby)
SELECT pg_is_in_recovery();

-- ========================================
-- 12. Failover Commands
-- ========================================

-- PostgreSQL: Promote standby to primary
-- pg_ctl promote -D /var/lib/postgresql/data

-- Or using SQL
SELECT pg_promote();

-- MySQL: Promote slave to master
STOP SLAVE;
RESET SLAVE ALL;
-- Update application to point to new master

-- ========================================
-- 13. Backup Retention Management
-- ========================================

-- Delete old backups (Linux)
-- find /backups -name "backup_*.sql" -mtime +30 -delete

-- PostgreSQL: Remove old WAL files
SELECT pg_wal_replay_pause();
-- Manually remove old WAL files from archive
SELECT pg_wal_replay_resume();

-- MySQL: Purge old binary logs
PURGE BINARY LOGS BEFORE '2025-09-18 00:00:00';

-- Keep only last 3 binary logs
PURGE BINARY LOGS TO 'mysql-bin.000010';

-- ========================================
-- 14. Cloud Backup Examples
-- ========================================

-- AWS S3 upload
-- aws s3 cp /backups/backup.sql.gz s3://my-db-backups/2025-10-18/

-- Automated S3 sync
-- aws s3 sync /backups/ s3://my-db-backups/ --delete

-- Azure Blob storage upload
-- az storage blob upload \\
--   --account-name myaccount \\
--   --container-name backups \\
--   --name backup.sql.gz \\
--   --file /backups/backup.sql.gz

-- ========================================
-- 15. Recovery Time Objectives (RTO/RPO)
-- ========================================

-- Measure backup time
SET @start_time = NOW();
-- Run backup
SET @end_time = NOW();
SELECT TIMEDIFF(@end_time, @start_time) AS backup_duration;

-- Measure restore time
SET @restore_start = NOW();
-- Run restore
SET @restore_end = NOW();
SELECT TIMEDIFF(@restore_end, @restore_start) AS restore_duration;

-- Calculate RPO (last backup time)
SELECT 
    'Last Backup',
    MAX(backup_time) AS last_backup,
    TIMEDIFF(NOW(), MAX(backup_time)) AS data_at_risk
FROM backup_log;
''',
    revisionPoints: [
      'Backup is a copy of data stored separately, recovery is the process of restoring data after failure',
      'High Availability ensures database remains accessible with minimal downtime',
      'Full backup is a complete database copy; simplest recovery but largest size and longest time',
      'Incremental backup stores changes since last backup; smallest size but most complex recovery',
      'Differential backup stores changes since last full backup; balance between full and incremental',
      'Logical backup exports schema and data in SQL format; platform-independent and human-readable',
      'Physical backup copies actual database files; faster than logical but platform-specific',
      'Point-in-Time Recovery (PITR) restores database to specific timestamp before failure',
      'Transaction logs (WAL, binary logs) enable log-based recovery and PITR',
      'Master-Slave replication has one writer (master) and multiple readers (slaves)',
      'Multi-Master replication allows multiple writable nodes; requires conflict resolution',
      'Synchronous replication ensures no data loss but slower; asynchronous is faster with potential data loss',
      'Failover is automatic switch to standby server when primary fails',
      'Clustering uses multiple database instances as one logical unit for fault tolerance',
      '3-2-1 backup rule: 3 copies of data, 2 different media, 1 offsite copy',
      'Automate backups using cron jobs or task schedulers for consistent protection',
      'Verify backups regularly by testing restoration in sandbox environment',
      'Encrypt backups to protect against data breaches; use AES-256, GPG, or SSL',
      'RTO (Recovery Time Objective) is maximum acceptable downtime',
      'RPO (Recovery Point Objective) is maximum acceptable data loss',
      'Tools for MySQL backup: mysqldump, Percona XtraBackup, MySQL Enterprise Backup',
      'Tools for PostgreSQL backup: pg_dump, pg_basebackup, pgBackRest, Barman',
      'HA tools include Patroni and repmgr for PostgreSQL, MHA and Orchestrator for MySQL',
      'Disaster Recovery Plan documents procedures for restoring services after failures',
      'Backup retention policy defines how long to keep backups based on compliance and business needs',
    ],
    quizQuestions: [
      Question(
        question: 'What does RPO stand for?',
        options: ['Recovery Point Objective', 'Restore Process Operation', 'Redundant Primary Object', 'Recovery Protocol Order'],
        correctIndex: 0,
      ),
      Question(
        question: 'What does RTO stand for?',
        options: ['Recovery Time Objective', 'Restore Transaction Operation', 'Real-Time Optimization', 'Replication Timing Order'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which backup type stores changes since the last full backup?',
        options: ['Full backup', 'Incremental backup', 'Differential backup', 'Partial backup'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the 3-2-1 backup rule?',
        options: [
          '3 backups per day, 2 servers, 1 database',
          '3 copies of data, 2 different media, 1 offsite',
          '3 databases, 2 replicas, 1 master',
          '3 years retention, 2 locations, 1 format'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Point-in-Time Recovery (PITR)?',
        options: [
          'Recovering data at regular intervals',
          'Restoring database to a specific timestamp before failure',
          'Backing up at precise times',
          'Scheduling recovery operations'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'In Master-Slave replication, where are write operations performed?',
        options: ['On slaves only', 'On master only', 'On both master and slaves', 'On neither'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main advantage of synchronous replication over asynchronous?',
        options: [
          'Faster performance',
          'No data loss',
          'Easier configuration',
          'Lower cost'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does failover mean in high availability systems?',
        options: [
          'System completely fails',
          'Automatic switch to standby server when primary fails',
          'Manual backup process',
          'Database corruption'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which backup type provides the fastest restore time?',
        options: ['Incremental', 'Differential', 'Full', 'Partial'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a logical backup?',
        options: [
          'Backup of logical drives only',
          'Exports schema and data in SQL format',
          'Backup using logical operators',
          'Theoretical backup plan'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which tool is commonly used for automated PostgreSQL high availability?',
        options: ['mysqldump', 'Patroni', 'phpMyAdmin', 'SQLyog'],
        correctIndex: 1,
      ),
      Question(
        question: 'What should you do to ensure backups are valid?',
        options: [
          'Store them in multiple locations',
          'Regularly test restoration in a sandbox environment',
          'Compress them',
          'Delete old backups'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'nosql_bigdata',
    title: '11. NoSQL & Big Data Databases',
    explanation: '''## NoSQL & Big Data Databases

### A. Introduction

**Definition:**

**NoSQL (Not Only SQL)** databases are **non-relational databases** that store data in flexible formats like documents, key-value pairs, graphs, or columns — instead of fixed relational tables.

**Big Data Databases** are systems designed to **store, process, and analyze massive volumes of data** efficiently — structured, semi-structured, and unstructured.

**Why NoSQL?**

Traditional RDBMS systems face difficulty handling:
- **Large-scale data:** Petabytes of information
- **Unstructured data:** JSON, XML, images, videos, logs
- **Rapidly changing data:** Dynamic schemas
- **High velocity:** Millions of transactions per second
- **Distributed systems:** Data across multiple servers/locations

NoSQL provides:
- ✅ **Scalability:** Horizontal scaling across commodity servers
- ✅ **High availability:** Fault tolerance through replication
- ✅ **Flexible schema design:** Schema-less or dynamic schemas
- ✅ **Performance:** Optimized for specific data models
- ✅ **Cloud-native:** Ideal for modern web, IoT, and analytics platforms

**Evolution Timeline:**
- **1970s-1990s:** Relational databases dominate (Oracle, MySQL, PostgreSQL)
- **2000s:** Web 2.0 creates need for scalable databases (Google BigTable, Amazon Dynamo)
- **2009:** MongoDB released, NoSQL movement gains momentum
- **2010s:** Big Data explosion (Hadoop, Cassandra, Redis)
- **2020s:** Polyglot persistence (mix of SQL and NoSQL)

---

### B. Characteristics of NoSQL Databases

#### 1. Schema-less Design

**Flexible Data Structure:**
- No fixed table structure required
- Each record (document) can have different fields
- Easy to add/remove fields without migrations
- Dynamic adaptation to changing requirements

**Example:**

```json
// Document 1
{
  "_id": 1,
  "name": "John",
  "age": 30
}

// Document 2 - different structure
{
  "_id": 2,
  "name": "Alice",
  "email": "alice@example.com",
  "courses": ["DBMS", "AI"]
}
```

**Benefits:**
- Faster development cycles
- No schema migrations
- Handles evolving data models

---

#### 2. Horizontal Scalability

**Sharding (Partitioning):**
- Data distributed across multiple servers (nodes)
- Each node handles a subset of data
- Linear scalability: Add more nodes → More capacity

**Comparison:**

| Scaling Type | RDBMS (Vertical)              | NoSQL (Horizontal)           |
| ------------ | ----------------------------- | ---------------------------- |
| Method       | Bigger server (CPU, RAM)      | More servers (distributed)   |
| Cost         | Exponential                   | Linear                       |
| Limit        | Hardware ceiling              | Nearly unlimited             |
| Complexity   | Simple                        | Complex (distributed system) |

**Sharding Example:**

```
Shard 1 (Node 1): Users A-G
Shard 2 (Node 2): Users H-N
Shard 3 (Node 3): Users O-Z
```

---

#### 3. High Availability

**Data Replication:**
- Multiple copies of data across nodes
- Automatic failover if node fails
- Ensures 99.99%+ uptime

**Replication Models:**
- **Master-Slave:** One primary write node, multiple read replicas
- **Multi-Master:** Multiple writable nodes
- **Peer-to-Peer:** All nodes are equal (Cassandra)

**Benefits:**
- Fault tolerance
- Disaster recovery
- Geographic distribution

---

#### 4. Eventual Consistency

**CAP Theorem Trade-off:**
- In distributed systems, perfect real-time consistency is impossible
- **Eventual Consistency:** All replicas will sync eventually (seconds/minutes)
- Acceptable for many use cases (social media, product catalogs)

**Example:**
1. User updates profile in US server
2. Asia server still shows old data (milliseconds lag)
3. Eventually (within seconds), all servers sync

**Contrast with ACID:**
- RDBMS: Strong consistency (immediate)
- NoSQL: Eventual consistency (delayed)

---

#### 5. Flexible Data Models

**Support for Multiple Formats:**
- **JSON/BSON:** MongoDB, CouchDB
- **XML:** MarkLogic
- **Binary:** Images, videos, files
- **Key-Value:** Simple pairs
- **Wide Columns:** Cassandra column families
- **Graph:** Nodes and edges (Neo4j)

**Advantages:**
- Store heterogeneous data
- No ORM impedance mismatch
- Native support for modern data formats

---

### C. Types of NoSQL Databases

| Type                         | Description                                       | Data Model              | Example Databases       | Best Use Case                        |
| ---------------------------- | ------------------------------------------------- | ----------------------- | ----------------------- | ------------------------------------ |
| **Document Store**           | Stores data as JSON-like documents                | Nested JSON/BSON        | MongoDB, CouchDB        | Content management, catalogs         |
| **Key-Value Store**          | Simple key-value pairs for fast lookups           | Key → Value             | Redis, DynamoDB, Riak   | Caching, sessions, real-time data    |
| **Column-Oriented Store**    | Stores data by columns for analytical queries     | Column families         | Cassandra, HBase        | Time-series, IoT, analytics          |
| **Graph Database**           | Stores data as nodes and relationships            | Nodes + Edges           | Neo4j, Amazon Neptune   | Social networks, recommendations     |

**Detailed Comparison:**

#### 1. Document Store (MongoDB, CouchDB)

**Structure:**
- Stores documents (JSON/BSON)
- Documents grouped in collections
- Supports nested objects and arrays

**Strengths:**
- Flexible schema
- Rich query language
- Good for hierarchical data

**Use Cases:**
- E-commerce product catalogs
- Content management systems
- User profiles

---

#### 2. Key-Value Store (Redis, DynamoDB)

**Structure:**
- Simple key → value mapping
- Value can be string, list, hash, set

**Strengths:**
- Extremely fast (in-memory)
- Simple operations (GET, SET, DELETE)
- Predictable performance

**Use Cases:**
- Session storage
- Caching layer
- Real-time leaderboards
- Rate limiting

---

#### 3. Column-Oriented Store (Cassandra, HBase)

**Structure:**
- Data organized by columns, not rows
- Column families group related columns
- Optimized for write-heavy workloads

**Strengths:**
- Massive write throughput
- Linear scalability
- Time-series optimized

**Use Cases:**
- IoT sensor data
- Log aggregation
- Financial transactions
- Event tracking

---

#### 4. Graph Database (Neo4j, Amazon Neptune)

**Structure:**
- Nodes (entities)
- Edges (relationships)
- Properties on both

**Strengths:**
- Fast relationship queries
- Natural data modeling for networks
- Pattern matching

**Use Cases:**
- Social networks (friends, followers)
- Fraud detection
- Recommendation engines
- Knowledge graphs

---

### D. Example – Document-Oriented (MongoDB)

#### 1. Basic Structure

**JSON-like Documents:**

```json
{
  "_id": 1,
  "name": "John Doe",
  "email": "john@example.com",
  "age": 30,
  "address": {
    "street": "123 Main St",
    "city": "New York",
    "zip": "10001"
  },
  "courses": ["DBMS", "Java", "AI"],
  "enrollmentDate": ISODate("2025-01-15")
}
```

**Features:**
- Nested objects (address)
- Arrays (courses)
- Rich data types (Date, ObjectId)
- No predefined schema

---

#### 2. CRUD Operations

**Create (Insert):**

```javascript
// Insert one document
db.students.insertOne({
  name: "Alice",
  age: 22,
  course: "Machine Learning",
  enrolled: true
});

// Insert multiple documents
db.students.insertMany([
  { name: "Bob", age: 24, course: "Data Science" },
  { name: "Charlie", age: 23, course: "AI" }
]);
```

**Read (Query):**

```javascript
// Find all documents
db.students.find();

// Find with filter
db.students.find({ course: "Machine Learning" });

// Find with projection (select specific fields)
db.students.find(
  { age: { \$gte: 22 } },
  { name: 1, course: 1, _id: 0 }
);

// Find one document
db.students.findOne({ name: "Alice" });

// Advanced queries
db.students.find({
  age: { \$gte: 20, \$lte: 25 },
  course: { \$in: ["AI", "Machine Learning"] }
});
```

**Update:**

```javascript
// Update one document
db.students.updateOne(
  { name: "Alice" },
  { \$set: { age: 23, email: "alice@example.com" } }
);

// Update multiple documents
db.students.updateMany(
  { course: "AI" },
  { \$inc: { age: 1 } }
);

// Replace entire document
db.students.replaceOne(
  { name: "Bob" },
  { name: "Bob Smith", age: 25, course: "Deep Learning" }
);

// Upsert (insert if not exists)
db.students.updateOne(
  { name: "Diana" },
  { \$set: { age: 21, course: "NLP" } },
  { upsert: true }
);
```

**Delete:**

```javascript
// Delete one document
db.students.deleteOne({ name: "Alice" });

// Delete multiple documents
db.students.deleteMany({ age: { \$lt: 20 } });

// Delete all documents in collection
db.students.deleteMany({});
```

---

#### 3. Indexes in MongoDB

**Create Indexes for Performance:**

```javascript
// Single field index
db.students.createIndex({ email: 1 });

// Compound index
db.students.createIndex({ course: 1, age: -1 });

// Text index for full-text search
db.students.createIndex({ name: "text", course: "text" });

// Unique index
db.students.createIndex({ email: 1 }, { unique: true });

// List all indexes
db.students.getIndexes();

// Drop index
db.students.dropIndex("email_1");
```

**Index Types:**
- **1:** Ascending order
- **-1:** Descending order
- **"text":** Full-text search
- **"2dsphere":** Geospatial queries

---

#### 4. Aggregation Pipeline

**Complex Data Processing:**

```javascript
db.students.aggregate([
  // Stage 1: Match documents
  { \$match: { age: { \$gte: 20 } } },
  
  // Stage 2: Group by course
  { \$group: {
      _id: "\$course",
      avgAge: { \$avg: "\$age" },
      count: { \$sum: 1 }
  }},
  
  // Stage 3: Sort by count
  { \$sort: { count: -1 } },
  
  // Stage 4: Limit results
  { \$limit: 5 }
]);
```

---

### E. Example – Key-Value Store (Redis)

**In-Memory Data Structure Store:**

#### Basic Commands

```bash
# String operations
SET user:1001 "John Doe"
GET user:1001
DEL user:1001
INCR page_views
EXPIRE session:abc123 3600  # TTL in seconds

# Hash (object storage)
HSET user:1001 name "John" age 30 email "john@example.com"
HGET user:1001 name
HGETALL user:1001

# List (queue)
LPUSH tasks "task1" "task2"
RPOP tasks
LRANGE tasks 0 -1

# Set (unique values)
SADD tags "nosql" "database" "redis"
SMEMBERS tags
SISMEMBER tags "nosql"

# Sorted Set (leaderboard)
ZADD leaderboard 100 "player1" 200 "player2" 150 "player3"
ZRANGE leaderboard 0 -1 WITHSCORES
ZREVRANGE leaderboard 0 2  # Top 3
```

**Use Cases:**
- **Caching:** Store frequently accessed data
- **Session Management:** User login sessions
- **Real-time Analytics:** Counters, leaderboards
- **Pub/Sub:** Message queues
- **Rate Limiting:** API throttling

---

### F. Example – Column-Oriented Store (Apache Cassandra)

**Wide-Column Store for Massive Scale:**

#### Schema Definition

```sql
-- Create keyspace (database)
CREATE KEYSPACE school WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': 3
};

USE school;

-- Create column family (table)
CREATE TABLE students (
  id UUID PRIMARY KEY,
  name text,
  grade int,
  enrollment_date timestamp,
  courses set<text>
);

-- Composite primary key
CREATE TABLE student_scores (
  student_id UUID,
  course_name text,
  score int,
  exam_date timestamp,
  PRIMARY KEY (student_id, course_name)
);
```

#### CRUD Operations

```sql
-- Insert
INSERT INTO students (id, name, grade, enrollment_date)
VALUES (uuid(), 'Vikram', 90, toTimestamp(now()));

-- Query by primary key
SELECT * FROM students WHERE id = 123e4567-e89b-12d3-a456-426614174000;

-- Query with clustering key
SELECT * FROM student_scores WHERE student_id = ? AND course_name = 'DBMS';

-- Update
UPDATE students SET grade = 95 WHERE id = ?;

-- Delete
DELETE FROM students WHERE id = ?;
```

**Key Features:**
- **Partition Key:** Determines data distribution
- **Clustering Key:** Sorts data within partition
- **No JOINs:** Denormalized data model
- **Tunable Consistency:** Choose between consistency and availability

**Use Cases:**
- IoT sensor data (millions of writes/sec)
- Time-series data
- Event logging
- Financial transactions

---

### G. Example – Graph Database (Neo4j)

**Cypher Query Language:**

#### Create Graph Data

```cypher
-- Create nodes
CREATE (alice:Student {name:'Alice', age:22})
CREATE (bob:Student {name:'Bob', age:24})
CREATE (dbms:Course {code:'CS301', title:'Database Systems'})

-- Create relationships
CREATE (alice)-[:ENROLLED_IN {semester:'Fall 2025'}]->(dbms)
CREATE (alice)-[:FRIENDS_WITH {since:2024}]->(bob)

-- Create connected graph
CREATE (alice:Student {name:'Alice'})-[:FRIENDS_WITH]->(bob:Student {name:'Bob'}),
       (alice)-[:ENROLLED_IN]->(dbms:Course {code:'CS301'}),
       (bob)-[:ENROLLED_IN]->(dbms)
```

#### Query Graph Data

```cypher
-- Find all friends of Alice
MATCH (a:Student {name:'Alice'})-[:FRIENDS_WITH]->(friend)
RETURN friend.name, friend.age;

-- Find courses Alice is enrolled in
MATCH (s:Student {name:'Alice'})-[:ENROLLED_IN]->(c:Course)
RETURN c.code, c.title;

-- Find friends of friends (2 degrees)
MATCH (alice:Student {name:'Alice'})-[:FRIENDS_WITH*2]->(fof)
RETURN DISTINCT fof.name;

-- Shortest path between two students
MATCH path = shortestPath(
  (alice:Student {name:'Alice'})-[:FRIENDS_WITH*]-(bob:Student {name:'Bob'})
)
RETURN path;

-- Recommendation: Courses friends are taking
MATCH (me:Student {name:'Alice'})-[:FRIENDS_WITH]->(friend)-[:ENROLLED_IN]->(course)
WHERE NOT (me)-[:ENROLLED_IN]->(course)
RETURN course.title, COUNT(friend) AS friend_count
ORDER BY friend_count DESC;
```

**Use Cases:**
- **Social Networks:** Friends, followers, connections
- **Recommendation Engines:** "People who bought X also bought Y"
- **Fraud Detection:** Identify suspicious patterns
- **Knowledge Graphs:** Wikipedia, Google Knowledge Graph
- **Network Analysis:** Telecommunications, logistics

---

### H. The CAP Theorem

**Fundamental Trade-off in Distributed Systems:**

| Property                   | Description                                                     |
| -------------------------- | --------------------------------------------------------------- |
| **Consistency (C)**        | Every node sees the same data at the same time                  |
| **Availability (A)**       | Every request gets a response, even if some nodes fail          |
| **Partition Tolerance (P)** | System continues to work even if network partitions occur       |

**CAP Theorem Statement:**

⚡ A distributed system **can only guarantee TWO of the three** properties at a time.

**Combinations:**

#### 1. CP (Consistency + Partition Tolerance)

**Sacrifice Availability:**
- System may become unavailable during network issues
- Ensures all nodes have consistent data

**Examples:**
- **MongoDB:** Strong consistency mode
- **HBase:** Consistent reads
- **Redis Cluster:** With wait command

**Use Case:** Banking systems, financial transactions (must be consistent)

---

#### 2. AP (Availability + Partition Tolerance)

**Sacrifice Consistency:**
- System always responds (high availability)
- Data may be temporarily inconsistent (eventual consistency)

**Examples:**
- **Cassandra:** Highly available
- **DynamoDB:** Default mode
- **CouchDB:** Eventual consistency

**Use Case:** Social media, content delivery, catalogs (availability > consistency)

---

#### 3. CA (Consistency + Availability)

**Sacrifice Partition Tolerance:**
- Typically single-node or tightly coupled systems
- Not truly distributed

**Examples:**
- **Traditional RDBMS:** MySQL, PostgreSQL (single node)
- **In-memory databases:** H2, SQLite

**Limitation:** Not suitable for distributed systems

---

**Real-World Example:**

```
Scenario: Social media post update

CP System (MongoDB):
- User A updates status
- System waits for all replicas to sync
- If network partition, update fails (unavailable)
- ✅ Consistency guaranteed
- ❌ May be temporarily unavailable

AP System (Cassandra):
- User A updates status
- System accepts update immediately
- Replicas sync eventually (seconds)
- ✅ Always available
- ⚠️ Brief inconsistency possible
```

---

### I. Big Data Databases

**Definition:**

Databases designed to handle **massive data sets** that can't be managed by traditional RDBMS due to:
- **Volume:** Petabytes to exabytes of data
- **Velocity:** High-speed data ingestion (streaming)
- **Variety:** Structured, semi-structured, unstructured
- **Veracity:** Data quality and accuracy challenges

**Core Components:**

#### 1. Storage

**Distributed File Systems:**
- **HDFS (Hadoop Distributed File System):** Stores large files across cluster
- **Amazon S3:** Cloud object storage
- **Google Cloud Storage:** Cloud-based
- **Azure Blob Storage:** Microsoft cloud

**Features:**
- Fault-tolerant (replication)
- Scalable (add more nodes)
- Optimized for large files

---

#### 2. Processing

**Parallel Processing Frameworks:**

**MapReduce (Hadoop):**
- **Map:** Process data in parallel
- **Reduce:** Aggregate results

**Apache Spark:**
- In-memory processing (100x faster than MapReduce)
- Supports batch and streaming
- Rich APIs (SQL, ML, Graph)

**Apache Flink:**
- True stream processing
- Event-time processing

---

#### 3. Analytics

**Query Engines:**

**Apache Hive:**
- SQL-like queries on Hadoop
- Batch processing

**Apache Impala:**
- Real-time SQL on Hadoop
- Low latency

**Apache Drill:**
- Schema-free SQL
- Query multiple data sources

---

### J. Big Data Ecosystem Overview

| Category              | Tools/Technologies                              | Purpose                              |
| --------------------- | ----------------------------------------------- | ------------------------------------ |
| **Storage**           | HDFS, Amazon S3, Google Cloud Storage, Azure    | Distributed file storage             |
| **Processing**        | Hadoop MapReduce, Spark, Flink, Storm           | Parallel data processing             |
| **Querying**          | Hive, Impala, Drill, Presto                     | SQL on big data                      |
| **NoSQL Databases**   | HBase, Cassandra, MongoDB                       | Scalable databases                   |
| **Streaming**         | Kafka, Flume, Kinesis                           | Real-time data ingestion             |
| **Data Warehouse**    | Snowflake, BigQuery, Redshift, Synapse          | Cloud analytics warehouse            |
| **Coordination**      | ZooKeeper, Consul                               | Cluster management                   |
| **Workflow**          | Airflow, Oozie, Luigi                           | Job scheduling                       |
| **Visualization**     | Tableau, Power BI, Looker                       | Data visualization                   |

---

### K. When to Use NoSQL vs. SQL

| Feature            | SQL (Relational DB)               | NoSQL                                  |
| ------------------ | --------------------------------- | -------------------------------------- |
| **Schema**         | Fixed, predefined                 | Flexible, dynamic                      |
| **Transactions**   | Strong ACID guarantees            | Eventual consistency (BASE)            |
| **Data Size**      | Moderate (GBs to TBs)             | Very Large (TBs to PBs)                |
| **Scaling**        | Vertical (bigger servers)         | Horizontal (more servers)              |
| **Query Language** | Standard SQL                      | Custom (varies by database)            |
| **Joins**          | Efficient joins                   | Limited or no joins                    |
| **Consistency**    | Immediate consistency             | Eventual consistency                   |
| **Data Model**     | Normalized tables                 | Denormalized documents/columns         |
| **Best For**       | Banking, ERP, CRM                 | Social Media, IoT, Analytics, Gaming   |
| **Examples**       | MySQL, PostgreSQL, Oracle         | MongoDB, Cassandra, Redis, Neo4j       |

**Decision Tree:**

```
Need ACID transactions? → Yes → SQL
Need flexible schema? → Yes → NoSQL (Document)
Need fast key lookups? → Yes → NoSQL (Key-Value)
Need graph relationships? → Yes → NoSQL (Graph)
Need massive write throughput? → Yes → NoSQL (Column)
Need complex queries with joins? → Yes → SQL
```

---

### L. Advantages of NoSQL

✅ **Horizontal Scalability:**
- Scale out by adding more nodes
- Linear cost scaling
- Handle petabytes of data

✅ **Schema Flexibility:**
- No migrations required
- Dynamic data structures
- Fast development cycles

✅ **High Performance:**
- Optimized for specific use cases
- Sub-millisecond latency (Redis)
- Millions of operations/sec

✅ **Handles Unstructured Data:**
- JSON, XML, binary
- Images, videos, logs
- Natural fit for modern apps

✅ **Cloud-Native:**
- Built for distributed environments
- Auto-scaling
- Multi-region replication

✅ **Cost-Effective:**
- Runs on commodity hardware
- Open-source options available
- Pay-as-you-grow model

---

### M. Disadvantages of NoSQL

❌ **Weaker Transactional Guarantees:**
- No strong ACID (in most NoSQL)
- Eventual consistency issues
- Complex to maintain data integrity

❌ **Non-Standard Query Language:**
- Each database has its own syntax
- Harder to learn and switch
- Limited tooling compared to SQL

❌ **Complex Joins and Aggregations:**
- Joins discouraged or impossible
- Data denormalization required
- More application logic needed

❌ **Data Consistency Issues:**
- Race conditions in distributed setups
- Conflict resolution needed
- Debugging is harder

❌ **Maturity:**
- Less mature than RDBMS
- Fewer experts available
- Tooling ecosystem still growing

❌ **No Standardization:**
- Different APIs and concepts
- Vendor lock-in risk
- Migration challenges

---

### N. Example Use Cases

| Domain               | Application                                  | Recommended NoSQL Type | Why?                                      |
| -------------------- | -------------------------------------------- | ---------------------- | ----------------------------------------- |
| **Social Media**     | Facebook, Twitter, Instagram                 | Graph + Document       | Relationships + flexible user profiles    |
| **E-commerce**       | Amazon, eBay product catalogs                | Document (MongoDB)     | Flexible product attributes               |
| **Gaming**           | Session data, leaderboards                   | Key-Value (Redis)      | Ultra-fast reads/writes                   |
| **IoT**              | Sensor data, time-series                     | Column (Cassandra)     | High write throughput, time-series        |
| **Analytics**        | Log processing, clickstream                  | Big Data (Hadoop)      | Massive volume, batch processing          |
| **Content Management** | CMS, blogs, wikis                          | Document (MongoDB)     | Flexible content structure                |
| **Real-Time Bidding** | Ad serving                                  | Key-Value (Redis)      | Microsecond latency                       |
| **Fraud Detection**  | Banking, insurance                           | Graph (Neo4j)          | Pattern detection, relationships          |
| **Recommendation**   | Netflix, Spotify                             | Graph + Document       | User behavior + content metadata          |
| **Mobile Apps**      | Chat apps, messaging                         | Document + Key-Value   | Flexible schema + session management      |

---

### O. Best Practices

#### 1. Choose the Right Type

**Match database to data model:**
- **Hierarchical data?** → Document (MongoDB)
- **Simple lookups?** → Key-Value (Redis)
- **Time-series?** → Column (Cassandra)
- **Relationships?** → Graph (Neo4j)

---

#### 2. Index Wisely

**Create indexes for query patterns:**

```javascript
// MongoDB: Index frequently queried fields
db.users.createIndex({ email: 1 });
db.orders.createIndex({ user_id: 1, created_at: -1 });

// Avoid over-indexing (slows writes)
// Monitor index usage
db.users.aggregate([{ \$indexStats: {} }]);
```

---

#### 3. Plan Sharding Keys

**Choose shard key carefully:**

```javascript
// Good: Evenly distributed
sh.shardCollection("mydb.users", { user_id: "hashed" });

// Bad: Uneven distribution (hotspots)
sh.shardCollection("mydb.orders", { created_at: 1 });
```

**Shard Key Criteria:**
- High cardinality (many unique values)
- Even distribution
- Avoid hotspots

---

#### 4. Polyglot Persistence

**Combine SQL and NoSQL:**

```
E-commerce System:
- Product Catalog → MongoDB (flexible attributes)
- Order Transactions → PostgreSQL (ACID)
- Session Cache → Redis (fast lookups)
- Recommendation → Neo4j (graph relationships)
```

**Benefits:**
- Use right tool for each job
- Optimize performance and cost
- Avoid one-size-fits-all

---

#### 5. Data Versioning and Replication

**Ensure fault tolerance:**

```javascript
// MongoDB: Replica set with 3 nodes
rs.initiate({
  _id: "myReplicaSet",
  members: [
    { _id: 0, host: "server1:27017" },
    { _id: 1, host: "server2:27017" },
    { _id: 2, host: "server3:27017" }
  ]
});

// Cassandra: Replication factor
CREATE KEYSPACE mydb WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'datacenter1': 3
};
```

---

#### 6. Monitor Performance

**Track key metrics:**
- Query latency
- Throughput (ops/sec)
- Disk usage
- Replication lag
- Cache hit ratio

**Tools:**
- MongoDB Ops Manager
- Datadog, New Relic
- Prometheus + Grafana

---

### P. Exam Tips

#### 1. Know Four Types of NoSQL

**Memorize:**
- **Document:** MongoDB, CouchDB
- **Key-Value:** Redis, DynamoDB
- **Column:** Cassandra, HBase
- **Graph:** Neo4j, Neptune

**Quick Recall:**
- **DKCG:** Document, Key-Value, Column, Graph

---

#### 2. Understand CAP Theorem

**Remember:**
- **CAP:** Consistency, Availability, Partition Tolerance
- **Pick TWO only** in distributed systems
- **CP:** MongoDB (consistent but may be unavailable)
- **AP:** Cassandra (available but eventually consistent)

**Exam Question Pattern:**
"Which CAP properties does Cassandra prioritize?" → **AP**

---

#### 3. Write Basic CRUD

**MongoDB:**

```javascript
// Insert
db.collection.insertOne({ name: "John", age: 30 });

// Read
db.collection.find({ age: { \$gte: 25 } });

// Update
db.collection.updateOne({ name: "John" }, { \$set: { age: 31 } });

// Delete
db.collection.deleteOne({ name: "John" });
```

**Redis:**

```bash
SET key "value"
GET key
DEL key
```

---

#### 4. Differentiate Big Data Tools

| Tool       | Purpose                | Type       |
| ---------- | ---------------------- | ---------- |
| Hadoop     | Batch processing       | Framework  |
| Spark      | In-memory processing   | Engine     |
| HBase      | Column-oriented DB     | Database   |
| Kafka      | Streaming              | Messaging  |
| Hive       | SQL on Hadoop          | Query tool |

**Mnemonic:** **HSHHK** (Hadoop, Spark, HBase, Hive, Kafka)

---

#### 5. Prepare Use Case

**Sample Answer:**

**Q:** "Why choose MongoDB for an e-commerce site?"

**A:**
- ✅ **Flexible schema:** Product attributes vary (books have ISBN, clothes have size)
- ✅ **Horizontal scaling:** Handle millions of products
- ✅ **JSON format:** Natural fit for web APIs
- ✅ **Rich queries:** Search, filter, sort products
- ✅ **High availability:** Replica sets for 24/7 uptime

---

### Summary

NoSQL databases provide flexible, scalable alternatives to traditional RDBMS, with four main types: Document (MongoDB), Key-Value (Redis), Column-Oriented (Cassandra), and Graph (Neo4j). The CAP theorem explains trade-offs in distributed systems (Consistency, Availability, Partition Tolerance — pick two). Big Data databases handle massive volumes using distributed storage (HDFS, S3) and parallel processing (Hadoop, Spark). Choose NoSQL for scalability, flexible schemas, and high performance, but consider SQL for strong ACID transactions and complex joins. Polyglot persistence combines both approaches for optimal results.
''',
    codeSnippet: '''
-- ========================================
-- 1. MongoDB - Document Database
-- ========================================

// Insert documents
db.students.insertOne({
  name: "Alice",
  age: 22,
  email: "alice@example.com",
  courses: ["DBMS", "Machine Learning", "AI"],
  address: {
    city: "New York",
    zip: "10001"
  }
});

db.students.insertMany([
  { name: "Bob", age: 24, course: "Data Science" },
  { name: "Charlie", age: 23, course: "AI" }
]);

// Query documents
db.students.find({ course: "Machine Learning" });

// Find with conditions
db.students.find({
  age: { \$gte: 22, \$lte: 25 },
  courses: { \$in: ["AI", "DBMS"] }
});

// Projection (select specific fields)
db.students.find(
  { age: { \$gte: 22 } },
  { name: 1, email: 1, _id: 0 }
);

// Update documents
db.students.updateOne(
  { name: "Alice" },
  { \$set: { age: 23, phone: "555-1234" } }
);

db.students.updateMany(
  { course: "AI" },
  { \$inc: { age: 1 } }
);

// Upsert (insert if not exists)
db.students.updateOne(
  { name: "Diana" },
  { \$set: { age: 21, course: "NLP" } },
  { upsert: true }
);

// Delete documents
db.students.deleteOne({ name: "Bob" });
db.students.deleteMany({ age: { \$lt: 20 } });

// Create indexes
db.students.createIndex({ email: 1 });
db.students.createIndex({ course: 1, age: -1 });
db.students.createIndex({ name: "text" });

// List indexes
db.students.getIndexes();

// Aggregation pipeline
db.students.aggregate([
  // Match stage
  { \$match: { age: { \$gte: 20 } } },
  
  // Group stage
  { \$group: {
      _id: "\$course",
      avgAge: { \$avg: "\$age" },
      count: { \$sum: 1 }
  }},
  
  // Sort stage
  { \$sort: { count: -1 } },
  
  // Limit stage
  { \$limit: 5 }
]);

// Nested array operations
db.students.find({ courses: "DBMS" });
db.students.updateOne(
  { name: "Alice" },
  { \$push: { courses: "Cloud Computing" } }
);

-- ========================================
-- 2. Redis - Key-Value Database
-- ========================================

# String operations
SET user:1001 "John Doe"
GET user:1001
DEL user:1001

# Increment counter
INCR page_views
INCRBY page_views 10

# Set with expiration (TTL)
SET session:abc123 "user_data" EX 3600
EXPIRE session:abc123 1800

# Hash (object storage)
HSET user:1001 name "John" age 30 email "john@example.com"
HGET user:1001 name
HGETALL user:1001
HDEL user:1001 email

# List (queue/stack)
LPUSH tasks "task1" "task2" "task3"
RPUSH tasks "task4"
LPOP tasks
RPOP tasks
LRANGE tasks 0 -1

# Set (unique values)
SADD tags "nosql" "database" "redis" "cache"
SMEMBERS tags
SISMEMBER tags "nosql"
SREM tags "cache"

# Sorted Set (leaderboard)
ZADD leaderboard 100 "player1" 200 "player2" 150 "player3"
ZRANGE leaderboard 0 -1 WITHSCORES
ZREVRANGE leaderboard 0 2  # Top 3 players
ZINCRBY leaderboard 50 "player1"

# Bit operations
SETBIT online_users 100 1
GETBIT online_users 100
BITCOUNT online_users

-- ========================================
-- 3. Cassandra - Column-Oriented Database
-- ========================================

-- Create keyspace
CREATE KEYSPACE school WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': 3
};

CREATE KEYSPACE ecommerce WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'datacenter1': 3,
  'datacenter2': 2
};

USE school;

-- Create table (column family)
CREATE TABLE students (
  id UUID PRIMARY KEY,
  name text,
  grade int,
  enrollment_date timestamp,
  courses set<text>
);

-- Composite primary key
CREATE TABLE student_scores (
  student_id UUID,
  course_name text,
  exam_date timestamp,
  score int,
  PRIMARY KEY (student_id, course_name)
);

-- Clustering order
CREATE TABLE sensor_data (
  sensor_id int,
  timestamp timestamp,
  temperature float,
  humidity float,
  PRIMARY KEY (sensor_id, timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC);

-- Insert data
INSERT INTO students (id, name, grade, enrollment_date)
VALUES (uuid(), 'Vikram', 90, toTimestamp(now()));

INSERT INTO student_scores (student_id, course_name, exam_date, score)
VALUES (123e4567-e89b-12d3-a456-426614174000, 'DBMS', toTimestamp(now()), 95);

-- Query by primary key
SELECT * FROM students WHERE id = 123e4567-e89b-12d3-a456-426614174000;

-- Query with clustering key
SELECT * FROM student_scores 
WHERE student_id = 123e4567-e89b-12d3-a456-426614174000 
AND course_name = 'DBMS';

-- Range query on clustering column
SELECT * FROM sensor_data
WHERE sensor_id = 101
AND timestamp >= '2025-10-01' AND timestamp <= '2025-10-18';

-- Update
UPDATE students SET grade = 95 WHERE id = ?;

-- Delete
DELETE FROM students WHERE id = ?;

-- Collection operations
UPDATE students SET courses = courses + {'Cloud Computing'} WHERE id = ?;

-- TTL (Time To Live)
INSERT INTO students (id, name) VALUES (uuid(), 'Temp') USING TTL 86400;

-- ========================================
-- 4. Neo4j - Graph Database (Cypher)
-- ========================================

-- Create nodes
CREATE (alice:Student {name:'Alice', age:22, major:'CS'})
CREATE (bob:Student {name:'Bob', age:24, major:'AI'})
CREATE (dbms:Course {code:'CS301', title:'Database Systems', credits:3})
CREATE (ai:Course {code:'CS401', title:'Artificial Intelligence', credits:4})

-- Create relationships
CREATE (alice)-[:ENROLLED_IN {semester:'Fall 2025', grade:'A'}]->(dbms)
CREATE (alice)-[:FRIENDS_WITH {since:2024}]->(bob)
CREATE (bob)-[:ENROLLED_IN {semester:'Fall 2025'}]->(ai)

-- Create complete graph in one statement
CREATE (alice:Student {name:'Alice'})-[:FRIENDS_WITH]->(bob:Student {name:'Bob'}),
       (alice)-[:ENROLLED_IN]->(dbms:Course {code:'CS301'}),
       (bob)-[:ENROLLED_IN]->(dbms)

-- Query: Find all friends of Alice
MATCH (a:Student {name:'Alice'})-[:FRIENDS_WITH]->(friend)
RETURN friend.name, friend.age;

-- Query: Find courses Alice is enrolled in
MATCH (s:Student {name:'Alice'})-[:ENROLLED_IN]->(c:Course)
RETURN c.code, c.title, c.credits;

-- Query: Find friends of friends (2 degrees)
MATCH (alice:Student {name:'Alice'})-[:FRIENDS_WITH*2]->(fof)
RETURN DISTINCT fof.name;

-- Shortest path
MATCH path = shortestPath(
  (alice:Student {name:'Alice'})-[:FRIENDS_WITH*]-(bob:Student {name:'Bob'})
)
RETURN path, length(path);

-- Recommendation: Courses friends are taking
MATCH (me:Student {name:'Alice'})-[:FRIENDS_WITH]->(friend)-[:ENROLLED_IN]->(course)
WHERE NOT (me)-[:ENROLLED_IN]->(course)
RETURN course.title, COUNT(friend) AS friend_count
ORDER BY friend_count DESC
LIMIT 5;

-- Update node properties
MATCH (s:Student {name:'Alice'})
SET s.age = 23, s.email = 'alice@example.com';

-- Delete relationship
MATCH (a:Student {name:'Alice'})-[r:FRIENDS_WITH]->(b:Student {name:'Bob'})
DELETE r;

-- Delete node and relationships
MATCH (s:Student {name:'Alice'})
DETACH DELETE s;

-- Pattern matching: Students in same course
MATCH (s1:Student)-[:ENROLLED_IN]->(c:Course)<-[:ENROLLED_IN]-(s2:Student)
WHERE s1.name < s2.name
RETURN s1.name, s2.name, c.title;

-- Count relationships
MATCH (s:Student)-[r:ENROLLED_IN]->()
RETURN s.name, COUNT(r) AS course_count
ORDER BY course_count DESC;

-- ========================================
-- 5. MongoDB Replica Set Configuration
-- ========================================

// Initialize replica set
rs.initiate({
  _id: "myReplicaSet",
  members: [
    { _id: 0, host: "server1:27017", priority: 2 },
    { _id: 1, host: "server2:27017", priority: 1 },
    { _id: 2, host: "server3:27017", arbiterOnly: true }
  ]
});

// Check replica set status
rs.status();

// Add new member
rs.add("server4:27017");

// Remove member
rs.remove("server4:27017");

// Read preference
db.students.find().readPref("secondary");

-- ========================================
-- 6. Cassandra Replication & Consistency
-- ========================================

-- Create keyspace with NetworkTopologyStrategy
CREATE KEYSPACE production WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'us_east': 3,
  'us_west': 2,
  'europe': 2
};

-- Consistency levels (in application code)
-- CONSISTENCY ONE;
-- CONSISTENCY QUORUM;
-- CONSISTENCY ALL;

-- Check cluster status
DESCRIBE CLUSTER;

-- ========================================
-- 7. Redis Persistence Configuration
-- ========================================

# RDB snapshot
SAVE
BGSAVE

# AOF (Append Only File)
CONFIG SET appendonly yes
BGREWRITEAOF

# Replication
SLAVEOF 192.168.1.100 6379
SLAVEOF NO ONE  # Promote to master

-- ========================================
-- 8. Big Data - Hadoop/Hive Examples
-- ========================================

-- Hive: Create external table
CREATE EXTERNAL TABLE logs (
  timestamp STRING,
  user_id INT,
  action STRING,
  page STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hadoop/logs';

-- Hive: Query
SELECT action, COUNT(*) as count
FROM logs
WHERE timestamp >= '2025-10-01'
GROUP BY action
ORDER BY count DESC;

-- Hive: Partitioned table
CREATE TABLE sales (
  order_id INT,
  product STRING,
  amount DECIMAL(10,2)
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET;

-- HBase: Create table
create 'students', 'personal', 'academic'

-- HBase: Put data
put 'students', 'row1', 'personal:name', 'Alice'
put 'students', 'row1', 'academic:grade', '90'

-- HBase: Get data
get 'students', 'row1'
scan 'students'

-- ========================================
-- 9. Comparison Queries
-- ========================================

-- Same query in different databases

-- MongoDB: Find students older than 22
db.students.find({ age: { \$gt: 22 } });

-- SQL (MySQL/PostgreSQL)
SELECT * FROM students WHERE age > 22;

-- Cassandra
SELECT * FROM students WHERE age > 22 ALLOW FILTERING;

-- Redis (Hash)
HSCAN user:* MATCH age:* COUNT 100

-- Neo4j (Cypher)
MATCH (s:Student) WHERE s.age > 22 RETURN s;
''',
    revisionPoints: [
      'NoSQL (Not Only SQL) databases are non-relational databases with flexible data models',
      'NoSQL provides horizontal scalability, high availability, and schema-less design',
      'Four main types: Document (MongoDB), Key-Value (Redis), Column-Oriented (Cassandra), Graph (Neo4j)',
      'Document databases store JSON-like documents with nested objects and arrays',
      'Key-Value stores provide simple key-value pairs for ultra-fast lookups and caching',
      'Column-oriented databases organize data by columns, optimized for analytics and time-series',
      'Graph databases store nodes and relationships, ideal for social networks and recommendations',
      'CAP Theorem: Distributed systems can only guarantee 2 of 3: Consistency, Availability, Partition Tolerance',
      'CP systems (MongoDB) prioritize Consistency and Partition Tolerance, may sacrifice Availability',
      'AP systems (Cassandra) prioritize Availability and Partition Tolerance, use eventual consistency',
      'MongoDB uses BSON format and supports rich queries, indexes, and aggregation pipelines',
      'Redis is in-memory database with data structures: strings, hashes, lists, sets, sorted sets',
      'Cassandra uses partition key for data distribution and clustering key for sorting within partitions',
      'Neo4j uses Cypher query language for pattern matching and graph traversal',
      'Schema-less design allows different fields in each document without migrations',
      'Horizontal scaling adds more servers (nodes) for linear scalability, unlike vertical scaling',
      'Eventual consistency means all replicas sync eventually, acceptable for many use cases',
      'Big Data databases handle massive volumes using distributed storage (HDFS, S3) and parallel processing',
      'Hadoop provides MapReduce for batch processing; Spark offers in-memory processing (100x faster)',
      'Hive enables SQL-like queries on Hadoop; HBase provides column-oriented NoSQL on HDFS',
      'Use SQL for ACID transactions, complex joins, and fixed schemas (banking, ERP)',
      'Use NoSQL for flexible schemas, massive scale, high performance (social media, IoT, gaming)',
      'Polyglot persistence combines SQL and NoSQL, using the right database for each use case',
      'NoSQL advantages: horizontal scalability, schema flexibility, high performance, cloud-native',
      'NoSQL disadvantages: weaker ACID, non-standard query languages, complex joins, eventual consistency',
    ],
    quizQuestions: [
      Question(
        question: 'Which is NOT a type of NoSQL database?',
        options: ['Document', 'Key-Value', 'Relational', 'Graph'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which NoSQL database is best for storing social network relationships?',
        options: ['MongoDB', 'Redis', 'Neo4j', 'Cassandra'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does CAP theorem stand for?',
        options: [
          'Create, Alter, Process',
          'Consistency, Availability, Partition Tolerance',
          'Cache, Archive, Persist',
          'Cluster, Availability, Performance'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which CAP properties does Cassandra prioritize?',
        options: ['CP (Consistency + Partition)', 'AP (Availability + Partition)', 'CA (Consistency + Availability)', 'All three'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which database is best for caching and session storage?',
        options: ['MongoDB', 'Redis', 'Neo4j', 'HBase'],
        correctIndex: 1,
      ),
      Question(
        question: 'What format does MongoDB use to store documents?',
        options: ['XML', 'CSV', 'BSON (Binary JSON)', 'Plain text'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which is a column-oriented NoSQL database?',
        options: ['MongoDB', 'Redis', 'Cassandra', 'Neo4j'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the main advantage of horizontal scaling?',
        options: [
          'Easier to implement',
          'Linear cost scaling by adding more servers',
          'Requires less hardware',
          'Better for single-node systems'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which query language is used by Neo4j?',
        options: ['SQL', 'MongoDB Query Language', 'Cypher', 'CQL'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does HDFS stand for in Big Data?',
        options: [
          'High Data File System',
          'Hadoop Distributed File System',
          'Hybrid Database File Store',
          'Horizontal Data Forwarding System'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is faster for in-memory processing: Hadoop MapReduce or Apache Spark?',
        options: ['Hadoop MapReduce', 'Apache Spark', 'Both are equal', 'Neither, they are not for processing'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is eventual consistency in NoSQL?',
        options: [
          'Data is always consistent across all nodes',
          'Data will be consistent eventually, but may be temporarily inconsistent',
          'Data is never consistent',
          'Only one node has consistent data'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'data_warehousing',
    title: '12. Data Warehousing & OLAP',
    explanation: '''## Data Warehousing & OLAP

### A. Introduction

**Definition:**

A **Data Warehouse (DW)** is a **centralized repository** that stores large volumes of historical data from multiple sources for **reporting, analysis, and decision-making**.

**OLAP (Online Analytical Processing)** is a technology that allows users to **analyze data from multiple perspectives** — using dimensions, hierarchies, and aggregations.

**Purpose:**

**Data Warehouse:**
- Consolidates data from disparate sources (databases, files, APIs)
- Stores historical data for trend analysis
- Optimized for complex queries and reporting
- Provides a single source of truth for the organization

**OLAP:**
- Supports **Business Intelligence (BI)**, dashboards, and analytics
- Enables **data-driven decisions** using summaries, trends, and patterns
- Allows interactive multidimensional analysis
- Provides fast query response for analytical workloads

**Example:**

An e-commerce company:
1. Stores sales data from **websites, mobile apps, and retail stores** into a **Data Warehouse**
2. Uses **OLAP cubes** to analyze:
   - Total revenue by **region** (North, South, East, West)
   - Sales by **product category** (Electronics, Clothing, Books)
   - Trends over **time** (Daily, Monthly, Yearly)
   - Performance by **customer segment** (New, Returning, VIP)

**Key Characteristics:**

| Feature               | Description                                |
| --------------------- | ------------------------------------------ |
| **Subject-Oriented**  | Organized around key business subjects    |
| **Integrated**        | Consolidates data from multiple sources   |
| **Time-Variant**      | Maintains historical data over time       |
| **Non-Volatile**      | Data is stable; rarely updated or deleted |

---

### B. Key Concepts

#### 1. Operational Database (OLTP) vs. Data Warehouse (OLAP)

**Fundamental Differences:**

| Feature              | OLTP (Online Transaction Processing)      | OLAP (Online Analytical Processing)        |
| -------------------- | ----------------------------------------- | ------------------------------------------ |
| **Purpose**          | Real-time transactions                    | Historical analysis and reporting          |
| **Data Type**        | Current, detailed, operational            | Historical, summarized, aggregated         |
| **Operations**       | INSERT, UPDATE, DELETE                    | SELECT, GROUP BY, ROLLUP, CUBE             |
| **Query Type**       | Simple, predefined                        | Complex, ad-hoc                            |
| **Users**            | Clerks, developers, customers             | Analysts, managers, executives             |
| **Data Size**        | Gigabytes                                 | Terabytes to Petabytes                     |
| **Schema**           | Highly normalized (3NF)                   | Denormalized (Star/Snowflake)              |
| **Response Time**    | Milliseconds                              | Seconds to minutes                         |
| **Transaction Rate** | Thousands per second                      | Low (batch queries)                        |
| **Example System**   | Banking transactions, e-commerce checkout | Sales analytics dashboard, financial reports |
| **Database**         | MySQL, PostgreSQL, Oracle                 | Redshift, BigQuery, Snowflake              |

**Detailed Comparison:**

**OLTP Example:**
```sql
-- Insert new order (real-time transaction)
INSERT INTO orders (customer_id, product_id, quantity, price)
VALUES (101, 205, 2, 59.99);

-- Update inventory (immediate)
UPDATE products SET stock = stock - 2 WHERE product_id = 205;
```

**OLAP Example:**
```sql
-- Analyze total sales by region and quarter (historical analysis)
SELECT region, QUARTER(order_date), SUM(total_amount)
FROM sales_fact
WHERE YEAR(order_date) = 2025
GROUP BY region, QUARTER(order_date);
```

---

#### 2. Data Mart

**Definition:**
- A **subset** of a data warehouse focused on a **specific business area**
- Smaller, departmental-level data warehouse

**Types:**
- **Dependent Data Mart:** Derived from central data warehouse
- **Independent Data Mart:** Standalone, sourced directly from operational systems

**Example:**
- **Sales Data Mart:** Only sales-related data
- **Finance Data Mart:** Only financial data
- **Marketing Data Mart:** Only marketing campaign data

**Benefits:**
- Faster queries (smaller dataset)
- Department-specific optimization
- Easier to manage
- Lower cost

---

### C. Data Warehouse Architecture

**Three-Tier Architecture:**

```
+------------------+
|  Presentation    |  ← OLAP Cubes, Dashboards, Reports
|  Layer           |     (Power BI, Tableau, Looker)
+------------------+
         ↑
+------------------+
|  Data Warehouse  |  ← Integrated, Historical Data
|  Database        |     (Star/Snowflake Schema)
+------------------+
         ↑
+------------------+
|  ETL Layer       |  ← Extract, Transform, Load
|  (Staging)       |     (Data Integration & Cleansing)
+------------------+
         ↑
+------------------+
|  Data Sources    |  ← ERP, CRM, APIs, Logs, Files
+------------------+
```

**Components:**

#### 1. Data Sources

**Types:**
- **Operational Databases:** ERP (SAP), CRM (Salesforce), HR systems
- **External Data:** Market data, weather, social media
- **Files:** CSV, Excel, JSON, XML
- **APIs:** Web services, streaming data
- **Logs:** Application logs, web server logs

---

#### 2. ETL Layer (Extract, Transform, Load)

**Extract:**
- Pull data from multiple heterogeneous sources
- Incremental extraction (only new/changed data)
- Full extraction (entire dataset)

**Transform:**
- **Cleansing:** Remove duplicates, fix errors, handle missing values
- **Standardization:** Unified formats (dates, currencies, codes)
- **Aggregation:** Summarize data (daily totals, monthly averages)
- **Enrichment:** Add calculated fields, lookups, derived columns
- **Validation:** Ensure data quality and integrity

**Load:**
- Insert transformed data into warehouse
- Batch loading (scheduled intervals)
- Real-time/streaming loading (CDC - Change Data Capture)

**ETL Example (Python + Pandas):**

```python
import pandas as pd

# EXTRACT: Read from multiple sources
sales = pd.read_csv("sales.csv")
customers = pd.read_csv("customers.csv")
products = pd.read_json("products.json")

# TRANSFORM: Clean and merge
sales['order_date'] = pd.to_datetime(sales['order_date'])
sales['total'] = sales['quantity'] * sales['price']

merged = sales.merge(customers, on="customer_id") \\
              .merge(products, on="product_id")

# Aggregation
summary = merged.groupby(['region', 'product_category']).agg({
    'total': 'sum',
    'quantity': 'sum'
}).reset_index()

# LOAD: Save to data warehouse
summary.to_sql('sales_fact', engine, if_exists='append', index=False)
```

---

#### 3. Staging Area

**Purpose:**
- Temporary storage for raw extracted data
- Buffer before transformation
- Allows validation and error handling

**Characteristics:**
- Not visible to end users
- Cleaned regularly (after successful load)
- May store rejected/error records

---

#### 4. Data Warehouse Database

**Storage:**
- Stores integrated, transformed data
- Optimized for analytical queries
- Columnar storage for faster aggregations

**Schema Design:**
- Star Schema (most common)
- Snowflake Schema (normalized dimensions)
- Galaxy Schema (multiple fact tables)

---

#### 5. Presentation Layer

**OLAP Cubes:**
- Multidimensional data structures
- Pre-aggregated for fast queries

**Dashboards & Reports:**
- **Power BI:** Microsoft's BI tool
- **Tableau:** Visual analytics platform
- **Looker:** Google's data exploration tool
- **QlikView:** Associative analytics

---

### D. Data Warehouse Schema Models

#### 1. Star Schema

**Structure:**
- **Central Fact Table:** Contains measures (metrics)
- **Dimension Tables:** Surround fact table (denormalized)
- Simple, intuitive design

**Visual Representation:**

```
          +---------------+
          |   Time Dim    |
          +---------------+
          | date_key (PK) |
          | date          |
          | month         |
          | quarter       |
          | year          |
          +---------------+
                 |
                 |
+-------------+  |  +----------------+     +---------------+
| Product Dim |  |  |   Sales Fact   |     | Customer Dim  |
+-------------+  |  +----------------+     +---------------+
| product_key |--+--| sale_id (PK)   |-----| customer_key  |
| name        |     | date_key (FK)  |     | name          |
| category    |     | product_key(FK)|     | city          |
| price       |     | customer_key(FK)|    | state         |
+-------------+     | quantity       |     | country       |
                    | amount         |     +---------------+
                    +----------------+
                           |
                           |
                    +---------------+
                    |  Region Dim   |
                    +---------------+
                    | region_key(PK)|
                    | region_name   |
                    | country       |
                    +---------------+
```

**SQL Example:**

```sql
-- Create fact table
CREATE TABLE sales_fact (
  sale_id INT PRIMARY KEY,
  date_key INT,
  product_key INT,
  customer_key INT,
  region_key INT,
  quantity INT,
  amount DECIMAL(10,2),
  FOREIGN KEY (date_key) REFERENCES dim_date(date_key),
  FOREIGN KEY (product_key) REFERENCES dim_product(product_key),
  FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key),
  FOREIGN KEY (region_key) REFERENCES dim_region(region_key)
);

-- Create dimension table
CREATE TABLE dim_product (
  product_key INT PRIMARY KEY,
  product_name VARCHAR(100),
  category VARCHAR(50),
  price DECIMAL(10,2)
);
```

**Advantages:**
- Simple to understand and implement
- Fast query performance (fewer joins)
- Easy to navigate for end users

**Disadvantages:**
- Data redundancy in dimensions
- Larger storage footprint

---

#### 2. Snowflake Schema

**Structure:**
- **Normalized Dimensions:** Dimension tables split into sub-dimensions
- Reduces redundancy
- More complex structure

**Visual Representation:**

```
+----------------+
|  Sales Fact    |
+----------------+
| product_key(FK)|---+
| category_key   |   |
+----------------+   |
                     |
              +-------------+      +---------------+
              | Product Dim |      | Category Dim  |
              +-------------+      +---------------+
              | product_key |------| category_key  |
              | name        |      | category_name |
              +-------------+      | department    |
                                   +---------------+
                                          |
                                   +---------------+
                                   | Department Dim|
                                   +---------------+
                                   | dept_key      |
                                   | dept_name     |
                                   +---------------+
```

**SQL Example:**

```sql
-- Normalized dimension hierarchy
CREATE TABLE dim_product (
  product_key INT PRIMARY KEY,
  product_name VARCHAR(100),
  category_key INT,
  FOREIGN KEY (category_key) REFERENCES dim_category(category_key)
);

CREATE TABLE dim_category (
  category_key INT PRIMARY KEY,
  category_name VARCHAR(50),
  department_key INT,
  FOREIGN KEY (department_key) REFERENCES dim_department(dept_key)
);

CREATE TABLE dim_department (
  dept_key INT PRIMARY KEY,
  dept_name VARCHAR(50)
);
```

**Advantages:**
- Less storage (normalized)
- Easier maintenance
- Data consistency

**Disadvantages:**
- More complex queries (more joins)
- Slower query performance

---

#### 3. Galaxy Schema (Fact Constellation)

**Structure:**
- **Multiple Fact Tables** sharing dimension tables
- Complex enterprise-wide warehouse

**Example:**

```
         +---------------+
         |   Time Dim    |
         +---------------+
               |     |
       +-------+     +-------+
       |                     |
+----------------+    +------------------+
|  Sales Fact    |    |  Inventory Fact  |
+----------------+    +------------------+
| product_key(FK)|----| product_key (FK) |
+----------------+    +------------------+
       |                     |
       +-------+     +-------+
               |     |
         +---------------+
         | Product Dim   |
         +---------------+
```

**Use Case:**
- Multiple business processes (Sales + Inventory)
- Shared dimensions (Time, Product)

---

### E. ETL Process (Detailed)

#### 1. Extract

**Methods:**

**Full Extraction:**
- Extract entire dataset
- Simple but resource-intensive
- Used for initial load

```sql
SELECT * FROM source_db.orders;
```

**Incremental Extraction:**
- Extract only new/changed records
- Uses timestamps or change tracking

```sql
SELECT * FROM source_db.orders
WHERE updated_at > '2025-10-17 00:00:00';
```

**Change Data Capture (CDC):**
- Track database changes (inserts, updates, deletes)
- Near real-time extraction

---

#### 2. Transform

**Data Cleansing:**

```sql
-- Remove duplicates
SELECT DISTINCT customer_id, customer_name, email
FROM staging.customers;

-- Handle NULL values
SELECT 
  customer_id,
  COALESCE(phone, 'N/A') AS phone,
  COALESCE(address, 'Unknown') AS address
FROM staging.customers;

-- Fix data types
SELECT 
  customer_id,
  UPPER(customer_name) AS customer_name,
  TO_DATE(registration_date, 'YYYY-MM-DD') AS reg_date
FROM staging.customers;
```

**Data Standardization:**

```sql
-- Standardize country codes
SELECT 
  CASE country
    WHEN 'USA' THEN 'United States'
    WHEN 'UK' THEN 'United Kingdom'
    WHEN 'UAE' THEN 'United Arab Emirates'
    ELSE country
  END AS standardized_country
FROM staging.customers;

-- Currency conversion
SELECT 
  amount,
  currency,
  amount * exchange_rate AS amount_usd
FROM staging.transactions;
```

**Data Aggregation:**

```sql
-- Daily sales summary
SELECT 
  DATE(order_date) AS sale_date,
  product_id,
  SUM(quantity) AS total_quantity,
  SUM(amount) AS total_amount,
  COUNT(*) AS order_count
FROM staging.orders
GROUP BY DATE(order_date), product_id;
```

**Derived Columns:**

```sql
-- Add calculated fields
SELECT 
  customer_id,
  order_date,
  amount,
  YEAR(order_date) AS sale_year,
  QUARTER(order_date) AS sale_quarter,
  MONTH(order_date) AS sale_month,
  CASE 
    WHEN amount >= 1000 THEN 'High Value'
    WHEN amount >= 500 THEN 'Medium Value'
    ELSE 'Low Value'
  END AS order_segment
FROM staging.orders;
```

---

#### 3. Load

**Bulk Load:**

```sql
-- Insert transformed data into fact table
INSERT INTO warehouse.sales_fact 
  (product_id, date_id, customer_id, quantity, amount)
SELECT 
  product_id, 
  date_id, 
  customer_id,
  SUM(quantity) AS total_quantity,
  SUM(total) AS total_amount
FROM staging.sales
GROUP BY product_id, date_id, customer_id;
```

**Incremental Load:**

```sql
-- Upsert (insert or update)
MERGE INTO warehouse.sales_fact AS target
USING staging.sales AS source
ON target.sale_id = source.sale_id
WHEN MATCHED THEN
  UPDATE SET quantity = source.quantity, amount = source.amount
WHEN NOT MATCHED THEN
  INSERT (sale_id, product_id, date_id, quantity, amount)
  VALUES (source.sale_id, source.product_id, source.date_id, 
          source.quantity, source.amount);
```

---

### F. OLAP (Online Analytical Processing)

**Definition:**

OLAP provides a **multidimensional view** of data, enabling **fast analytical queries** through pre-aggregated cubes and optimized structures.

**Key Features:**

✅ **Aggregation and Summarization:** Pre-calculated totals, averages, counts
✅ **Drill-down and Roll-up:** Navigate hierarchies (Year → Quarter → Month)
✅ **Pivot and Slice-and-Dice:** View data from different angles
✅ **Fast Query Response:** Optimized for complex analytical queries
✅ **Multidimensional Analysis:** Analyze across multiple dimensions simultaneously

**OLAP Cube Structure:**

```
           Time Dimension
              (Years)
                 ↓
    Product ←   CUBE   → Region
   Dimension              Dimension
                 ↑
            Measures
        (Sales, Quantity)
```

**OLAP Types:**

| Type        | Full Name                    | Description                                           | Storage             | Example                      |
| ----------- | ---------------------------- | ----------------------------------------------------- | ------------------- | ---------------------------- |
| **MOLAP**   | Multidimensional OLAP        | Data stored in multidimensional cubes                 | Proprietary cubes   | Microsoft Analysis Services  |
| **ROLAP**   | Relational OLAP              | Uses relational databases for OLAP operations         | Relational tables   | Oracle OLAP, SQL-based       |
| **HOLAP**   | Hybrid OLAP                  | Combines MOLAP and ROLAP                              | Cubes + Tables      | SAP BW, IBM Cognos           |
| **DOLAP**   | Desktop OLAP                 | Local cubes on user's desktop                         | Local files         | Excel Pivot Tables           |

**Detailed Comparison:**

**MOLAP:**
- **Pros:** Fastest query performance, pre-aggregated
- **Cons:** Storage intensive, limited scalability
- **Use Case:** Executive dashboards, fixed reports

**ROLAP:**
- **Pros:** Scalable, uses existing RDBMS
- **Cons:** Slower than MOLAP, complex SQL
- **Use Case:** Large datasets, flexible queries

**HOLAP:**
- **Pros:** Best of both worlds
- **Cons:** Complex architecture
- **Use Case:** Enterprise BI systems

---

### G. OLAP Operations

#### 1. Roll-Up (Aggregation)

**Definition:** Summarize data along a dimension hierarchy (detailed → summary)

**Example:** City → State → Country → Region

```sql
-- Roll-up from city to country
SELECT country, SUM(sales_amount) AS total_sales
FROM sales_fact
JOIN dim_location ON sales_fact.location_key = dim_location.location_key
GROUP BY country;

-- Roll-up from month to year
SELECT YEAR(order_date) AS year, SUM(amount) AS total_sales
FROM sales_fact
GROUP BY YEAR(order_date);
```

**Visual:**
```
City Sales:
  NYC: 100K, LA: 80K, Chicago: 60K

State Roll-up:
  NY: 100K, CA: 80K, IL: 60K

Country Roll-up:
  USA: 240K
```

---

#### 2. Drill-Down (Detailed View)

**Definition:** Move from summary to detailed data (summary → detailed)

**Example:** Country → State → City → Store

```sql
-- Drill-down from country to city
SELECT city, SUM(sales_amount) AS total_sales
FROM sales_fact
JOIN dim_location ON sales_fact.location_key = dim_location.location_key
WHERE country = 'USA'
GROUP BY city;

-- Drill-down from year to month
SELECT MONTH(order_date) AS month, SUM(amount) AS total_sales
FROM sales_fact
WHERE YEAR(order_date) = 2025
GROUP BY MONTH(order_date);
```

---

#### 3. Slice

**Definition:** Extract a single "slice" from the cube (filter one dimension)

**Example:** Sales for year **2025** only

```sql
-- Slice: Only 2025 data
SELECT product_category, region, SUM(sales_amount)
FROM sales_fact
WHERE YEAR(order_date) = 2025
GROUP BY product_category, region;
```

**Visual:**
```
Full Cube: [Product × Region × Time]
Sliced Cube: [Product × Region] for Time=2025
```

---

#### 4. Dice

**Definition:** Select a sub-cube based on multiple dimension filters

**Example:** Sales for **2024-2025** AND **Category = Electronics** AND **Region = North**

```sql
-- Dice: Multiple filters
SELECT product_name, YEAR(order_date), SUM(sales_amount)
FROM sales_fact
WHERE YEAR(order_date) BETWEEN 2024 AND 2025
  AND product_category = 'Electronics'
  AND region = 'North'
GROUP BY product_name, YEAR(order_date);
```

**Visual:**
```
Full Cube: [Product × Region × Time]
Diced Cube: [Electronics × North × 2024-2025]
```

---

#### 5. Pivot (Rotate)

**Definition:** Reorient data to view different perspectives (swap dimensions)

**Example:** Rows ↔ Columns

```sql
-- Before Pivot: Rows = Products, Columns = Metrics
SELECT product_name, SUM(sales_amount), SUM(quantity)
FROM sales_fact
GROUP BY product_name;

-- After Pivot: Rows = Regions, Columns = Products
SELECT region,
  SUM(CASE WHEN product_category = 'Electronics' THEN sales_amount ELSE 0 END) AS Electronics,
  SUM(CASE WHEN product_category = 'Clothing' THEN sales_amount ELSE 0 END) AS Clothing,
  SUM(CASE WHEN product_category = 'Books' THEN sales_amount ELSE 0 END) AS Books
FROM sales_fact
GROUP BY region;
```

---

### H. Data Warehouse Technologies

| Category              | Tools/Technologies                                          | Description                          |
| --------------------- | ----------------------------------------------------------- | ------------------------------------ |
| **ETL Tools**         | Informatica, Talend, Apache NiFi, AWS Glue, Azure Data Factory | Extract, Transform, Load pipelines |
| **Data Warehouses**   | Amazon Redshift, Google BigQuery, Snowflake, Teradata, Azure Synapse | Cloud/on-prem warehouses           |
| **OLAP Engines**      | Apache Kylin, Microsoft SSAS, SAP BW, Oracle Essbase        | Multidimensional analysis           |
| **Visualization**     | Tableau, Power BI, Looker, QlikView, Sisense                | Dashboards and reports              |
| **Data Integration**  | Fivetran, Stitch, Apache Airflow                            | Automated data pipelines            |
| **Columnar Storage**  | Apache Parquet, ORC, Amazon Redshift Spectrum               | Optimized for analytics             |

**Cloud Data Warehouses:**

| Platform           | Provider   | Key Features                                    |
| ------------------ | ---------- | ----------------------------------------------- |
| **Amazon Redshift** | AWS       | Massively parallel processing, columnar storage |
| **Google BigQuery** | GCP       | Serverless, petabyte-scale, SQL queries         |
| **Snowflake**      | Multi-cloud| Separation of storage and compute, auto-scaling |
| **Azure Synapse**  | Microsoft  | Unified analytics, Spark integration            |

---

### I. Advantages of Data Warehousing

✅ **Data Integration:** Consolidates data from multiple heterogeneous sources
✅ **Faster Analytics:** Optimized for complex queries and reporting
✅ **Historical Analysis:** Stores years of historical data for trend analysis
✅ **Improved Decision-Making:** Provides accurate, timely insights to executives
✅ **Single Source of Truth:** Eliminates data silos and inconsistencies
✅ **Business Intelligence:** Powers dashboards, scorecards, and KPIs
✅ **Data Quality:** ETL process cleanses and standardizes data
✅ **Scalability:** Cloud warehouses scale to petabytes

---

### J. Challenges

❌ **Complex ETL Processes:** Requires skilled developers, error handling, monitoring
❌ **High Costs:** Storage, compute, licensing (on-prem warehouses)
❌ **Data Latency:** Batch loads cause delays (hours to days)
❌ **Schema Rigidity:** Schema changes require redesign and reload
❌ **Maintenance Overhead:** Regular tuning, indexing, partitioning needed
❌ **Data Governance:** Security, privacy, compliance challenges
❌ **Integration Complexity:** Connecting diverse data sources

---

### K. Real-World Use Cases

| Industry       | Application                                          | OLAP Analysis                              |
| -------------- | ---------------------------------------------------- | ------------------------------------------ |
| **Retail**     | Sales trends, demand forecasting, inventory          | Revenue by region, product, time           |
| **Banking**    | Fraud detection, customer analytics, risk assessment | Transaction patterns, customer segments    |
| **Healthcare** | Patient data analysis, billing, outcomes             | Treatments by condition, demographics      |
| **Telecom**    | Customer churn, usage analytics, network performance | Call patterns, data usage by region        |
| **Education**  | Student performance dashboards, enrollment trends    | Grades by course, demographics, time       |
| **E-commerce** | Customer behavior, product recommendations           | Purchase patterns, cart abandonment        |
| **Manufacturing** | Supply chain optimization, quality control        | Production by plant, defects by product    |

**Detailed Example: Retail Analytics**

**Business Question:**
"Which product categories drive the most revenue in each region during the holiday season?"

**OLAP Query:**
```sql
SELECT 
  r.region_name,
  p.product_category,
  SUM(f.amount) AS total_revenue,
  COUNT(DISTINCT f.customer_key) AS unique_customers
FROM sales_fact f
JOIN dim_region r ON f.region_key = r.region_key
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.month IN (11, 12) -- November, December
  AND d.year = 2025
GROUP BY r.region_name, p.product_category
ORDER BY total_revenue DESC;
```

---

### L. Example SQL: OLAP Aggregation

#### 1. ROLLUP

**Hierarchical Aggregation:**

```sql
-- ROLLUP: Subtotals at each level
SELECT 
  region, 
  country, 
  SUM(sales_amount) AS total_sales
FROM sales_fact
GROUP BY ROLLUP(region, country);

-- Output:
-- Region    Country       Total_Sales
-- North     USA           5000
-- North     Canada        3000
-- North     NULL          8000  ← Subtotal for North
-- South     Mexico        2000
-- South     Brazil        1500
-- South     NULL          3500  ← Subtotal for South
-- NULL      NULL          11500 ← Grand Total
```

**With Multiple Dimensions:**

```sql
SELECT 
  YEAR(order_date) AS year,
  QUARTER(order_date) AS quarter,
  product_category,
  SUM(amount) AS total_sales
FROM sales_fact
GROUP BY ROLLUP(YEAR(order_date), QUARTER(order_date), product_category);
```

---

#### 2. CUBE

**Multidimensional Aggregates:**

```sql
-- CUBE: All possible combinations
SELECT 
  product_category, 
  region, 
  SUM(sales_amount) AS total_sales
FROM sales_fact
GROUP BY CUBE(product_category, region);

-- Output includes:
-- 1. Total by category and region
-- 2. Total by category (all regions)
-- 3. Total by region (all categories)
-- 4. Grand total (all categories, all regions)
```

**Example Output:**

| Category      | Region | Total_Sales |
| ------------- | ------ | ----------- |
| Electronics   | North  | 5000        |
| Electronics   | South  | 3000        |
| Electronics   | NULL   | 8000        |
| Clothing      | North  | 2000        |
| Clothing      | South  | 1500        |
| Clothing      | NULL   | 3500        |
| NULL          | North  | 7000        |
| NULL          | South  | 4500        |
| NULL          | NULL   | 11500       |

---

#### 3. GROUPING SETS

**Custom Aggregation Groups:**

```sql
SELECT 
  product_category,
  region,
  YEAR(order_date) AS year,
  SUM(sales_amount) AS total_sales
FROM sales_fact
GROUP BY GROUPING SETS (
  (product_category, region),
  (product_category, YEAR(order_date)),
  (region, YEAR(order_date)),
  ()
);
```

---

#### 4. Window Functions for OLAP

```sql
-- Running total
SELECT 
  order_date,
  amount,
  SUM(amount) OVER (ORDER BY order_date) AS running_total
FROM sales_fact;

-- Ranking by region
SELECT 
  region,
  product_name,
  sales_amount,
  RANK() OVER (PARTITION BY region ORDER BY sales_amount DESC) AS rank
FROM sales_fact;

-- Moving average
SELECT 
  order_date,
  amount,
  AVG(amount) OVER (
    ORDER BY order_date 
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS moving_avg_7days
FROM sales_fact;
```

---

### M. Best Practices

#### 1. Design Using Star Schema

**Why Star Schema?**
- Simplest to understand for business users
- Fastest query performance (fewer joins)
- Easy to navigate and maintain

**Implementation:**
```sql
-- Keep dimensions denormalized
CREATE TABLE dim_product (
  product_key INT PRIMARY KEY,
  product_name VARCHAR(100),
  category VARCHAR(50),
  subcategory VARCHAR(50),
  brand VARCHAR(50),
  price DECIMAL(10,2)
);
```

---

#### 2. Use Indexes and Materialized Views

**Indexes:**

```sql
-- Index foreign keys in fact table
CREATE INDEX idx_sales_date ON sales_fact(date_key);
CREATE INDEX idx_sales_product ON sales_fact(product_key);

-- Composite index for common queries
CREATE INDEX idx_sales_date_region ON sales_fact(date_key, region_key);
```

**Materialized Views:**

```sql
-- Pre-aggregate common queries
CREATE MATERIALIZED VIEW mv_monthly_sales AS
SELECT 
  YEAR(d.order_date) AS year,
  MONTH(d.order_date) AS month,
  p.product_category,
  SUM(f.amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY YEAR(d.order_date), MONTH(d.order_date), p.product_category;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW mv_monthly_sales;
```

---

#### 3. Incremental ETL Jobs

**Schedule Smart Loads:**

```sql
-- Load only new data (delta load)
INSERT INTO sales_fact (date_key, product_key, amount)
SELECT date_key, product_key, SUM(amount)
FROM staging.sales
WHERE load_date = CURRENT_DATE
GROUP BY date_key, product_key;

-- Track last load timestamp
UPDATE etl_metadata 
SET last_load_time = CURRENT_TIMESTAMP
WHERE table_name = 'sales_fact';
```

**Benefits:**
- Reduced load time
- Lower resource consumption
- Near real-time updates

---

#### 4. Partition Large Fact Tables

**Date Partitioning:**

```sql
-- Partition by year
CREATE TABLE sales_fact (
  sale_id INT,
  order_date DATE,
  amount DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(order_date)) (
  PARTITION p2023 VALUES LESS THAN (2024),
  PARTITION p2024 VALUES LESS THAN (2025),
  PARTITION p2025 VALUES LESS THAN (2026)
);
```

**Benefits:**
- Faster queries (partition pruning)
- Easier maintenance (drop old partitions)
- Improved performance

---

#### 5. Archive Old Data

**Data Retention Policy:**

```sql
-- Archive data older than 3 years
INSERT INTO sales_fact_archive
SELECT * FROM sales_fact
WHERE YEAR(order_date) < YEAR(CURRENT_DATE) - 3;

-- Delete archived data from active warehouse
DELETE FROM sales_fact
WHERE YEAR(order_date) < YEAR(CURRENT_DATE) - 3;
```

---

### N. Exam Tips

#### 1. Understand OLTP vs OLAP

**Key Differences to Memorize:**
- **OLTP:** Transactions, INSERT/UPDATE/DELETE, normalized, current data
- **OLAP:** Analysis, SELECT/GROUP BY, denormalized, historical data

**Sample Question:**
"Which system is optimized for complex analytical queries?" → **OLAP**

---

#### 2. Draw and Explain Schemas

**Star Schema:**
- Central fact table
- Denormalized dimensions
- Simple design

**Snowflake Schema:**
- Normalized dimensions
- Multiple hierarchy levels
- Reduced redundancy

**Practice:**
Draw a Star Schema for a university (Students, Courses, Enrollments)

---

#### 3. Remember ETL Stages

**Mnemonic: ETL**
- **E**xtract: Pull from sources
- **T**ransform: Clean, standardize, aggregate
- **L**oad: Insert into warehouse

**Sample Question:**
"What is the purpose of the Transform stage?" → Clean, standardize, and prepare data

---

#### 4. Practice ROLLUP and CUBE

**ROLLUP:**
```sql
GROUP BY ROLLUP(region, country) -- Hierarchical subtotals
```

**CUBE:**
```sql
GROUP BY CUBE(category, region) -- All combinations
```

**Tip:** ROLLUP is hierarchical, CUBE is multidimensional

---

#### 5. Know OLAP Types

**Memorize:**
- **MOLAP:** Multidimensional cubes (fastest)
- **ROLAP:** Relational tables (scalable)
- **HOLAP:** Hybrid (best of both)

**Sample Question:**
"Which OLAP type stores data in cubes?" → **MOLAP**

---

#### 6. Real-World Use Cases

**Prepare Short Answers:**

**Q:** "How do retail companies use OLAP for trend analysis?"

**A:**
- Build data warehouse from POS systems, online stores, inventory
- Create OLAP cube with dimensions: Time, Product, Region, Customer
- Analyze sales trends using drill-down (year → quarter → month)
- Identify best-selling products by region
- Forecast demand for inventory planning

---

### Summary

Data Warehousing consolidates data from multiple sources into a centralized repository for analytics and reporting. OLAP (Online Analytical Processing) enables multidimensional analysis through cubes, allowing drill-down, roll-up, slice, and dice operations. Star and Snowflake schemas organize data for efficient querying. The ETL process extracts data from sources, transforms it (cleaning, aggregation), and loads it into the warehouse. Technologies like Redshift, BigQuery, and Snowflake power modern cloud data warehouses, while tools like Tableau and Power BI provide visualization. OLAP types include MOLAP (cubes), ROLAP (relational), and HOLAP (hybrid), each with trade-offs between performance and scalability.
''',
    codeSnippet: '''
-- ========================================
-- 1. Star Schema - Fact and Dimension Tables
-- ========================================

-- Create Date Dimension
CREATE TABLE dim_date (
  date_key INT PRIMARY KEY,
  date DATE NOT NULL,
  day_of_week VARCHAR(10),
  day_of_month INT,
  month INT,
  month_name VARCHAR(10),
  quarter INT,
  year INT,
  is_weekend BOOLEAN,
  is_holiday BOOLEAN
);

-- Create Product Dimension
CREATE TABLE dim_product (
  product_key INT PRIMARY KEY,
  product_id VARCHAR(20) UNIQUE,
  product_name VARCHAR(100),
  category VARCHAR(50),
  subcategory VARCHAR(50),
  brand VARCHAR(50),
  unit_price DECIMAL(10,2),
  cost_price DECIMAL(10,2)
);

-- Create Customer Dimension
CREATE TABLE dim_customer (
  customer_key INT PRIMARY KEY,
  customer_id VARCHAR(20) UNIQUE,
  customer_name VARCHAR(100),
  email VARCHAR(100),
  city VARCHAR(50),
  state VARCHAR(50),
  country VARCHAR(50),
  customer_segment VARCHAR(20),
  registration_date DATE
);

-- Create Region Dimension
CREATE TABLE dim_region (
  region_key INT PRIMARY KEY,
  region_name VARCHAR(50),
  country VARCHAR(50),
  state VARCHAR(50),
  city VARCHAR(50),
  postal_code VARCHAR(10)
);

-- Create Sales Fact Table (Star Schema)
CREATE TABLE sales_fact (
  sale_id INT PRIMARY KEY,
  date_key INT NOT NULL,
  product_key INT NOT NULL,
  customer_key INT NOT NULL,
  region_key INT NOT NULL,
  quantity INT,
  unit_price DECIMAL(10,2),
  discount_percent DECIMAL(5,2),
  sales_amount DECIMAL(12,2),
  cost_amount DECIMAL(12,2),
  profit_amount DECIMAL(12,2),
  FOREIGN KEY (date_key) REFERENCES dim_date(date_key),
  FOREIGN KEY (product_key) REFERENCES dim_product(product_key),
  FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key),
  FOREIGN KEY (region_key) REFERENCES dim_region(region_key)
);

-- ========================================
-- 2. Snowflake Schema - Normalized Dimensions
-- ========================================

-- Category Dimension (normalized from Product)
CREATE TABLE dim_category (
  category_key INT PRIMARY KEY,
  category_name VARCHAR(50),
  department_key INT,
  FOREIGN KEY (department_key) REFERENCES dim_department(dept_key)
);

-- Department Dimension
CREATE TABLE dim_department (
  dept_key INT PRIMARY KEY,
  dept_name VARCHAR(50)
);

-- Product Dimension (Snowflake - references Category)
CREATE TABLE dim_product_snowflake (
  product_key INT PRIMARY KEY,
  product_name VARCHAR(100),
  category_key INT,
  brand VARCHAR(50),
  unit_price DECIMAL(10,2),
  FOREIGN KEY (category_key) REFERENCES dim_category(category_key)
);

-- ========================================
-- 3. ETL - Extract, Transform, Load
-- ========================================

-- EXTRACT: Pull data from source
CREATE TABLE staging_sales AS
SELECT * FROM source_db.orders
WHERE order_date >= CURRENT_DATE - INTERVAL '7 days';

-- TRANSFORM: Clean and prepare data
CREATE TABLE staging_sales_cleaned AS
SELECT 
  order_id,
  customer_id,
  product_id,
  UPPER(TRIM(customer_name)) AS customer_name,
  TO_DATE(order_date, 'YYYY-MM-DD') AS order_date,
  COALESCE(quantity, 0) AS quantity,
  CAST(price AS DECIMAL(10,2)) AS price,
  quantity * price AS total_amount,
  CASE 
    WHEN total_amount >= 1000 THEN 'High Value'
    WHEN total_amount >= 500 THEN 'Medium Value'
    ELSE 'Low Value'
  END AS order_segment
FROM staging_sales
WHERE customer_id IS NOT NULL
  AND product_id IS NOT NULL;

-- LOAD: Insert into fact table
INSERT INTO sales_fact (date_key, product_key, customer_key, quantity, sales_amount)
SELECT 
  d.date_key,
  p.product_key,
  c.customer_key,
  SUM(s.quantity) AS total_quantity,
  SUM(s.total_amount) AS total_sales
FROM staging_sales_cleaned s
JOIN dim_date d ON s.order_date = d.date
JOIN dim_product p ON s.product_id = p.product_id
JOIN dim_customer c ON s.customer_id = c.customer_id
GROUP BY d.date_key, p.product_key, c.customer_key;

-- ========================================
-- 4. OLAP Operations - Roll-Up
-- ========================================

-- Roll-up from City to Country
SELECT 
  c.country,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_customer c ON f.customer_key = c.customer_key
GROUP BY c.country
ORDER BY total_sales DESC;

-- Roll-up from Month to Year
SELECT 
  d.year,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY d.year
ORDER BY d.year;

-- Hierarchical Roll-up: City → State → Country
SELECT 
  c.country,
  c.state,
  c.city,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_customer c ON f.customer_key = c.customer_key
GROUP BY ROLLUP(c.country, c.state, c.city);

-- ========================================
-- 5. OLAP Operations - Drill-Down
-- ========================================

-- Drill-down from Year to Quarter
SELECT 
  d.year,
  d.quarter,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2025
GROUP BY d.year, d.quarter
ORDER BY d.quarter;

-- Drill-down from Country to City
SELECT 
  c.country,
  c.state,
  c.city,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE c.country = 'USA'
GROUP BY c.country, c.state, c.city
ORDER BY total_sales DESC;

-- ========================================
-- 6. OLAP Operations - Slice and Dice
-- ========================================

-- Slice: Only 2025 data
SELECT 
  p.category,
  c.country,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE d.year = 2025
GROUP BY p.category, c.country;

-- Dice: Multiple filters (2024-2025, Electronics, USA)
SELECT 
  d.year,
  d.quarter,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE d.year BETWEEN 2024 AND 2025
  AND p.category = 'Electronics'
  AND c.country = 'USA'
GROUP BY d.year, d.quarter;

-- ========================================
-- 7. OLAP Operations - Pivot
-- ========================================

-- Pivot: Rows = Region, Columns = Product Categories
SELECT 
  r.region_name,
  SUM(CASE WHEN p.category = 'Electronics' THEN f.sales_amount ELSE 0 END) AS Electronics,
  SUM(CASE WHEN p.category = 'Clothing' THEN f.sales_amount ELSE 0 END) AS Clothing,
  SUM(CASE WHEN p.category = 'Books' THEN f.sales_amount ELSE 0 END) AS Books,
  SUM(CASE WHEN p.category = 'Home & Garden' THEN f.sales_amount ELSE 0 END) AS Home_Garden
FROM sales_fact f
JOIN dim_region r ON f.region_key = r.region_key
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY r.region_name;

-- ========================================
-- 8. ROLLUP - Hierarchical Aggregation
-- ========================================

-- ROLLUP: Subtotals at each level
SELECT 
  c.country,
  c.state,
  c.city,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_customer c ON f.customer_key = c.customer_key
GROUP BY ROLLUP(c.country, c.state, c.city);

-- ROLLUP with Time Hierarchy
SELECT 
  d.year,
  d.quarter,
  d.month,
  p.category,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY ROLLUP(d.year, d.quarter, d.month, p.category);

-- ========================================
-- 9. CUBE - Multidimensional Aggregation
-- ========================================

-- CUBE: All possible combinations
SELECT 
  p.category,
  c.country,
  d.year,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_customer c ON f.customer_key = c.customer_key
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY CUBE(p.category, c.country, d.year);

-- CUBE with 2 dimensions
SELECT 
  p.category,
  r.region_name,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_region r ON f.region_key = r.region_key
GROUP BY CUBE(p.category, r.region_name);

-- ========================================
-- 10. GROUPING SETS - Custom Aggregations
-- ========================================

-- GROUPING SETS: Specific aggregation groups
SELECT 
  p.category,
  c.country,
  d.year,
  SUM(f.sales_amount) AS total_sales
FROM sales_fact f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_customer c ON f.customer_key = c.customer_key
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY GROUPING SETS (
  (p.category, c.country),
  (p.category, d.year),
  (c.country, d.year),
  ()
);

-- ========================================
-- 11. Materialized Views for Performance
-- ========================================

-- Create materialized view for monthly sales
CREATE MATERIALIZED VIEW mv_monthly_sales AS
SELECT 
  d.year,
  d.month,
  p.category,
  c.country,
  SUM(f.sales_amount) AS total_sales,
  SUM(f.quantity) AS total_quantity,
  SUM(f.profit_amount) AS total_profit
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_customer c ON f.customer_key = c.customer_key
GROUP BY d.year, d.month, p.category, c.country;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW mv_monthly_sales;

-- Query materialized view (fast!)
SELECT * FROM mv_monthly_sales
WHERE year = 2025 AND category = 'Electronics';

-- ========================================
-- 12. Window Functions for OLAP Analysis
-- ========================================

-- Running total by date
SELECT 
  d.date,
  f.sales_amount,
  SUM(f.sales_amount) OVER (ORDER BY d.date) AS running_total
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key;

-- Ranking products by sales within each category
SELECT 
  p.category,
  p.product_name,
  SUM(f.sales_amount) AS total_sales,
  RANK() OVER (PARTITION BY p.category ORDER BY SUM(f.sales_amount) DESC) AS rank
FROM sales_fact f
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY p.category, p.product_name;

-- Moving average (7-day)
SELECT 
  d.date,
  SUM(f.sales_amount) AS daily_sales,
  AVG(SUM(f.sales_amount)) OVER (
    ORDER BY d.date 
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS moving_avg_7days
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY d.date;

-- Year-over-Year comparison
SELECT 
  d.year,
  d.month,
  SUM(f.sales_amount) AS current_sales,
  LAG(SUM(f.sales_amount), 12) OVER (ORDER BY d.year, d.month) AS prev_year_sales,
  SUM(f.sales_amount) - LAG(SUM(f.sales_amount), 12) OVER (ORDER BY d.year, d.month) AS yoy_diff
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY d.year, d.month;

-- ========================================
-- 13. Partitioning for Performance
-- ========================================

-- Partition fact table by year
CREATE TABLE sales_fact_partitioned (
  sale_id INT,
  date_key INT,
  product_key INT,
  sales_amount DECIMAL(12,2)
)
PARTITION BY RANGE (YEAR(date_key)) (
  PARTITION p2023 VALUES LESS THAN (2024),
  PARTITION p2024 VALUES LESS THAN (2025),
  PARTITION p2025 VALUES LESS THAN (2026),
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- ========================================
-- 14. Incremental ETL with Change Tracking
-- ========================================

-- Create ETL metadata table
CREATE TABLE etl_metadata (
  table_name VARCHAR(50) PRIMARY KEY,
  last_load_time TIMESTAMP,
  last_load_status VARCHAR(20)
);

-- Incremental load: only new records
INSERT INTO sales_fact (date_key, product_key, customer_key, sales_amount)
SELECT 
  d.date_key,
  p.product_key,
  c.customer_key,
  SUM(s.amount) AS total_sales
FROM staging.sales s
JOIN dim_date d ON s.order_date = d.date
JOIN dim_product p ON s.product_id = p.product_id
JOIN dim_customer c ON s.customer_id = c.customer_id
WHERE s.load_timestamp > (
  SELECT last_load_time FROM etl_metadata WHERE table_name = 'sales_fact'
)
GROUP BY d.date_key, p.product_key, c.customer_key;

-- Update metadata
UPDATE etl_metadata 
SET last_load_time = CURRENT_TIMESTAMP,
    last_load_status = 'SUCCESS'
WHERE table_name = 'sales_fact';

-- ========================================
-- 15. Complex Analytical Queries
-- ========================================

-- Top 10 products by profit margin
SELECT 
  p.product_name,
  SUM(f.sales_amount) AS total_revenue,
  SUM(f.profit_amount) AS total_profit,
  (SUM(f.profit_amount) / SUM(f.sales_amount)) * 100 AS profit_margin_pct
FROM sales_fact f
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY p.product_name
ORDER BY profit_margin_pct DESC
LIMIT 10;

-- Customer segmentation by purchase value
SELECT 
  c.customer_segment,
  COUNT(DISTINCT f.customer_key) AS customer_count,
  SUM(f.sales_amount) AS total_sales,
  AVG(f.sales_amount) AS avg_order_value
FROM sales_fact f
JOIN dim_customer c ON f.customer_key = c.customer_key
GROUP BY c.customer_segment;

-- Seasonal sales pattern
SELECT 
  d.month_name,
  AVG(f.sales_amount) AS avg_sales
FROM sales_fact f
JOIN dim_date d ON f.date_key = d.date_key
GROUP BY d.month, d.month_name
ORDER BY d.month;
''',
    revisionPoints: [
      'Data Warehouse is a centralized repository storing historical data from multiple sources for analytics',
      'OLAP (Online Analytical Processing) enables multidimensional analysis using dimensions, hierarchies, and aggregations',
      'OLTP focuses on transactions (INSERT, UPDATE, DELETE) while OLAP focuses on analysis (SELECT, GROUP BY)',
      'OLTP uses normalized schemas; OLAP uses denormalized schemas (Star/Snowflake) for faster queries',
      'ETL (Extract, Transform, Load) process integrates data from sources into the data warehouse',
      'Extract pulls data from operational systems; Transform cleans and standardizes; Load inserts into warehouse',
      'Star Schema has a central fact table surrounded by denormalized dimension tables',
      'Snowflake Schema normalizes dimensions into multiple related tables, reducing redundancy',
      'Galaxy Schema (Fact Constellation) uses multiple fact tables sharing dimension tables',
      'Fact tables store measures (metrics like sales, quantity); Dimension tables store descriptive attributes',
      'OLAP operations: Roll-Up (summarize), Drill-Down (detail), Slice (filter), Dice (sub-cube), Pivot (rotate)',
      'Roll-Up aggregates data up the hierarchy (City → State → Country)',
      'Drill-Down shows detailed data down the hierarchy (Year → Quarter → Month → Day)',
      'Slice extracts a single dimension slice; Dice selects a sub-cube with multiple filters',
      'MOLAP stores data in multidimensional cubes (fastest); ROLAP uses relational tables (scalable)',
      'HOLAP is hybrid OLAP combining MOLAP cubes and ROLAP tables',
      'ROLLUP creates hierarchical subtotals at each grouping level',
      'CUBE generates all possible aggregation combinations (multidimensional)',
      'GROUPING SETS allows custom aggregation groups for specific analysis needs',
      'Materialized views pre-aggregate common queries for faster performance',
      'Partitioning large fact tables by date or region improves query performance',
      'Data Warehouse technologies: Redshift, BigQuery, Snowflake, Teradata',
      'ETL tools: Informatica, Talend, Apache NiFi, AWS Glue, Azure Data Factory',
      'Visualization tools: Tableau, Power BI, Looker, QlikView for dashboards and reports',
      'Data Warehousing enables historical analysis, trend identification, and data-driven decision making',
    ],
    quizQuestions: [
      Question(
        question: 'What does OLAP stand for?',
        options: ['Online Analytical Processing', 'Offline Analysis Protocol', 'Operational Layer Access Point', 'Optimized Linear Algorithm'],
        correctIndex: 0,
      ),
      Question(
        question: 'What does OLTP stand for?',
        options: ['Online Transaction Processing', 'Offline Transfer Protocol', 'Optimized Logic Transport', 'Operational Load Testing Process'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which schema has a central fact table surrounded by denormalized dimension tables?',
        options: ['Snowflake Schema', 'Star Schema', 'Galaxy Schema', 'Normalized Schema'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does ETL stand for?',
        options: ['Execute, Test, Load', 'Extract, Transform, Load', 'Enhance, Transfer, Link', 'Export, Translate, Launch'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which OLAP operation moves from summary to detailed data?',
        options: ['Roll-Up', 'Drill-Down', 'Slice', 'Pivot'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which OLAP type stores data in multidimensional cubes?',
        options: ['ROLAP', 'MOLAP', 'HOLAP', 'DOLAP'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of the Transform stage in ETL?',
        options: [
          'Extract data from sources',
          'Clean, standardize, and prepare data',
          'Load data into warehouse',
          'Create backups'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which SQL clause creates hierarchical subtotals?',
        options: ['GROUP BY', 'ROLLUP', 'HAVING', 'ORDER BY'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which SQL clause generates all possible aggregation combinations?',
        options: ['ROLLUP', 'CUBE', 'GROUPING SETS', 'PARTITION BY'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a Data Mart?',
        options: [
          'A type of database index',
          'A subset of data warehouse for specific business area',
          'An ETL tool',
          'A visualization platform'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is a cloud data warehouse platform?',
        options: ['MongoDB', 'Redis', 'Amazon Redshift', 'Neo4j'],
        correctIndex: 2,
      ),
      Question(
        question: 'In a Star Schema, what stores measures/metrics?',
        options: ['Dimension tables', 'Fact table', 'Staging table', 'Materialized views'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'db_performance',
    title: '13. Database Performance & Monitoring',
    explanation: '''## Database Performance & Monitoring

### 1. Introduction

**Definition:**

Database performance and monitoring focus on **improving query execution speed**, **resource utilization**, and **ensuring consistent system performance**. It involves tracking how efficiently a database handles workloads and identifying bottlenecks.

**Why Performance Matters:**

**Business Impact:**
- **User Experience:** Slow queries lead to frustrated users and abandoned transactions
- **Revenue Loss:** E-commerce sites lose 1% sales for every 100ms delay
- **Cost Efficiency:** Poor performance requires expensive hardware upgrades
- **Scalability:** Optimized databases handle more users with same resources
- **Competitive Advantage:** Fast systems attract and retain customers

**Example Scenario:**

E-commerce website:
- **Before Optimization:** Product page loads in 5 seconds
- **Problem:** Missing index on product_category column
- **After Optimization:** Product page loads in 0.3 seconds
- **Result:** 40% increase in conversions

**Key Objectives:**

1. **Minimize Response Time:** Queries complete quickly
2. **Maximize Throughput:** Handle more concurrent requests
3. **Optimize Resource Usage:** Efficient CPU, memory, disk utilization
4. **Ensure Availability:** 99.99% uptime with fast failover
5. **Identify Bottlenecks:** Pinpoint slow queries and resource constraints
6. **Prevent Issues:** Proactive monitoring and alerting

**Performance Lifecycle:**

```
Monitor → Measure → Analyze → Optimize → Repeat
```

---

### 2. Key Performance Factors

#### 1. Query Optimization

**Importance:**
- Poorly written queries can be 100x slower than optimized ones
- Small changes can yield massive performance gains

**Techniques:**
- Use efficient SQL queries with proper WHERE clauses
- Avoid SELECT * (fetch only needed columns)
- Minimize JOINs and optimize JOIN order
- Use EXISTS instead of IN for subqueries
- Leverage query hints and optimizer directives

**Example:**

```sql
-- Inefficient: Full table scan
SELECT * FROM orders WHERE YEAR(order_date) = 2025;

-- Efficient: Uses index on order_date
SELECT * FROM orders 
WHERE order_date >= '2025-01-01' AND order_date < '2026-01-01';
```

---

#### 2. Indexing

**Benefits:**
- Dramatically speeds up SELECT queries (10x-1000x faster)
- Accelerates JOIN operations
- Improves ORDER BY and GROUP BY performance

**Trade-offs:**
- Slows down INSERT, UPDATE, DELETE operations
- Requires additional storage space
- Needs maintenance (rebuild/reorg)

**Best Practices:**
- Index foreign keys and frequently queried columns
- Use composite indexes for multi-column searches
- Avoid over-indexing (max 5-7 per table)
- Monitor index usage and drop unused ones

---

#### 3. Normalization Level

**Balance:**
- **Normalized (3NF):** Less redundancy, more JOINs
- **Denormalized:** Fewer JOINs, more redundancy

**Decision Criteria:**

| Factor              | Normalized       | Denormalized     |
| ------------------- | ---------------- | ---------------- |
| **Data Integrity**  | High             | Medium           |
| **Storage**         | Less space       | More space       |
| **Read Performance**| Slower (JOINs)   | Faster (no JOINs)|
| **Write Performance**| Faster          | Slower           |
| **Use Case**        | OLTP systems     | OLAP/Analytics   |

---

#### 4. Caching

**Types:**

**Query Result Cache:**
- Store frequently accessed query results
- Invalidate on data changes

**Object Cache:**
- Cache application objects (Redis, Memcached)
- Reduce database load by 70-90%

**Buffer Pool:**
- Database keeps frequently used pages in memory
- Configure appropriate buffer pool size

**Benefits:**
- Reduces database load
- Faster response times
- Scales horizontally

---

#### 5. Hardware Resources

**CPU:**
- Multi-core processors for parallel query execution
- Monitor CPU usage (< 80% ideal)

**Memory (RAM):**
- Larger buffer pool = fewer disk I/O
- Recommendation: 60-70% of data size

**Disk:**
- **SSD:** 100x faster than HDD
- **RAID:** Redundancy and performance
- **IOPS:** 10,000+ for high-performance systems

**Network:**
- Low latency between app and DB servers
- High bandwidth for data-intensive operations

---

#### 6. Concurrency Management

**Challenges:**
- Lock contention and deadlocks
- Connection pool exhaustion
- Resource starvation

**Solutions:**
- Optimistic locking for read-heavy workloads
- Connection pooling (reuse connections)
- Read replicas for load distribution
- Queue-based processing for write-heavy loads

---

### 3. Common Performance Metrics

| Metric                     | Description                              | Target/Ideal Value           | How to Measure                     |
| -------------------------- | ---------------------------------------- | ---------------------------- | ---------------------------------- |
| **Query Response Time**    | Time to execute a query                  | < 100ms for simple queries   | EXPLAIN ANALYZE, slow query log    |
| **Throughput**             | Queries handled per second               | 1000+ QPS for high-traffic   | Performance counters, monitoring   |
| **CPU Usage**              | Database process CPU consumption         | < 70% average                | top, htop, Task Manager            |
| **Memory Usage**           | RAM used by database                     | 60-70% of available RAM      | Free memory, buffer pool stats     |
| **Disk I/O**               | Read/write operations per second         | 10,000+ IOPS (SSD)           | iostat, perfmon                    |
| **Cache Hit Ratio**        | % of reads served from cache             | > 95% for optimal performance| SHOW STATUS, pg_stat_database      |
| **Connection Count**       | Active database connections              | < max_connections limit      | SHOW PROCESSLIST, pg_stat_activity |
| **Deadlocks**              | Transaction conflicts per hour           | < 5 per hour                 | Error logs, performance schema     |
| **Lock Wait Time**         | Time waiting for locks                   | < 50ms average               | Performance schema, sys.dm_tran_locks |
| **Replication Lag**        | Delay between master and replica         | < 1 second                   | SHOW SLAVE STATUS, pg_stat_replication |
| **Table Scan Ratio**       | % of queries using full table scans      | < 10%                        | Handler_read_rnd_next              |
| **Index Usage**            | % of queries using indexes               | > 90%                        | sys.schema_unused_indexes          |

**Metrics Dashboard Example:**

```
┌─────────────────────────────────────┐
│  Database Performance Dashboard     │
├─────────────────────────────────────┤
│ Avg Query Time:    85ms    ✓ Good   │
│ Throughput:        1200 QPS ✓ Good  │
│ CPU Usage:         65%      ✓ Good  │
│ Cache Hit Ratio:   97%      ✓ Good  │
│ Active Connections: 45/100  ✓ Good  │
│ Deadlocks/hour:    2        ⚠ Watch │
└─────────────────────────────────────┘
```

---

### 4. SQL Performance Optimization Techniques

#### a) Use Proper Indexes

**Single Column Index:**

```sql
-- Create index on frequently searched column
CREATE INDEX idx_customer_name ON customers(name);

-- Speeds up queries like:
SELECT * FROM customers WHERE name = 'John Doe';
```

**Composite Index:**

```sql
-- Index multiple columns for combined searches
CREATE INDEX idx_order_customer_date ON orders(customer_id, order_date);

-- Optimizes queries like:
SELECT * FROM orders 
WHERE customer_id = 101 AND order_date >= '2025-01-01';
```

**Covering Index:**

```sql
-- Include all columns needed by query
CREATE INDEX idx_product_covering ON products(category, price, product_name);

-- No table lookup needed:
SELECT product_name, price FROM products WHERE category = 'Electronics';
```

**Best Practices:**
- Index foreign keys
- Index columns in WHERE, JOIN, ORDER BY clauses
- Use prefix indexes for long strings
- Monitor index usage with EXPLAIN

---

#### b) Avoid SELECT * (Use Only Needed Columns)

**Problem:**

```sql
-- Bad: Fetches all columns (slow, high network traffic)
SELECT * FROM orders;
```

**Solution:**

```sql
-- Good: Fetch only required columns
SELECT order_id, customer_id, total_amount FROM orders;
```

**Benefits:**
- **30-50% faster** query execution
- Reduced network bandwidth
- Better cache utilization
- Enables covering indexes

---

#### c) Use Joins Instead of Subqueries

**Inefficient Subquery:**

```sql
-- Subquery executes for each row
SELECT name FROM customers
WHERE id IN (SELECT customer_id FROM orders WHERE total > 1000);
```

**Optimized JOIN:**

```sql
-- Join executes once
SELECT DISTINCT c.name 
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.total > 1000;
```

**Performance Gain:** 5-10x faster for large datasets

---

#### d) Analyze and Use Execution Plans

**MySQL EXPLAIN:**

```sql
EXPLAIN SELECT * FROM orders WHERE customer_id = 101;

-- Output shows:
-- type: ref (good - uses index)
-- rows: 5 (few rows scanned)
-- Extra: Using index (excellent - covering index)
```

**PostgreSQL EXPLAIN ANALYZE:**

```sql
EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 101;

-- Shows:
-- Planning Time: 0.125 ms
-- Execution Time: 0.892 ms
-- Index Scan on orders (cost=0.42..8.44)
```

**Key Indicators:**

| Indicator         | Good                  | Bad                   |
| ----------------- | --------------------- | --------------------- |
| **Type**          | const, ref, range     | ALL (full table scan) |
| **Rows**          | Few (< 100)           | Many (> 10,000)       |
| **Extra**         | Using index           | Using filesort        |
| **Cost**          | Low (< 100)           | High (> 10,000)       |

---

#### e) Use LIMIT and Pagination

**Without Pagination:**

```sql
-- Bad: Loads all 1 million rows
SELECT * FROM products;
```

**With Pagination:**

```sql
-- Good: Load 10 rows at a time
SELECT * FROM products 
ORDER BY product_id
LIMIT 10 OFFSET 0;  -- Page 1

SELECT * FROM products 
ORDER BY product_id
LIMIT 10 OFFSET 10; -- Page 2
```

**Optimized Pagination (Keyset):**

```sql
-- Even faster: No OFFSET overhead
SELECT * FROM products 
WHERE product_id > 100
ORDER BY product_id
LIMIT 10;
```

---

#### f) Additional Optimization Techniques

**Avoid Functions on Indexed Columns:**

```sql
-- Bad: Index not used
SELECT * FROM orders WHERE YEAR(order_date) = 2025;

-- Good: Index used
SELECT * FROM orders 
WHERE order_date >= '2025-01-01' AND order_date < '2026-01-01';
```

**Use EXISTS Instead of IN:**

```sql
-- Slower for large subqueries
SELECT * FROM customers 
WHERE id IN (SELECT customer_id FROM orders);

-- Faster: Stops at first match
SELECT * FROM customers c
WHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id);
```

**Batch Operations:**

```sql
-- Slow: 1000 individual inserts
INSERT INTO products (name, price) VALUES ('Product1', 10.99);
-- ... repeat 1000 times

-- Fast: Single batch insert
INSERT INTO products (name, price) VALUES 
  ('Product1', 10.99),
  ('Product2', 15.99),
  -- ... 1000 rows
  ('Product1000', 9.99);
```

---

### 5. Database Monitoring Tools

| Tool                          | Database         | Description                                    | Features                                 |
| ----------------------------- | ---------------- | ---------------------------------------------- | ---------------------------------------- |
| **pgAdmin**                   | PostgreSQL       | Web-based administration and monitoring        | Query tool, dashboard, server monitoring |
| **MySQL Workbench**           | MySQL            | Visual database design and admin tool          | Performance dashboard, query profiler    |
| **phpMyAdmin**                | MySQL/MariaDB    | Web-based database management                  | SQL execution, performance stats         |
| **Prometheus + Grafana**      | All              | Real-time metrics collection and visualization | Custom dashboards, alerting              |
| **Datadog**                   | All              | Cloud-based APM and monitoring platform        | Auto-discovery, anomaly detection        |
| **New Relic**                 | All              | Application performance monitoring             | Database insights, query analysis        |
| **Oracle Enterprise Manager** | Oracle           | Comprehensive Oracle database management       | Performance tuning, diagnostics          |
| **SQL Server Profiler**       | SQL Server       | Trace and analyze SQL Server activity          | Query capture, execution plans           |
| **pt-query-digest**           | MySQL            | Analyze MySQL slow query logs                  | Query ranking, optimization suggestions  |
| **pg_stat_statements**        | PostgreSQL       | Track execution stats of SQL statements        | Top queries, avg execution time          |
| **Percona Monitoring (PMM)**  | MySQL/PostgreSQL | Open-source database monitoring                | Query analytics, dashboards              |
| **SolarWinds DPA**            | All              | Database performance analyzer                  | Wait time analysis, tuning advisor       |

**Tool Categories:**

**Built-in Tools:**
- Database-specific (pgAdmin, MySQL Workbench)
- Lightweight, no extra cost
- Limited cross-database support

**APM Platforms:**
- Datadog, New Relic, AppDynamics
- Comprehensive monitoring (app + DB + infrastructure)
- Expensive but feature-rich

**Open-Source:**
- Prometheus + Grafana
- PMM (Percona Monitoring and Management)
- Free, customizable, community support

---

### 6. Real-Time Monitoring Examples

#### MySQL Slow Query Monitoring

**Step 1: Enable Slow Query Log**

```sql
-- Enable slow query logging
SET GLOBAL slow_query_log = 'ON';

-- Log queries taking > 2 seconds
SET GLOBAL long_query_time = 2;

-- Log queries not using indexes
SET GLOBAL log_queries_not_using_indexes = 'ON';
```

**Step 2: Check Log Location**

```sql
SHOW VARIABLES LIKE 'slow_query_log_file';
-- Output: /var/log/mysql/slow-query.log
```

**Step 3: Analyze Slow Queries**

```bash
# Using mysqldumpslow (built-in)
mysqldumpslow -s t -t 10 /var/log/mysql/slow-query.log

# Using pt-query-digest (Percona Toolkit)
pt-query-digest /var/log/mysql/slow-query.log
```

**Sample Output:**

```
# Query 1: 120 QPS, 5.2s avg, 12% of total time
# SELECT * FROM orders WHERE customer_id = N;

# Query 2: 80 QPS, 3.1s avg, 8% of total time  
# SELECT * FROM products WHERE category = 'S';
```

---

#### PostgreSQL Query Performance

**Enable pg_stat_statements:**

```sql
-- In postgresql.conf
shared_preload_libraries = 'pg_stat_statements'

-- Restart PostgreSQL, then:
CREATE EXTENSION pg_stat_statements;
```

**View Top Slow Queries:**

```sql
SELECT 
  query,
  calls,
  total_exec_time,
  mean_exec_time,
  max_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

#### Real-Time Connection Monitoring

**MySQL:**

```sql
-- Show active connections
SHOW PROCESSLIST;

-- Count connections by state
SELECT state, COUNT(*) 
FROM information_schema.processlist 
GROUP BY state;

-- Kill long-running query
KILL 12345;
```

**PostgreSQL:**

```sql
-- Show active queries
SELECT pid, usename, state, query, query_start
FROM pg_stat_activity
WHERE state = 'active';

-- Terminate query
SELECT pg_terminate_backend(12345);
```

---

#### Performance Schema (MySQL)

**Enable Performance Schema:**

```sql
-- In my.cnf
performance_schema = ON

-- Check status
SHOW VARIABLES LIKE 'performance_schema';
```

**Top Queries by Execution Time:**

```sql
SELECT 
  DIGEST_TEXT,
  COUNT_STAR AS exec_count,
  AVG_TIMER_WAIT/1000000000 AS avg_ms,
  SUM_TIMER_WAIT/1000000000 AS total_ms
FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;
```

---

### 7. Performance Tuning Checklist

#### Database Design

✅ **Index frequently used columns**
- Foreign keys
- WHERE clause columns
- JOIN columns
- ORDER BY columns

✅ **Normalize appropriately**
- 3NF for OLTP
- Denormalize for OLAP

✅ **Partition large tables**
- By date (monthly, yearly)
- By region or category
- Improves query performance and maintenance

---

#### Query Optimization

✅ **Optimize queries and joins**
- Use EXPLAIN to analyze
- Rewrite subqueries as JOINs
- Avoid SELECT *

✅ **Avoid unnecessary subqueries**
- Use JOINs or EXISTS
- Materialized views for complex queries

✅ **Use prepared statements**
- Reduces parsing overhead
- Improves security (prevents SQL injection)

---

#### Configuration

✅ **Use connection pooling**
- Reuse database connections
- Reduce connection overhead
- Configure pool size (10-50 per server)

✅ **Keep statistics updated**
```sql
-- MySQL
ANALYZE TABLE orders;

-- PostgreSQL
VACUUM ANALYZE orders;
```

✅ **Configure buffer pool**
```sql
-- MySQL: 60-70% of RAM
innodb_buffer_pool_size = 8G

-- PostgreSQL
shared_buffers = 4GB
```

---

#### Maintenance

✅ **Regularly vacuum/analyze tables (PostgreSQL)**
```sql
-- Manual vacuum
VACUUM FULL orders;

-- Automatic (configure autovacuum)
autovacuum = on
```

✅ **Monitor deadlocks and lock waits**
```sql
-- MySQL: Check InnoDB status
SHOW ENGINE INNODB STATUS;

-- PostgreSQL: Lock monitoring
SELECT * FROM pg_locks WHERE NOT granted;
```

✅ **Rebuild fragmented indexes**
```sql
-- MySQL
OPTIMIZE TABLE orders;

-- PostgreSQL
REINDEX TABLE orders;
```

---

#### Caching & Scaling

✅ **Use caching (Redis, Memcached)**
- Cache query results
- Session data
- Computed values

✅ **Implement read replicas**
- Distribute read load
- Master for writes, replicas for reads

✅ **Archive old data**
```sql
-- Move old orders to archive table
INSERT INTO orders_archive 
SELECT * FROM orders WHERE order_date < '2023-01-01';

DELETE FROM orders WHERE order_date < '2023-01-01';
```

---

### 8. Automation & Alerting

#### Automated Alerts

**CPU/Memory Alerts:**

```yaml
# Prometheus alert rule
- alert: HighDatabaseCPU
  expr: database_cpu_usage > 80
  for: 5m
  annotations:
    summary: "Database CPU usage above 80% for 5 minutes"
```

**Slow Query Alerts:**

```sql
-- Monitor slow queries (> 5 seconds)
SELECT * FROM mysql.slow_log 
WHERE query_time > 5
ORDER BY query_time DESC;
```

**Deadlock Alerts:**

```sql
-- Track deadlocks in MySQL
SELECT * FROM information_schema.INNODB_METRICS 
WHERE NAME = 'lock_deadlocks';
```

---

#### Query Timeout Thresholds

**MySQL:**

```sql
-- Set max execution time (5 seconds)
SET SESSION max_execution_time = 5000;
```

**PostgreSQL:**

```sql
-- Set statement timeout
SET statement_timeout = '5s';
```

---

#### Grafana Dashboards

**Metrics to Display:**
- Query response time (line chart)
- Throughput (queries/sec)
- CPU and memory usage
- Cache hit ratio
- Active connections
- Slow query count

**Sample Dashboard Config:**

```json
{
  "dashboard": {
    "title": "Database Performance",
    "panels": [
      {
        "title": "Query Response Time",
        "targets": [{"expr": "avg(query_duration_seconds)"}]
      },
      {
        "title": "Cache Hit Ratio",
        "targets": [{"expr": "cache_hits / (cache_hits + cache_misses)"}]
      }
    ]
  }
}
```

---

#### Scheduled Maintenance

**Nightly Backups:**

```bash
# Cron job: Daily backup at 2 AM
0 2 * * * mysqldump -u root -p mydb > /backups/mydb_\$(date +\\%Y\\%m\\%d).sql
```

**Index Maintenance:**

```sql
-- Weekly index rebuild (Sunday 3 AM)
-- MySQL
OPTIMIZE TABLE orders;

-- PostgreSQL
REINDEX DATABASE mydb;
```

**Statistics Update:**

```sql
-- Daily statistics update
ANALYZE TABLE orders;
```

---

### 9. Case Study Example

#### Scenario

**Company:** E-commerce platform
**Problem:** "orders" table grows to 10 million rows
**Symptom:** Reports taking 5-10 minutes to load
**Impact:** Business decisions delayed, analysts frustrated

---

#### Analysis

**1. Identified Issues:**

```sql
-- Slow query (5.2 seconds)
EXPLAIN SELECT * FROM orders 
WHERE customer_id = 12345 
AND order_date BETWEEN '2024-01-01' AND '2025-01-01';

-- Output:
-- type: ALL (full table scan)
-- rows: 10,000,000
-- Extra: Using where
```

**Problems Found:**
- No index on (customer_id, order_date)
- SELECT * fetching unnecessary columns
- No partitioning on large table
- Reports hitting live database

---

#### Solution Implemented

**1. Added Composite Index:**

```sql
CREATE INDEX idx_orders_customer_date 
ON orders(customer_id, order_date);
```

**2. Partitioned by Month:**

```sql
ALTER TABLE orders 
PARTITION BY RANGE (YEAR(order_date) * 100 + MONTH(order_date)) (
  PARTITION p202401 VALUES LESS THAN (202402),
  PARTITION p202402 VALUES LESS THAN (202403),
  -- ... monthly partitions
  PARTITION p202512 VALUES LESS THAN (202601)
);
```

**3. Implemented Redis Cache:**

```python
# Cache recent reports for 5 minutes
cache_key = f"report:customer:{customer_id}:recent"
cached = redis.get(cache_key)

if cached:
    return cached
else:
    result = db.query("SELECT ...")
    redis.setex(cache_key, 300, result)
    return result
```

**4. Created Materialized View:**

```sql
CREATE MATERIALIZED VIEW customer_order_summary AS
SELECT 
  customer_id,
  DATE_TRUNC('month', order_date) AS month,
  COUNT(*) AS order_count,
  SUM(total_amount) AS total_spent
FROM orders
GROUP BY customer_id, DATE_TRUNC('month', order_date);

-- Refresh nightly
REFRESH MATERIALIZED VIEW customer_order_summary;
```

---

#### Results

| Metric                | Before    | After     | Improvement |
| --------------------- | --------- | --------- | ----------- |
| **Query Time**        | 5.2s      | 0.08s     | 98% faster  |
| **Report Load Time**  | 8 minutes | 2 seconds | 99.6% faster|
| **Database CPU**      | 85%       | 45%       | 47% reduction|
| **Cache Hit Ratio**   | N/A       | 92%       | New metric  |

**Business Impact:**
- Analysts can run reports in real-time
- 40% increase in report usage
- Faster decision-making
- \$50K saved in hardware upgrade costs

---

### 10. Summary

**Database Performance & Monitoring ensures:**

✅ **Efficient Operations:**
- Databases run smoothly under heavy loads
- Resources (CPU, memory, disk) used optimally
- Minimal downtime and fast recovery

✅ **Quick Query Results:**
- Sub-second response times for most queries
- Pagination and caching for large datasets
- Optimized indexes and query plans

✅ **Proactive Issue Detection:**
- Automated alerts when performance degrades
- Real-time dashboards (Grafana, Datadog)
- Slow query logs and analysis

✅ **Data-Driven Optimization:**
- Metrics guide tuning decisions
- EXPLAIN plans reveal bottlenecks
- Benchmarks track improvements

**Key Takeaways:**

1. **Monitor, Measure, Optimize** — that's the cycle of a healthy database
2. **Indexes** are the #1 performance booster (but don't over-index)
3. **EXPLAIN** is your best friend for query optimization
4. **Caching** can reduce database load by 70-90%
5. **Proactive monitoring** prevents issues before users notice
6. **Automation** ensures consistent performance with minimal manual effort

**Performance Mantra:**
> "A slow database is a business risk. Monitor it, optimize it, automate it."
''',
    codeSnippet: '''
-- ========================================
-- 1. Performance Monitoring Queries
-- ========================================

-- MySQL: Show active connections
SHOW PROCESSLIST;

-- MySQL: Show connection statistics
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Threads_running';
SHOW STATUS LIKE 'Max_used_connections';

-- PostgreSQL: Show active queries
SELECT pid, usename, state, query, query_start, state_change
FROM pg_stat_activity
WHERE state = 'active';

-- PostgreSQL: Connection count
SELECT count(*) FROM pg_stat_activity;

-- MySQL: Show database status
SHOW STATUS;
SHOW STATUS LIKE 'Innodb%';

-- PostgreSQL: Database statistics
SELECT * FROM pg_stat_database;

-- ========================================
-- 2. Query Execution Plan Analysis
-- ========================================

-- MySQL: EXPLAIN
EXPLAIN SELECT * FROM orders WHERE customer_id = 101;

EXPLAIN SELECT o.*, c.name 
FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE o.order_date >= '2025-01-01';

-- MySQL: EXPLAIN with FORMAT=JSON
EXPLAIN FORMAT=JSON 
SELECT * FROM orders WHERE customer_id = 101;

-- PostgreSQL: EXPLAIN ANALYZE
EXPLAIN ANALYZE 
SELECT * FROM orders WHERE customer_id = 101;

-- PostgreSQL: Detailed execution plan
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) 
SELECT * FROM orders 
WHERE customer_id = 101 AND total_amount > 100;

-- ========================================
-- 3. Index Creation and Management
-- ========================================

-- Single column index
CREATE INDEX idx_customer_email ON customers(email);

-- Composite index
CREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);

-- Covering index
CREATE INDEX idx_product_covering 
ON products(category, price, product_name);

-- Partial index (PostgreSQL)
CREATE INDEX idx_active_orders 
ON orders(order_date) 
WHERE status = 'active';

-- Full-text index (MySQL)
CREATE FULLTEXT INDEX idx_product_search ON products(name, description);

-- Show indexes
SHOW INDEX FROM orders;

-- PostgreSQL: Show indexes
SELECT * FROM pg_indexes WHERE tablename = 'orders';

-- Drop unused index
DROP INDEX idx_old_index ON orders;

-- ========================================
-- 4. Slow Query Optimization Examples
-- ========================================

-- Bad: SELECT * (fetches all columns)
SELECT * FROM orders WHERE customer_id = 101;

-- Good: Select only needed columns
SELECT order_id, order_date, total_amount 
FROM orders WHERE customer_id = 101;

-- Bad: Function on indexed column
SELECT * FROM orders WHERE YEAR(order_date) = 2025;

-- Good: Range query uses index
SELECT * FROM orders 
WHERE order_date >= '2025-01-01' AND order_date < '2026-01-01';

-- Bad: Inefficient subquery
SELECT name FROM customers
WHERE id IN (SELECT customer_id FROM orders WHERE total > 1000);

-- Good: JOIN instead of subquery
SELECT DISTINCT c.name 
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.total > 1000;

-- Bad: Leading wildcard prevents index use
SELECT * FROM products WHERE name LIKE '%phone%';

-- Good: Prefix search uses index
SELECT * FROM products WHERE name LIKE 'phone%';

-- ========================================
-- 5. Pagination Optimization
-- ========================================

-- Basic pagination (works but slow for large offsets)
SELECT * FROM products 
ORDER BY product_id
LIMIT 10 OFFSET 1000;

-- Keyset pagination (faster for large datasets)
SELECT * FROM products 
WHERE product_id > 1000
ORDER BY product_id
LIMIT 10;

-- Cursor-based pagination
SELECT * FROM products 
WHERE (created_at, id) > ('2025-01-01', 12345)
ORDER BY created_at, id
LIMIT 10;

-- ========================================
-- 6. Slow Query Log Configuration
-- ========================================

-- MySQL: Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;  -- Log queries > 2 seconds
SET GLOBAL log_queries_not_using_indexes = 'ON';

-- Check slow query log location
SHOW VARIABLES LIKE 'slow_query_log_file';

-- View slow query settings
SHOW VARIABLES LIKE 'slow_query%';
SHOW VARIABLES LIKE 'long_query_time';

-- PostgreSQL: Configure logging
-- In postgresql.conf:
-- log_min_duration_statement = 1000  # Log queries > 1 second
-- log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

-- ========================================
-- 7. Performance Schema (MySQL)
-- ========================================

-- Enable Performance Schema
-- In my.cnf: performance_schema = ON

-- Check if enabled
SHOW VARIABLES LIKE 'performance_schema';

-- Top queries by execution time
SELECT 
  DIGEST_TEXT AS query,
  COUNT_STAR AS exec_count,
  AVG_TIMER_WAIT/1000000000 AS avg_ms,
  SUM_TIMER_WAIT/1000000000 AS total_ms,
  MAX_TIMER_WAIT/1000000000 AS max_ms
FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;

-- Table I/O statistics
SELECT 
  OBJECT_SCHEMA,
  OBJECT_NAME,
  COUNT_READ,
  COUNT_WRITE,
  SUM_TIMER_WAIT/1000000000 AS total_ms
FROM performance_schema.table_io_waits_summary_by_table
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;

-- Index usage statistics
SELECT 
  OBJECT_SCHEMA,
  OBJECT_NAME,
  INDEX_NAME,
  COUNT_STAR AS uses,
  SUM_TIMER_WAIT/1000000000 AS total_ms
FROM performance_schema.table_io_waits_summary_by_index_usage
ORDER BY COUNT_STAR DESC
LIMIT 10;

-- ========================================
-- 8. pg_stat_statements (PostgreSQL)
-- ========================================

-- Enable extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Top slow queries
SELECT 
  query,
  calls,
  total_exec_time,
  mean_exec_time,
  max_exec_time,
  stddev_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Most frequently executed queries
SELECT 
  query,
  calls,
  total_exec_time/calls AS avg_time_ms
FROM pg_stat_statements
ORDER BY calls DESC
LIMIT 10;

-- Reset statistics
SELECT pg_stat_statements_reset();

-- ========================================
-- 9. Cache and Buffer Statistics
-- ========================================

-- MySQL: InnoDB buffer pool status
SHOW STATUS LIKE 'Innodb_buffer_pool%';

-- Calculate cache hit ratio (should be > 95%)
SHOW STATUS LIKE 'Innodb_buffer_pool_reads';
SHOW STATUS LIKE 'Innodb_buffer_pool_read_requests';
-- Hit Ratio = (read_requests - reads) / read_requests * 100

-- PostgreSQL: Cache hit ratio
SELECT 
  sum(heap_blks_read) as heap_read,
  sum(heap_blks_hit)  as heap_hit,
  sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 AS cache_hit_ratio
FROM pg_statio_user_tables;

-- PostgreSQL: Buffer cache usage
SELECT 
  c.relname,
  pg_size_pretty(count(*) * 8192) as buffered,
  round(100.0 * count(*) / (SELECT setting FROM pg_settings WHERE name='shared_buffers')::integer,1) AS buffers_percent
FROM pg_buffercache b
JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid)
WHERE b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database()))
GROUP BY c.relname
ORDER BY 2 DESC
LIMIT 10;

-- ========================================
-- 10. Lock and Deadlock Monitoring
-- ========================================

-- MySQL: Show InnoDB locks
SELECT * FROM information_schema.INNODB_LOCKS;
SELECT * FROM information_schema.INNODB_LOCK_WAITS;

-- MySQL: Deadlock count
SHOW STATUS LIKE 'Innodb_deadlocks';

-- MySQL: Detailed InnoDB status (includes deadlock info)
SHOW ENGINE INNODB STATUS;

-- PostgreSQL: Show locks
SELECT 
  locktype,
  relation::regclass,
  mode,
  granted,
  pid,
  query
FROM pg_locks
LEFT JOIN pg_stat_activity ON pg_locks.pid = pg_stat_activity.pid
WHERE NOT granted;

-- PostgreSQL: Blocking queries
SELECT 
  blocking.pid AS blocking_pid,
  blocking.query AS blocking_query,
  blocked.pid AS blocked_pid,
  blocked.query AS blocked_query
FROM pg_stat_activity AS blocked
JOIN pg_stat_activity AS blocking 
  ON blocking.pid = ANY(pg_blocking_pids(blocked.pid));

-- ========================================
-- 11. Table and Index Statistics
-- ========================================

-- MySQL: Table statistics
SELECT 
  table_name,
  table_rows,
  data_length,
  index_length,
  ROUND(data_length / (1024 * 1024), 2) AS data_mb,
  ROUND(index_length / (1024 * 1024), 2) AS index_mb
FROM information_schema.tables
WHERE table_schema = 'mydb'
ORDER BY data_length DESC;

-- MySQL: Analyze table (update statistics)
ANALYZE TABLE orders;

-- PostgreSQL: Table statistics
SELECT 
  schemaname,
  tablename,
  n_tup_ins,
  n_tup_upd,
  n_tup_del,
  n_live_tup,
  n_dead_tup,
  last_vacuum,
  last_autovacuum,
  last_analyze
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC;

-- PostgreSQL: Vacuum and analyze
VACUUM ANALYZE orders;

-- PostgreSQL: Table bloat
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) AS index_size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;

-- ========================================
-- 12. Query Timeout Configuration
-- ========================================

-- MySQL: Set max execution time (5 seconds)
SET SESSION max_execution_time = 5000;

-- MySQL: Global timeout
SET GLOBAL max_execution_time = 10000;

-- PostgreSQL: Statement timeout
SET statement_timeout = '5s';

-- PostgreSQL: Lock timeout
SET lock_timeout = '2s';

-- ========================================
-- 13. Connection Pooling Settings
-- ========================================

-- MySQL: Connection limits
SHOW VARIABLES LIKE 'max_connections';
SET GLOBAL max_connections = 200;

-- MySQL: Connection timeout
SHOW VARIABLES LIKE 'wait_timeout';
SET GLOBAL wait_timeout = 300;

-- PostgreSQL: Connection limits
SHOW max_connections;
-- In postgresql.conf: max_connections = 200

-- PostgreSQL: Connection stats
SELECT count(*), state 
FROM pg_stat_activity 
GROUP BY state;

-- ========================================
-- 14. Partitioning for Performance
-- ========================================

-- MySQL: Range partitioning by date
CREATE TABLE orders_partitioned (
  order_id INT,
  customer_id INT,
  order_date DATE,
  total_amount DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(order_date)) (
  PARTITION p2023 VALUES LESS THAN (2024),
  PARTITION p2024 VALUES LESS THAN (2025),
  PARTITION p2025 VALUES LESS THAN (2026),
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- PostgreSQL: Declarative partitioning
CREATE TABLE orders_partitioned (
  order_id INT,
  customer_id INT,
  order_date DATE,
  total_amount DECIMAL(10,2)
) PARTITION BY RANGE (order_date);

CREATE TABLE orders_2024 PARTITION OF orders_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

CREATE TABLE orders_2025 PARTITION OF orders_partitioned
FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

-- ========================================
-- 15. Maintenance and Optimization
-- ========================================

-- MySQL: Optimize tables (defragment)
OPTIMIZE TABLE orders;

-- MySQL: Check table integrity
CHECK TABLE orders;

-- MySQL: Repair table
REPAIR TABLE orders;

-- PostgreSQL: Vacuum full (reclaim space)
VACUUM FULL orders;

-- PostgreSQL: Reindex
REINDEX TABLE orders;
REINDEX DATABASE mydb;

-- PostgreSQL: Cluster table by index (physical reordering)
CLUSTER orders USING idx_orders_customer_date;

-- Update table statistics
-- MySQL
ANALYZE TABLE orders;

-- PostgreSQL
ANALYZE orders;
''',
    revisionPoints: [
      'Database performance monitoring tracks query execution speed, resource utilization, and system bottlenecks',
      'Key performance metrics: query response time, throughput (QPS), CPU usage, disk I/O, cache hit ratio, deadlocks',
      'Query optimization techniques: proper indexes, avoid SELECT *, use JOINs over subqueries, EXPLAIN plans',
      'Indexes dramatically speed up SELECT queries but slow down INSERT/UPDATE/DELETE operations',
      'Composite indexes optimize multi-column searches; covering indexes eliminate table lookups',
      'EXPLAIN command shows query execution plan, revealing if indexes are used or full table scans occur',
      'Avoid functions on indexed columns in WHERE clause as they prevent index usage',
      'Use LIMIT and pagination to avoid loading large datasets all at once',
      'Normalization (3NF) suits OLTP; denormalization suits OLAP for faster reads with fewer JOINs',
      'Caching (Redis, Memcached) reduces database load by 70-90% for frequently accessed data',
      'Hardware factors: CPU (multi-core), RAM (buffer pool), SSD (vs HDD), network latency affect performance',
      'Cache hit ratio > 95% is ideal; measures percentage of reads served from memory vs disk',
      'Connection pooling reuses database connections, reducing connection overhead',
      'Slow query log captures queries exceeding time threshold for analysis and optimization',
      'Performance Schema (MySQL) and pg_stat_statements (PostgreSQL) track query execution statistics',
      'Monitoring tools: pgAdmin, MySQL Workbench, Prometheus+Grafana, Datadog, New Relic',
      'Deadlocks occur when transactions wait for each other; should be < 5 per hour',
      'Table partitioning by date or region improves query performance on large tables',
      'VACUUM (PostgreSQL) reclaims space from updated/deleted rows; ANALYZE updates statistics',
      'Automated alerts notify when CPU, memory, or query time exceeds thresholds',
      'Materialized views pre-aggregate complex queries for faster reporting',
      'Batch operations (bulk INSERT) are 100x faster than individual row inserts',
      'Read replicas distribute read load; master handles writes, replicas handle reads',
      'EXISTS is faster than IN for large subqueries as it stops at first match',
      'Performance tuning cycle: Monitor → Measure → Analyze → Optimize → Repeat',
    ],
    quizQuestions: [
      Question(
        question: 'What does the EXPLAIN command do?',
        options: ['Executes a query', 'Shows query execution plan', 'Creates an index', 'Backs up data'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the ideal cache hit ratio for optimal database performance?',
        options: ['> 50%', '> 70%', '> 95%', '100%'],
        correctIndex: 2,
      ),
      Question(
        question: 'Which is faster for large subqueries?',
        options: ['IN', 'EXISTS', 'LIKE', 'BETWEEN'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main disadvantage of adding indexes?',
        options: [
          'Slows down SELECT queries',
          'Slows down INSERT/UPDATE/DELETE operations',
          'Increases query response time',
          'Reduces data accuracy'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which tool is used to analyze MySQL slow query logs?',
        options: ['EXPLAIN', 'mysqldumpslow', 'SHOW STATUS', 'ANALYZE TABLE'],
        correctIndex: 1,
      ),
      Question(
        question: 'What PostgreSQL extension tracks query execution statistics?',
        options: ['pg_monitor', 'pg_stat_statements', 'pg_query_log', 'pg_performance'],
        correctIndex: 1,
      ),
      Question(
        question: 'Why should you avoid SELECT * in queries?',
        options: [
          'It causes syntax errors',
          'It fetches unnecessary columns, slowing performance',
          'It requires special permissions',
          'It only works in MySQL'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a covering index?',
        options: [
          'An index that covers the entire table',
          'An index that includes all columns needed by a query',
          'An index on all primary keys',
          'An index created automatically'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which PostgreSQL command reclaims space from deleted rows?',
        options: ['ANALYZE', 'VACUUM', 'REINDEX', 'CLUSTER'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of connection pooling?',
        options: [
          'Encrypt database connections',
          'Reuse connections to reduce overhead',
          'Backup connections for failover',
          'Monitor connection statistics'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which normalization level is best for OLTP systems?',
        options: ['1NF', '2NF', '3NF', 'Denormalized'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is the recommended buffer pool size for MySQL?',
        options: ['10-20% of RAM', '30-40% of RAM', '60-70% of RAM', '100% of RAM'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'stored_procedures',
    title: '14. Stored Procedures & Functions',
    explanation: '''## Stored Procedures & Functions

### 1. Introduction

**Definition:**

Stored Procedures and Functions are **precompiled SQL blocks** stored inside the database. They help automate repetitive operations, improve performance, and enhance security.

**Key Concepts:**

**Stored Procedure:**
- Performs one or more tasks
- May return multiple results
- Can modify data (INSERT, UPDATE, DELETE)
- Cannot be used directly in SELECT statements
- Called using CALL statement

**Function:**
- Returns a single value
- Used within queries (SELECT, WHERE)
- Usually read-only (doesn't modify data)
- Returns DETERMINISTIC or NON-DETERMINISTIC results
- Called by name in SQL expressions

**Analogy:**
> 💡 Think of a **Stored Procedure** as a "script" that executes tasks, and a **Function** as a "formula" that calculates and returns a value.

**Benefits:**
- **Precompiled:** Faster execution (parsed once, executed many times)
- **Encapsulation:** Business logic stored in database
- **Security:** Users can execute procedures without direct table access
- **Maintainability:** Update logic in one place
- **Network Efficiency:** Reduced traffic (send procedure name, not entire query)

---

### 2. Why Use Stored Procedures & Functions

**Comparison Table:**

| Feature                  | Stored Procedure                       | Function                          |
| ------------------------ | -------------------------------------- | --------------------------------- |
| **Returns value**        | Optional (can return via OUT params)   | Mandatory (RETURNS clause)        |
| **Can modify data**      | ✅ Yes (INSERT, UPDATE, DELETE)        | ❌ No (usually read-only)          |
| **Can use in SELECT**    | ❌ No                                   | ✅ Yes                             |
| **Can call another SP**  | ✅ Yes                                  | ✅ Yes                             |
| **Transaction control**  | ✅ Yes (COMMIT, ROLLBACK)              | ❌ No                              |
| **Error handling**       | ✅ Yes (DECLARE HANDLER)               | Limited                           |
| **Called by**            | CALL statement                         | Used in expressions               |
| **Return type**          | Multiple result sets                   | Single value                      |
| **Use Case**             | Business logic, CRUD operations        | Calculations, data validation     |
| **Example**              | Process order, update inventory        | Calculate tax, format date        |

**When to Use:**

**Stored Procedure:**
- Complex business workflows
- Batch operations (process 1000 orders)
- Data migrations
- Administrative tasks
- Multi-step transactions

**Function:**
- Reusable calculations (tax, discount)
- Data formatting (date, string manipulation)
- Validation checks (email format, age calculation)
- Aggregations used in queries
- Conditional logic in WHERE clauses

---

### 3. Creating a Stored Procedure

#### Basic Syntax (MySQL)

```sql
DELIMITER //
CREATE PROCEDURE procedure_name(parameter_list)
BEGIN
    -- SQL statements
END //
DELIMITER ;
```

**Note:** `DELIMITER` changes statement terminator from `;` to `//` so procedure body can contain multiple statements.

---

#### Example 1: Simple Stored Procedure

```sql
DELIMITER //
CREATE PROCEDURE GetCustomerOrders(IN customerID INT)
BEGIN
    SELECT order_id, order_date, total_amount
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;
```

**Execution:**

```sql
CALL GetCustomerOrders(102);
```

**Output:**

| order_id | order_date | total_amount |
| -------- | ---------- | ------------ |
| 201      | 2024-06-10 | 300.00       |
| 202      | 2024-06-12 | 450.00       |

---

#### Example 2: Procedure with Multiple Statements

```sql
DELIMITER //
CREATE PROCEDURE ProcessOrder(IN orderID INT)
BEGIN
    -- Update order status
    UPDATE orders SET status = 'processing' WHERE order_id = orderID;
    
    -- Reduce inventory
    UPDATE products p
    JOIN order_items oi ON p.product_id = oi.product_id
    SET p.stock = p.stock - oi.quantity
    WHERE oi.order_id = orderID;
    
    -- Insert audit log
    INSERT INTO audit_logs(action, timestamp, order_id)
    VALUES('Order processed', NOW(), orderID);
END //
DELIMITER ;
```

**Execution:**

```sql
CALL ProcessOrder(201);
```

---

#### Example 3: Procedure with Conditional Logic

```sql
DELIMITER //
CREATE PROCEDURE ApplyDiscount(IN orderID INT)
BEGIN
    DECLARE orderTotal DECIMAL(10,2);
    
    SELECT total_amount INTO orderTotal FROM orders WHERE order_id = orderID;
    
    IF orderTotal > 5000 THEN
        UPDATE orders SET discount = 0.10 WHERE order_id = orderID;
    ELSEIF orderTotal > 2000 THEN
        UPDATE orders SET discount = 0.05 WHERE order_id = orderID;
    ELSE
        UPDATE orders SET discount = 0 WHERE order_id = orderID;
    END IF;
END //
DELIMITER ;
```

---

### 4. Creating a Function

#### Basic Syntax

```sql
DELIMITER //
CREATE FUNCTION function_name(parameter_list)
RETURNS return_type
DETERMINISTIC
BEGIN
    -- SQL statements
    RETURN value;
END //
DELIMITER ;
```

**DETERMINISTIC vs NON-DETERMINISTIC:**
- **DETERMINISTIC:** Always returns same result for same inputs (e.g., `TotalPrice(100, 2)` always returns 200)
- **NON-DETERMINISTIC:** May return different results (e.g., `NOW()`, `RAND()`)

---

#### Example 1: Simple Function

```sql
DELIMITER //
CREATE FUNCTION TotalPrice(price DECIMAL(10,2), quantity INT)
RETURNS DECIMAL(10,2)
DETERMINISTIC
BEGIN
    RETURN price * quantity;
END //
DELIMITER ;
```

**Usage in SELECT:**

```sql
SELECT product_name, price, quantity, TotalPrice(price, quantity) AS total
FROM products;
```

**Output:**

| product_name | price    | quantity | total    |
| ------------ | -------- | -------- | -------- |
| Laptop       | 45000.00 | 2        | 90000.00 |
| Mouse        | 600.00   | 2        | 1200.00  |

---

#### Example 2: Function with Conditional Logic

```sql
DELIMITER //
CREATE FUNCTION GetGrade(marks INT)
RETURNS VARCHAR(10)
DETERMINISTIC
BEGIN
    DECLARE grade VARCHAR(10);
    
    IF marks >= 90 THEN
        SET grade = 'A+';
    ELSEIF marks >= 80 THEN
        SET grade = 'A';
    ELSEIF marks >= 70 THEN
        SET grade = 'B';
    ELSEIF marks >= 60 THEN
        SET grade = 'C';
    ELSE
        SET grade = 'F';
    END IF;
    
    RETURN grade;
END //
DELIMITER ;
```

**Usage:**

```sql
SELECT student_name, marks, GetGrade(marks) AS grade
FROM students;
```

**Output:**

| student_name | marks | grade |
| ------------ | ----- | ----- |
| John         | 95    | A+    |
| Alice        | 78    | B     |
| Bob          | 55    | F     |

---

#### Example 3: Date Calculation Function

```sql
DELIMITER //
CREATE FUNCTION CalculateAge(birthdate DATE)
RETURNS INT
DETERMINISTIC
BEGIN
    RETURN YEAR(CURDATE()) - YEAR(birthdate);
END //
DELIMITER ;
```

**Usage:**

```sql
SELECT name, birthdate, CalculateAge(birthdate) AS age
FROM employees
WHERE CalculateAge(birthdate) > 30;
```

---

### 5. Parameters in Procedures

**Parameter Types:**

| Type      | Description                     | Example                      |
| --------- | ------------------------------- | ---------------------------- |
| **IN**    | Input parameter (read-only)     | IN customerID INT            |
| **OUT**   | Output parameter (write-only)   | OUT totalOrders INT          |
| **INOUT** | Input + Output (read and write) | INOUT totalAmount DECIMAL    |

---

#### Example 1: IN Parameter

```sql
DELIMITER //
CREATE PROCEDURE GetOrderCount(IN customerID INT, OUT orderCount INT)
BEGIN
    SELECT COUNT(*) INTO orderCount
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;
```

**Execution:**

```sql
CALL GetOrderCount(102, @count);
SELECT @count AS 'Total Orders';
```

**Output:**

| Total Orders |
| ------------ |
| 5            |

---

#### Example 2: OUT Parameter

```sql
DELIMITER //
CREATE PROCEDURE GetTotalSales(OUT totalSales DECIMAL(10,2))
BEGIN
    SELECT SUM(total_amount) INTO totalSales FROM orders;
END //
DELIMITER ;
```

**Execution:**

```sql
CALL GetTotalSales(@sales);
SELECT @sales AS 'Total Sales';
```

**Output:**

| Total Sales |
| ----------- |
| 125000.00   |

---

#### Example 3: INOUT Parameter

```sql
DELIMITER //
CREATE PROCEDURE CalculateTax(INOUT total DECIMAL(10,2))
BEGIN
    SET total = total + (total * 0.18); -- Add 18% tax
END //
DELIMITER ;
```

**Execution:**

```sql
SET @bill = 1000;
CALL CalculateTax(@bill);
SELECT @bill AS FinalAmount;
```

**Output:**

| FinalAmount |
| ----------- |
| 1180.00     |

---

#### Example 4: Multiple Parameters

```sql
DELIMITER //
CREATE PROCEDURE GetOrderStats(
    IN customerID INT,
    OUT orderCount INT,
    OUT totalSpent DECIMAL(10,2),
    OUT avgOrderValue DECIMAL(10,2)
)
BEGIN
    SELECT 
        COUNT(*),
        SUM(total_amount),
        AVG(total_amount)
    INTO orderCount, totalSpent, avgOrderValue
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;
```

**Execution:**

```sql
CALL GetOrderStats(102, @count, @total, @avg);
SELECT @count AS Orders, @total AS TotalSpent, @avg AS AvgOrder;
```

---

### 6. Advantages

**Performance Boost:**
- ✅ Stored once, precompiled, faster execution
- ✅ Execution plan cached by database
- ✅ Reduced parsing overhead
- ✅ Example: 30-50% faster than dynamic SQL

**Reusability:**
- ✅ Define logic once, call multiple times
- ✅ Consistent business rules across applications
- ✅ Multiple apps can use same procedure
- ✅ Easier to update logic (change once, affects all callers)

**Security:**
- ✅ Restrict direct access to tables
- ✅ Grant EXECUTE permission only
- ✅ SQL injection prevention (parameterized)
- ✅ Hide table structure from users
- ✅ Audit trail (who called what)

**Maintainability:**
- ✅ Easier to update logic in one place
- ✅ Version control for database logic
- ✅ Centralized business rules
- ✅ Debugging and testing in database

**Reduced Network Traffic:**
- ✅ Only procedure calls sent over network
- ✅ Not entire queries with data
- ✅ Example: CALL ProcessOrder(101) vs sending 10 SQL statements
- ✅ Especially beneficial for batch operations

**Transaction Management:**
- ✅ Group multiple operations atomically
- ✅ COMMIT or ROLLBACK as a unit
- ✅ Consistency guaranteed

---

### 7. Error Handling Inside Procedures

**DECLARE HANDLER Syntax:**

```sql
DECLARE {CONTINUE | EXIT} HANDLER FOR {error_condition}
BEGIN
    -- Error handling statements
END;
```

**Error Conditions:**
- **SQLEXCEPTION:** Any SQL error
- **SQLWARNING:** SQL warnings
- **NOT FOUND:** No data found (empty result)
- **Specific error code:** e.g., 1062 (duplicate key)

---

#### Example 1: Continue Handler

```sql
DELIMITER //
CREATE PROCEDURE SafeInsert()
BEGIN
    DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
    BEGIN
        INSERT INTO error_logs(message, timestamp) 
        VALUES('Error inserting customer', NOW());
    END;
    
    INSERT INTO customers(customer_id, name, email)
    VALUES(101, 'Alex', 'alex@email.com');
    
    -- Execution continues even if error occurs
    SELECT 'Procedure completed' AS status;
END //
DELIMITER ;
```

---

#### Example 2: Exit Handler

```sql
DELIMITER //
CREATE PROCEDURE TransferFunds(IN fromAccount INT, IN toAccount INT, IN amount DECIMAL(10,2))
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        SELECT 'Transaction failed' AS status;
    END;
    
    START TRANSACTION;
    
    UPDATE accounts SET balance = balance - amount WHERE account_id = fromAccount;
    UPDATE accounts SET balance = balance + amount WHERE account_id = toAccount;
    
    COMMIT;
    SELECT 'Transaction successful' AS status;
END //
DELIMITER ;
```

---

#### Example 3: SIGNAL (Raise Custom Error)

```sql
DELIMITER //
CREATE PROCEDURE WithdrawMoney(IN accountID INT, IN amount DECIMAL(10,2))
BEGIN
    DECLARE currentBalance DECIMAL(10,2);
    
    SELECT balance INTO currentBalance FROM accounts WHERE account_id = accountID;
    
    IF amount > currentBalance THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Insufficient balance';
    END IF;
    
    UPDATE accounts SET balance = balance - amount WHERE account_id = accountID;
END //
DELIMITER ;
```

---

### 8. Modifying & Dropping

**View Procedure Definition:**

```sql
SHOW CREATE PROCEDURE GetCustomerOrders;
```

**List All Procedures:**

```sql
SHOW PROCEDURE STATUS WHERE Db = 'mydb';
```

**Add Comment to Procedure:**

```sql
ALTER PROCEDURE GetCustomerOrders COMMENT 'Fetches all customer orders';
```

**Drop Procedure:**

```sql
DROP PROCEDURE IF EXISTS GetCustomerOrders;
```

**Drop Function:**

```sql
DROP FUNCTION IF EXISTS TotalPrice;
```

**Note:** You cannot ALTER the logic of a procedure/function. You must DROP and CREATE again.

---

### 9. Real-World Example: Automated Order Summary

**Scenario:**
An e-commerce app wants to calculate daily sales automatically and store results for reporting.

---

#### Step 1: Create Procedure

```sql
DELIMITER //
CREATE PROCEDURE DailySalesSummary()
BEGIN
    -- Insert daily summary into reporting table
    INSERT INTO daily_sales_report(report_date, total_orders, total_sales, avg_order_value)
    SELECT 
        DATE(order_date) AS report_date,
        COUNT(*) AS total_orders,
        SUM(total_amount) AS total_sales,
        AVG(total_amount) AS avg_order_value
    FROM orders
    WHERE DATE(order_date) = CURDATE()
    GROUP BY DATE(order_date);
END //
DELIMITER ;
```

---

#### Step 2: Schedule Using Event Scheduler

```sql
-- Enable event scheduler
SET GLOBAL event_scheduler = ON;

-- Create event to run daily at midnight
CREATE EVENT DailySalesEvent
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_DATE + INTERVAL 1 DAY
DO
    CALL DailySalesSummary();
```

---

#### Step 3: View Results

```sql
SELECT * FROM daily_sales_report ORDER BY report_date DESC LIMIT 7;
```

**Output:**

| report_date | total_orders | total_sales | avg_order_value |
| ----------- | ------------ | ----------- | --------------- |
| 2024-06-12  | 45           | 42000.00    | 933.33          |
| 2024-06-11  | 38           | 38500.00    | 1013.16         |
| 2024-06-10  | 52           | 51200.00    | 984.62          |

---

### 10. Performance Optimization Tips

**Avoid Complex Loops:**

```sql
-- Bad: Loop through each row
DELIMITER //
CREATE PROCEDURE UpdatePricesSlow()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE prodID INT;
    DECLARE cur CURSOR FOR SELECT product_id FROM products;
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN cur;
    read_loop: LOOP
        FETCH cur INTO prodID;
        IF done THEN
            LEAVE read_loop;
        END IF;
        UPDATE products SET price = price * 1.1 WHERE product_id = prodID;
    END LOOP;
    CLOSE cur;
END //
DELIMITER ;

-- Good: Single UPDATE statement
DELIMITER //
CREATE PROCEDURE UpdatePricesFast()
BEGIN
    UPDATE products SET price = price * 1.1;
END //
DELIMITER ;
```

**Use Proper Indexes:**
- Index columns used in WHERE, JOIN clauses
- Procedures benefit from same indexes as queries

**Keep Procedures Modular:**
- Break complex procedures into smaller ones
- Call sub-procedures for clarity
- Easier to test and debug

**Avoid SELECT *:**
- Fetch only needed columns
- Reduces memory usage and network traffic

**Reuse Functions:**
- Create functions for repeated calculations
- Example: `CalculateTax()` used in multiple procedures

**Cache Results When Possible:**
- Store computed results in temp tables
- Reuse within procedure

**Use Temporary Tables:**

```sql
DELIMITER //
CREATE PROCEDURE ComplexReport()
BEGIN
    -- Create temp table for intermediate results
    CREATE TEMPORARY TABLE temp_sales AS
    SELECT customer_id, SUM(total_amount) AS total
    FROM orders
    GROUP BY customer_id;
    
    -- Use temp table in multiple queries
    SELECT * FROM temp_sales WHERE total > 10000;
    
    DROP TEMPORARY TABLE temp_sales;
END //
DELIMITER ;
```

---

### 11. Stored Procedure vs Trigger

| Feature          | Stored Procedure                  | Trigger                                  |
| ---------------- | --------------------------------- | ---------------------------------------- |
| **Called by**    | User/Application (manual)         | Automatically on event                   |
| **Invocation**   | CALL statement                    | INSERT, UPDATE, DELETE on table          |
| **Usage**        | Manual execution                  | Responds to data changes                 |
| **Control**      | Explicit control                  | Implicit (automatic)                     |
| **Parameters**   | Can accept parameters             | No parameters (uses NEW/OLD)             |
| **Example**      | CALL UpdateStock(101, 50)         | AFTER INSERT ON orders                   |
| **Use Case**     | Business workflows, batch jobs    | Audit logs, data validation, cascading   |

---

### 12. Example: Function in Condition

**Using Function in WHERE Clause:**

```sql
SELECT name, salary
FROM employees
WHERE TotalPrice(salary, 12) > 600000;  -- Annual salary > 600k
```

**Using Function in ORDER BY:**

```sql
SELECT name, birthdate
FROM employees
ORDER BY CalculateAge(birthdate) DESC;
```

**Using Function in HAVING:**

```sql
SELECT department, AVG(salary) AS avg_salary
FROM employees
GROUP BY department
HAVING AVG(salary) > CalculateThreshold();
```

> Functions can be used in WHERE clauses, making logic more reusable and modular.

---

### 13. Case Study: Banking Application

**Scenario:**
A bank wants to calculate monthly interest for all savings accounts and automatically credit it.

---

#### Step 1: Create Function

```sql
DELIMITER //
CREATE FUNCTION MonthlyInterest(balance DECIMAL(10,2))
RETURNS DECIMAL(10,2)
DETERMINISTIC
BEGIN
    RETURN balance * 0.04 / 12;  -- 4% annual interest / 12 months
END //
DELIMITER ;
```

---

#### Step 2: Create Procedure to Credit Interest

```sql
DELIMITER //
CREATE PROCEDURE CreditMonthlyInterest()
BEGIN
    -- Update all savings accounts
    UPDATE accounts
    SET balance = balance + MonthlyInterest(balance)
    WHERE account_type = 'savings';
    
    -- Log the operation
    INSERT INTO transaction_log(action, timestamp, affected_accounts)
    SELECT 'Monthly interest credited', NOW(), COUNT(*)
    FROM accounts WHERE account_type = 'savings';
END //
DELIMITER ;
```

---

#### Step 3: View Results

```sql
SELECT account_id, balance, MonthlyInterest(balance) AS interest
FROM accounts
WHERE account_type = 'savings';
```

**Output:**

| account_id | balance   | interest |
| ---------- | --------- | -------- |
| 201        | 100000.00 | 333.33   |
| 202        | 80000.00  | 266.67   |
| 203        | 150000.00 | 500.00   |

---

#### Step 4: Schedule Monthly Execution

```sql
CREATE EVENT MonthlyInterestEvent
ON SCHEDULE EVERY 1 MONTH
STARTS '2024-07-01 00:00:00'
DO
    CALL CreditMonthlyInterest();
```

---

### 14. Summary

**Key Takeaways:**

**Stored Procedures:**
- ✅ Automate complex business logic
- ✅ Reusable across applications
- ✅ Enhance security (no direct table access)
- ✅ Support transactions, error handling
- ✅ Called using CALL statement

**Functions:**
- ✅ Return a single value
- ✅ Used inside queries (SELECT, WHERE, ORDER BY)
- ✅ Great for calculations and validations
- ✅ Deterministic or non-deterministic
- ✅ Cannot modify data (usually)

**Performance Benefits:**
- ✅ Precompiled (faster execution)
- ✅ Reduced network traffic
- ✅ Execution plan caching
- ✅ 30-50% faster than dynamic SQL

**Use Cases:**
- **Procedures:** Order processing, batch updates, data migrations, reporting
- **Functions:** Tax calculations, age computation, data formatting, validation

**Best Practices:**
- Keep procedures modular and short
- Use proper indexes on queried tables
- Implement error handling with DECLARE HANDLER
- Avoid complex loops (use set-based operations)
- Test thoroughly before deployment
- Document parameters and logic

**Database Intelligence:**
> "Stored Procedures and Functions make databases *smart enough* to handle logic — not just data."

**Remember:**
- Procedures = **Actions** (do something)
- Functions = **Calculations** (return something)
''',
    codeSnippet: '''
-- ========================================
-- 1. Creating Stored Procedures
-- ========================================

-- Basic procedure
DELIMITER //
CREATE PROCEDURE GetCustomerOrders(IN customerID INT)
BEGIN
    SELECT order_id, order_date, total_amount
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;

-- Execute procedure
CALL GetCustomerOrders(102);

-- ========================================
-- 2. Procedure with Multiple Statements
-- ========================================

DELIMITER //
CREATE PROCEDURE ProcessOrder(IN orderID INT)
BEGIN
    -- Update order status
    UPDATE orders SET status = 'processing' WHERE order_id = orderID;
    
    -- Reduce inventory
    UPDATE products p
    JOIN order_items oi ON p.product_id = oi.product_id
    SET p.stock = p.stock - oi.quantity
    WHERE oi.order_id = orderID;
    
    -- Insert audit log
    INSERT INTO audit_logs(action, timestamp, order_id)
    VALUES('Order processed', NOW(), orderID);
END //
DELIMITER ;

CALL ProcessOrder(201);

-- ========================================
-- 3. Procedure with Conditional Logic
-- ========================================

DELIMITER //
CREATE PROCEDURE ApplyDiscount(IN orderID INT)
BEGIN
    DECLARE orderTotal DECIMAL(10,2);
    
    SELECT total_amount INTO orderTotal FROM orders WHERE order_id = orderID;
    
    IF orderTotal > 5000 THEN
        UPDATE orders SET discount = 0.10 WHERE order_id = orderID;
    ELSEIF orderTotal > 2000 THEN
        UPDATE orders SET discount = 0.05 WHERE order_id = orderID;
    ELSE
        UPDATE orders SET discount = 0 WHERE order_id = orderID;
    END IF;
END //
DELIMITER ;

CALL ApplyDiscount(201);

-- ========================================
-- 4. Creating Functions
-- ========================================

-- Simple function
DELIMITER //
CREATE FUNCTION TotalPrice(price DECIMAL(10,2), quantity INT)
RETURNS DECIMAL(10,2)
DETERMINISTIC
BEGIN
    RETURN price * quantity;
END //
DELIMITER ;

-- Use function in SELECT
SELECT product_name, price, quantity, TotalPrice(price, quantity) AS total
FROM products;

-- ========================================
-- 5. Function with Conditional Logic
-- ========================================

DELIMITER //
CREATE FUNCTION GetGrade(marks INT)
RETURNS VARCHAR(10)
DETERMINISTIC
BEGIN
    DECLARE grade VARCHAR(10);
    
    IF marks >= 90 THEN
        SET grade = 'A+';
    ELSEIF marks >= 80 THEN
        SET grade = 'A';
    ELSEIF marks >= 70 THEN
        SET grade = 'B';
    ELSEIF marks >= 60 THEN
        SET grade = 'C';
    ELSE
        SET grade = 'F';
    END IF;
    
    RETURN grade;
END //
DELIMITER ;

-- Usage
SELECT student_name, marks, GetGrade(marks) AS grade
FROM students;

-- ========================================
-- 6. Date Calculation Function
-- ========================================

DELIMITER //
CREATE FUNCTION CalculateAge(birthdate DATE)
RETURNS INT
DETERMINISTIC
BEGIN
    RETURN YEAR(CURDATE()) - YEAR(birthdate);
END //
DELIMITER ;

-- Use in WHERE clause
SELECT name, birthdate, CalculateAge(birthdate) AS age
FROM employees
WHERE CalculateAge(birthdate) > 30;

-- ========================================
-- 7. IN Parameter (Input)
-- ========================================

DELIMITER //
CREATE PROCEDURE GetOrderCount(IN customerID INT, OUT orderCount INT)
BEGIN
    SELECT COUNT(*) INTO orderCount
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;

-- Execute with OUT parameter
CALL GetOrderCount(102, @count);
SELECT @count AS 'Total Orders';

-- ========================================
-- 8. OUT Parameter (Output)
-- ========================================

DELIMITER //
CREATE PROCEDURE GetTotalSales(OUT totalSales DECIMAL(10,2))
BEGIN
    SELECT SUM(total_amount) INTO totalSales FROM orders;
END //
DELIMITER ;

CALL GetTotalSales(@sales);
SELECT @sales AS 'Total Sales';

-- ========================================
-- 9. INOUT Parameter (Input + Output)
-- ========================================

DELIMITER //
CREATE PROCEDURE CalculateTax(INOUT total DECIMAL(10,2))
BEGIN
    SET total = total + (total * 0.18); -- Add 18% tax
END //
DELIMITER ;

-- Execute
SET @bill = 1000;
CALL CalculateTax(@bill);
SELECT @bill AS FinalAmount;

-- ========================================
-- 10. Multiple Parameters
-- ========================================

DELIMITER //
CREATE PROCEDURE GetOrderStats(
    IN customerID INT,
    OUT orderCount INT,
    OUT totalSpent DECIMAL(10,2),
    OUT avgOrderValue DECIMAL(10,2)
)
BEGIN
    SELECT 
        COUNT(*),
        SUM(total_amount),
        AVG(total_amount)
    INTO orderCount, totalSpent, avgOrderValue
    FROM orders
    WHERE customer_id = customerID;
END //
DELIMITER ;

-- Execute
CALL GetOrderStats(102, @count, @total, @avg);
SELECT @count AS Orders, @total AS TotalSpent, @avg AS AvgOrder;

-- ========================================
-- 11. Error Handling: CONTINUE Handler
-- ========================================

DELIMITER //
CREATE PROCEDURE SafeInsert()
BEGIN
    DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
    BEGIN
        INSERT INTO error_logs(message, timestamp) 
        VALUES('Error inserting customer', NOW());
    END;
    
    INSERT INTO customers(customer_id, name, email)
    VALUES(101, 'Alex', 'alex@email.com');
    
    SELECT 'Procedure completed' AS status;
END //
DELIMITER ;

-- ========================================
-- 12. Error Handling: EXIT Handler
-- ========================================

DELIMITER //
CREATE PROCEDURE TransferFunds(IN fromAccount INT, IN toAccount INT, IN amount DECIMAL(10,2))
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        SELECT 'Transaction failed' AS status;
    END;
    
    START TRANSACTION;
    
    UPDATE accounts SET balance = balance - amount WHERE account_id = fromAccount;
    UPDATE accounts SET balance = balance + amount WHERE account_id = toAccount;
    
    COMMIT;
    SELECT 'Transaction successful' AS status;
END //
DELIMITER ;

-- ========================================
-- 13. SIGNAL (Custom Error)
-- ========================================

DELIMITER //
CREATE PROCEDURE WithdrawMoney(IN accountID INT, IN amount DECIMAL(10,2))
BEGIN
    DECLARE currentBalance DECIMAL(10,2);
    
    SELECT balance INTO currentBalance FROM accounts WHERE account_id = accountID;
    
    IF amount > currentBalance THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Insufficient balance';
    END IF;
    
    UPDATE accounts SET balance = balance - amount WHERE account_id = accountID;
END //
DELIMITER ;

-- ========================================
-- 14. Managing Procedures
-- ========================================

-- Show procedure definition
SHOW CREATE PROCEDURE GetCustomerOrders;

-- List all procedures
SHOW PROCEDURE STATUS WHERE Db = 'mydb';

-- Add comment
ALTER PROCEDURE GetCustomerOrders COMMENT 'Fetches all customer orders';

-- Drop procedure
DROP PROCEDURE IF EXISTS GetCustomerOrders;

-- Drop function
DROP FUNCTION IF EXISTS TotalPrice;

-- ========================================
-- 15. Daily Sales Summary (Real-World)
-- ========================================

DELIMITER //
CREATE PROCEDURE DailySalesSummary()
BEGIN
    INSERT INTO daily_sales_report(report_date, total_orders, total_sales, avg_order_value)
    SELECT 
        DATE(order_date) AS report_date,
        COUNT(*) AS total_orders,
        SUM(total_amount) AS total_sales,
        AVG(total_amount) AS avg_order_value
    FROM orders
    WHERE DATE(order_date) = CURDATE()
    GROUP BY DATE(order_date);
END //
DELIMITER ;

-- Schedule using Event Scheduler
SET GLOBAL event_scheduler = ON;

CREATE EVENT DailySalesEvent
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_DATE + INTERVAL 1 DAY
DO
    CALL DailySalesSummary();

-- View results
SELECT * FROM daily_sales_report ORDER BY report_date DESC LIMIT 7;

-- ========================================
-- 16. Performance: Avoid Loops
-- ========================================

-- Bad: Loop through rows
DELIMITER //
CREATE PROCEDURE UpdatePricesSlow()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE prodID INT;
    DECLARE cur CURSOR FOR SELECT product_id FROM products;
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN cur;
    read_loop: LOOP
        FETCH cur INTO prodID;
        IF done THEN LEAVE read_loop; END IF;
        UPDATE products SET price = price * 1.1 WHERE product_id = prodID;
    END LOOP;
    CLOSE cur;
END //
DELIMITER ;

-- Good: Single UPDATE
DELIMITER //
CREATE PROCEDURE UpdatePricesFast()
BEGIN
    UPDATE products SET price = price * 1.1;
END //
DELIMITER ;

-- ========================================
-- 17. Using Temporary Tables
-- ========================================

DELIMITER //
CREATE PROCEDURE ComplexReport()
BEGIN
    -- Create temp table
    CREATE TEMPORARY TABLE temp_sales AS
    SELECT customer_id, SUM(total_amount) AS total
    FROM orders
    GROUP BY customer_id;
    
    -- Use in queries
    SELECT * FROM temp_sales WHERE total > 10000;
    
    DROP TEMPORARY TABLE temp_sales;
END //
DELIMITER ;

-- ========================================
-- 18. Banking Example: Monthly Interest
-- ========================================

-- Create function
DELIMITER //
CREATE FUNCTION MonthlyInterest(balance DECIMAL(10,2))
RETURNS DECIMAL(10,2)
DETERMINISTIC
BEGIN
    RETURN balance * 0.04 / 12;  -- 4% annual / 12 months
END //
DELIMITER ;

-- Create procedure to credit interest
DELIMITER //
CREATE PROCEDURE CreditMonthlyInterest()
BEGIN
    UPDATE accounts
    SET balance = balance + MonthlyInterest(balance)
    WHERE account_type = 'savings';
    
    INSERT INTO transaction_log(action, timestamp, affected_accounts)
    SELECT 'Monthly interest credited', NOW(), COUNT(*)
    FROM accounts WHERE account_type = 'savings';
END //
DELIMITER ;

-- View interest amounts
SELECT account_id, balance, MonthlyInterest(balance) AS interest
FROM accounts
WHERE account_type = 'savings';

-- Schedule monthly execution
CREATE EVENT MonthlyInterestEvent
ON SCHEDULE EVERY 1 MONTH
STARTS '2024-07-01 00:00:00'
DO
    CALL CreditMonthlyInterest();

-- ========================================
-- 19. Function in Various Clauses
-- ========================================

-- WHERE clause
SELECT name, salary
FROM employees
WHERE TotalPrice(salary, 12) > 600000;

-- ORDER BY clause
SELECT name, birthdate
FROM employees
ORDER BY CalculateAge(birthdate) DESC;

-- HAVING clause
SELECT department, AVG(salary) AS avg_salary
FROM employees
GROUP BY department
HAVING AVG(salary) > 50000;

-- ========================================
-- 20. Procedure with Loop Example
-- ========================================

DELIMITER //
CREATE PROCEDURE GenerateOrders(IN numOrders INT)
BEGIN
    DECLARE i INT DEFAULT 1;
    
    WHILE i <= numOrders DO
        INSERT INTO orders(customer_id, order_date, total_amount)
        VALUES(FLOOR(RAND() * 100) + 1, CURDATE(), RAND() * 10000);
        
        SET i = i + 1;
    END WHILE;
END //
DELIMITER ;

-- Generate 100 test orders
CALL GenerateOrders(100);
''',
    revisionPoints: [
      'Stored Procedures are precompiled SQL blocks stored in the database for automation and performance',
      'Functions return a single value and can be used in SELECT, WHERE, and ORDER BY clauses',
      'Stored Procedures can modify data (INSERT, UPDATE, DELETE); Functions usually cannot',
      'DELIMITER changes statement terminator to allow multiple statements in procedure body',
      'IN parameters are input-only, OUT parameters are output-only, INOUT parameters are both',
      'CALL statement executes a stored procedure; functions are called by name in expressions',
      'DETERMINISTIC functions always return same result for same inputs; NON-DETERMINISTIC may vary',
      'Advantages: performance boost (precompiled), reusability, security, maintainability, reduced network traffic',
      'DECLARE HANDLER manages errors: CONTINUE handler continues execution, EXIT handler stops execution',
      'SIGNAL raises custom errors with SQLSTATE and MESSAGE_TEXT',
      'DROP PROCEDURE IF EXISTS safely removes procedures; cannot ALTER logic (must DROP and CREATE)',
      'Event Scheduler automates procedure execution on schedule (daily, monthly, etc.)',
      'Avoid loops in procedures; use set-based operations (single UPDATE vs row-by-row)',
      'Temporary tables store intermediate results within procedure scope',
      'Functions can calculate values (tax, age, grade) for use across multiple queries',
      'Stored Procedures support transaction control (COMMIT, ROLLBACK) and error handling',
      'SHOW CREATE PROCEDURE displays procedure definition; SHOW PROCEDURE STATUS lists all',
      'Functions must have RETURNS clause specifying return type (INT, DECIMAL, VARCHAR)',
      'Procedures vs Triggers: Procedures called manually, Triggers fire automatically on events',
      'Use procedures for business workflows (order processing, batch updates, reporting)',
      'Use functions for calculations (tax, discount, age, formatting)',
      'Security: Grant EXECUTE permission only, no direct table access needed',
      'Performance: 30-50% faster than dynamic SQL due to execution plan caching',
      'Keep procedures modular and short for easier testing and debugging',
      'Document parameters and logic for maintainability and team collaboration',
    ],
    quizQuestions: [
      Question(
        question: 'What is the main advantage of stored procedures?',
        options: ['Faster execution', 'Better security', 'Code reusability', 'All of the above'],
        correctIndex: 3,
      ),
      Question(
        question: 'Which parameter type is used for both input and output?',
        options: ['IN', 'OUT', 'INOUT', 'BOTH'],
        correctIndex: 2,
      ),
      Question(
        question: 'Can functions modify data (INSERT, UPDATE, DELETE)?',
        options: ['Yes, always', 'No, usually not', 'Only with special permissions', 'Only in PostgreSQL'],
        correctIndex: 1,
      ),
      Question(
        question: 'What keyword is used to execute a stored procedure?',
        options: ['RUN', 'EXECUTE', 'CALL', 'INVOKE'],
        correctIndex: 2,
      ),
      Question(
        question: 'What does DETERMINISTIC mean for a function?',
        options: [
          'It can modify data',
          'It always returns the same result for same inputs',
          'It runs automatically',
          'It requires no parameters'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which error handler type stops procedure execution on error?',
        options: ['CONTINUE HANDLER', 'EXIT HANDLER', 'STOP HANDLER', 'BREAK HANDLER'],
        correctIndex: 1,
      ),
      Question(
        question: 'Why change DELIMITER when creating procedures?',
        options: [
          'To improve performance',
          'To allow multiple statements with semicolons in procedure body',
          'It is optional',
          'To enable error handling'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Can stored procedures be used directly in SELECT statements?',
        options: ['Yes, always', 'No, use CALL instead', 'Only with IN parameters', 'Only in MySQL'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does SIGNAL do in a stored procedure?',
        options: ['Logs a message', 'Raises a custom error', 'Sends a notification', 'Commits transaction'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is better for performance: looping through rows or set-based operations?',
        options: ['Looping', 'Set-based operations', 'Both are equal', 'Depends on database'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main difference between Stored Procedures and Triggers?',
        options: [
          'Procedures are faster',
          'Triggers fire automatically on events; Procedures are called manually',
          'Triggers cannot modify data',
          'Procedures cannot use parameters'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which clause specifies the return type of a function?',
        options: ['RETURN', 'RETURNS', 'OUTPUT', 'TYPE'],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'triggers_events',
    title: '15. Triggers & Event Handling',
    explanation: '''## Triggers & Event Handling

### 1. Introduction

#### What is a Trigger?

**Definition:**

A **Trigger** is a **special stored program** that automatically executes when a specific **event** occurs on a table (such as `INSERT`, `UPDATE`, or `DELETE`). Triggers are database objects that respond to data changes without manual invocation.

**Key Characteristics:**
- **Automatic execution:** No CALL statement needed
- **Event-driven:** Fires on INSERT, UPDATE, or DELETE
- **Row-level or statement-level:** Can process each row or entire operation
- **Database-maintained:** Logic stored in database, not application code

**Primary Uses:**

**Data Integrity:**
- Enforce business rules automatically
- Validate data before saving
- Maintain referential integrity beyond foreign keys

**Audit Logging:**
- Track who changed what and when
- Create audit trails for compliance
- Record all modifications automatically

**Automated Actions:**
- Update related tables automatically
- Send notifications on data changes
- Calculate derived values
- Cascade operations

**Example Scenario:**

E-commerce system:
- **BEFORE INSERT trigger:** Validates product price > 0
- **AFTER INSERT trigger:** Logs new order in audit table
- **AFTER UPDATE trigger:** Updates inventory when order status changes
- **BEFORE DELETE trigger:** Prevents deletion of orders with pending shipments

---

#### What is Event Handling?

**Definition:**

Event Handling refers to **scheduling and managing automated tasks** (events) that run at specific **time intervals or database conditions**. Events are like cron jobs but managed within the database.

**Key Features:**
- **Time-based execution:** Run daily, weekly, monthly, or at specific times
- **Automatic scheduling:** No external scheduler needed
- **Database-integrated:** Access all database features
- **Maintenance automation:** Clean old data, generate reports, optimize tables

**Analogy:**
> 💡 Think of **Triggers** as automatic reactions to data changes (like motion sensors), and **Events** as automatic scheduled jobs (like alarm clocks).

---

### 2. Types of Triggers

**Trigger Classification:**

| Trigger Type      | When it Executes          | Common Use Cases                          |
| ----------------- | ------------------------- | ----------------------------------------- |
| **BEFORE INSERT** | Before inserting a record | Validate data, set default values, generate IDs |
| **AFTER INSERT**  | After record insertion    | Log new data, update related tables, send notifications |
| **BEFORE UPDATE** | Before updating record    | Check constraints, prevent unauthorized changes |
| **AFTER UPDATE**  | After updating record     | Maintain audit trail, propagate changes, recalculate totals |
| **BEFORE DELETE** | Before deleting record    | Prevent accidental deletion, archive data |
| **AFTER DELETE**  | After deleting record     | Backup deleted info, cascade deletions, update counters |

**BEFORE vs AFTER Triggers:**

**BEFORE Triggers:**
- Can modify `NEW` values before saving
- Can prevent operation by raising error
- Used for validation and data transformation
- Example: Convert email to lowercase before INSERT

**AFTER Triggers:**
- Cannot modify `NEW` values (already saved)
- Used for logging and cascading operations
- Example: Log changes to audit table after UPDATE

---

### 3. Basic Syntax

**MySQL Trigger Syntax:**

```sql
CREATE TRIGGER trigger_name
{BEFORE | AFTER} {INSERT | UPDATE | DELETE}
ON table_name
FOR EACH ROW
BEGIN
    -- SQL statements here
    -- Access NEW (for INSERT/UPDATE) and OLD (for UPDATE/DELETE)
END;
```

**Components:**

**trigger_name:** Unique identifier for the trigger
**BEFORE | AFTER:** Timing of execution
**INSERT | UPDATE | DELETE:** Triggering event
**table_name:** Table to monitor
**FOR EACH ROW:** Execute for each affected row
**BEGIN...END:** Trigger body with SQL statements

**Important Note:**
> Use `DELIMITER //` in MySQL before creating multi-line triggers to change the statement terminator.

---

### 4. Example 1: Audit Log for INSERT

**Scenario:**
Track every new employee added to the system for compliance and security auditing.

**Implementation:**

```sql
-- Create audit log table
CREATE TABLE audit_log (
    log_id INT AUTO_INCREMENT PRIMARY KEY,
    action VARCHAR(50),
    username VARCHAR(100),
    action_time DATETIME,
    details TEXT
);

-- Create trigger
DELIMITER //
CREATE TRIGGER after_employee_insert
AFTER INSERT
ON employees
FOR EACH ROW
BEGIN
    INSERT INTO audit_log(action, username, action_time, details)
    VALUES('INSERT', NEW.name, NOW(), CONCAT('New employee added: ', NEW.name, ' (ID: ', NEW.employee_id, ')'));
END //
DELIMITER ;
```

**Explanation:**

**AFTER INSERT:** Trigger fires after the employee row is successfully inserted
**NEW.name:** Accesses the newly inserted employee's name
**NOW():** Records the exact timestamp of insertion
**CONCAT():** Creates detailed log message with employee information
**Automatic logging:** No application code changes needed

**Testing:**

```sql
INSERT INTO employees (employee_id, name, department, salary)
VALUES (101, 'John Doe', 'Engineering', 75000);

-- Check audit log
SELECT * FROM audit_log;
```

**Output:**

| log_id | action | username | action_time         | details                              |
| ------ | ------ | -------- | ------------------- | ------------------------------------ |
| 1      | INSERT | John Doe | 2025-10-18 14:30:00 | New employee added: John Doe (ID: 101) |

---

### 5. Example 2: Prevent Salary Reduction (BEFORE UPDATE)

**Scenario:**
Enforce business rule that employee salaries can only increase, never decrease, to protect employee rights.

**Implementation:**

```sql
DELIMITER //
CREATE TRIGGER prevent_salary_cut
BEFORE UPDATE
ON employees
FOR EACH ROW
BEGIN
    IF NEW.salary < OLD.salary THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Salary reduction not allowed! Salary can only be increased.';
    END IF;
END //
DELIMITER ;
```

**Explanation:**

**BEFORE UPDATE:** Executes before the update is committed
**OLD.salary:** Original salary value before update
**NEW.salary:** New salary value attempting to be saved
**SIGNAL SQLSTATE '45000':** Raises user-defined error (45000 = user exception)
**MESSAGE_TEXT:** Custom error message displayed to user
**Business rule enforcement:** Automatically prevents invalid operations

**Testing:**

```sql
-- This will succeed (salary increase)
UPDATE employees SET salary = 80000 WHERE employee_id = 101;

-- This will fail (salary decrease)
UPDATE employees SET salary = 70000 WHERE employee_id = 101;
-- Error: Salary reduction not allowed! Salary can only be increased.
```

**Benefits:**
- Centralized validation logic
- Cannot be bypassed by any application
- Consistent enforcement across all database clients
- Clear error messages for debugging

---

### 6. Example 3: AFTER DELETE Backup

**Scenario:**
Automatically backup deleted employee records for data recovery and historical reference.

**Implementation:**

```sql
-- Create backup table
CREATE TABLE deleted_employees (
    backup_id INT AUTO_INCREMENT PRIMARY KEY,
    employee_id INT,
    name VARCHAR(100),
    department VARCHAR(50),
    salary DECIMAL(10,2),
    deleted_on DATETIME,
    deleted_by VARCHAR(100)
);

-- Create trigger
DELIMITER //
CREATE TRIGGER backup_deleted_employee
AFTER DELETE
ON employees
FOR EACH ROW
BEGIN
    INSERT INTO deleted_employees(employee_id, name, department, salary, deleted_on, deleted_by)
    VALUES(OLD.employee_id, OLD.name, OLD.department, OLD.salary, NOW(), USER());
END //
DELIMITER ;
```

**Explanation:**

**AFTER DELETE:** Executes after the row is deleted from employees table
**OLD.column:** Accesses data from the deleted row
**USER():** Records which database user performed the deletion
**Automatic archiving:** Deleted data never truly lost

**Testing:**

```sql
DELETE FROM employees WHERE employee_id = 101;

-- Check backup
SELECT * FROM deleted_employees;
```

**Output:**

| backup_id | employee_id | name     | department  | salary   | deleted_on          | deleted_by |
| --------- | ----------- | -------- | ----------- | -------- | ------------------- | ---------- |
| 1         | 101         | John Doe | Engineering | 75000.00 | 2025-10-18 15:00:00 | root@localhost |

**Use Cases:**
- Data recovery after accidental deletion
- Compliance auditing
- Historical data analysis
- Undo functionality

---

### 7. Accessing OLD and NEW Values

**Understanding Pseudo-Records:**

Triggers provide special pseudo-records `OLD` and `NEW` to access row data before and after the triggering event.

**Availability Table:**

| Context       | OLD.column             | NEW.column             | Use Cases                           |
| ------------- | ---------------------- | ---------------------- | ----------------------------------- |
| **INSERT**    | ❌ Not available        | ✅ New row data         | Validate new data, set defaults     |
| **UPDATE**    | ✅ Original row data    | ✅ Modified row data    | Compare before/after, audit changes |
| **DELETE**    | ✅ Deleted row data     | ❌ Not available        | Backup data, cascade operations     |

**Examples:**

**INSERT Trigger:**
```sql
-- Only NEW is available
CREATE TRIGGER before_insert_example
BEFORE INSERT ON products
FOR EACH ROW
BEGIN
    -- Validate new data
    IF NEW.price < 0 THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Price cannot be negative';
    END IF;
    
    -- Set default values
    IF NEW.created_at IS NULL THEN
        SET NEW.created_at = NOW();
    END IF;
END;
```

**UPDATE Trigger:**
```sql
-- Both OLD and NEW are available
CREATE TRIGGER before_update_example
BEFORE UPDATE ON orders
FOR EACH ROW
BEGIN
    -- Compare before and after
    IF NEW.status = 'cancelled' AND OLD.status = 'shipped' THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Cannot cancel shipped orders';
    END IF;
    
    -- Track modification
    SET NEW.updated_at = NOW();
END;
```

**DELETE Trigger:**
```sql
-- Only OLD is available
CREATE TRIGGER after_delete_example
AFTER DELETE ON orders
FOR EACH ROW
BEGIN
    -- Log deletion
    INSERT INTO order_history (order_id, status, deleted_at)
    VALUES (OLD.order_id, OLD.status, NOW());
END;
```

---

### 8. Dropping and Viewing Triggers

**View All Triggers:**

```sql
-- Show all triggers in current database
SHOW TRIGGERS;

-- Show triggers for specific table
SHOW TRIGGERS WHERE `Table` = 'employees';

-- Show trigger definition
SHOW CREATE TRIGGER after_employee_insert;
```

**Drop Trigger:**

```sql
-- Drop trigger if exists
DROP TRIGGER IF EXISTS after_employee_insert;

-- Drop trigger (error if not exists)
DROP TRIGGER prevent_salary_cut;
```

**Query Trigger Metadata:**

```sql
-- Get trigger information from information_schema
SELECT 
    TRIGGER_NAME,
    EVENT_MANIPULATION,
    EVENT_OBJECT_TABLE,
    ACTION_TIMING,
    ACTION_STATEMENT
FROM information_schema.TRIGGERS
WHERE TRIGGER_SCHEMA = 'mydb';
```

**Disable/Enable Triggers (PostgreSQL):**

```sql
-- PostgreSQL syntax
ALTER TABLE employees DISABLE TRIGGER after_employee_insert;
ALTER TABLE employees ENABLE TRIGGER after_employee_insert;
```

**Note:** MySQL does not support disabling triggers without dropping them.

---

### 9. Real-World Example: Automatic Stock Update

**Scenario:**
E-commerce system where product inventory must automatically decrease when a sale is made.

**Implementation:**

```sql
-- Create tables
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    quantity INT,
    price DECIMAL(10,2)
);

CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity_sold INT,
    sale_date DATETIME DEFAULT NOW(),
    FOREIGN KEY (product_id) REFERENCES products(product_id)
);

-- Create trigger
DELIMITER //
CREATE TRIGGER update_stock_after_sale
AFTER INSERT
ON sales
FOR EACH ROW
BEGIN
    -- Reduce product stock
    UPDATE products
    SET quantity = quantity - NEW.quantity_sold
    WHERE product_id = NEW.product_id;
    
    -- Alert if stock is low
    IF (SELECT quantity FROM products WHERE product_id = NEW.product_id) < 10 THEN
        INSERT INTO alerts (message, created_at)
        VALUES (CONCAT('Low stock alert for product ID: ', NEW.product_id), NOW());
    END IF;
END //
DELIMITER ;
```

**Testing:**

```sql
-- Initial state
INSERT INTO products VALUES (1, 'Laptop', 50, 999.99);

-- Make a sale
INSERT INTO sales (product_id, quantity_sold) VALUES (1, 3);

-- Check updated stock
SELECT * FROM products WHERE product_id = 1;
-- Output: quantity = 47
```

**Benefits:**
- **Real-time inventory:** Stock levels always accurate
- **No application code:** Logic in database, works for all clients
- **Automatic alerts:** Low stock notifications
- **Data consistency:** Prevents overselling

**Use Cases:**
- E-commerce inventory management
- Retail point-of-sale systems
- Warehouse management
- Supply chain tracking

---

### 10. Database Event Handling (Scheduled Tasks)

#### Definition

**Events** are automated tasks that run **based on a schedule**, similar to cron jobs but managed entirely within the database. Events can execute any SQL statements at specified intervals.

**Key Features:**
- **Time-based execution:** Run at specific times or intervals
- **Database-integrated:** No external scheduler needed
- **Persistent:** Survive database restarts
- **Flexible scheduling:** One-time or recurring

---

#### Enable Event Scheduler

**MySQL:**

```sql
-- Enable event scheduler
SET GLOBAL event_scheduler = ON;

-- Check status
SHOW VARIABLES LIKE 'event_scheduler';

-- Permanently enable (add to my.cnf)
-- event_scheduler = ON
```

**View Events:**

```sql
SHOW EVENTS;

SHOW CREATE EVENT archive_old_orders;
```

---

#### Example: Auto Archive Old Orders

**Scenario:**
Automatically move orders older than 6 months to an archive table to keep the main table lean and improve query performance.

**Implementation:**

```sql
-- Create archive table
CREATE TABLE archive_orders LIKE orders;

-- Enable event scheduler
SET GLOBAL event_scheduler = ON;

-- Create event
DELIMITER //
CREATE EVENT archive_old_orders
ON SCHEDULE EVERY 1 MONTH
STARTS '2025-11-01 00:00:00'
DO
BEGIN
    -- Archive old orders
    INSERT INTO archive_orders 
    SELECT * FROM orders 
    WHERE order_date < NOW() - INTERVAL 6 MONTH;
    
    -- Delete archived orders from main table
    DELETE FROM orders 
    WHERE order_date < NOW() - INTERVAL 6 MONTH;
    
    -- Log archival
    INSERT INTO maintenance_log (action, rows_affected, timestamp)
    VALUES ('Archive old orders', ROW_COUNT(), NOW());
END //
DELIMITER ;
```

**Explanation:**

**ON SCHEDULE EVERY 1 MONTH:** Runs monthly automatically
**STARTS '2025-11-01 00:00:00':** First execution date/time
**NOW() - INTERVAL 6 MONTH:** Orders older than 6 months
**ROW_COUNT():** Number of rows affected by previous statement
**Automatic maintenance:** No manual intervention required

---

#### Event Scheduling Options

**One-Time Event:**

```sql
CREATE EVENT send_monthly_report
ON SCHEDULE AT '2025-11-01 09:00:00'
DO
    CALL generate_monthly_report();
```

**Recurring Event with End Date:**

```sql
CREATE EVENT daily_cleanup
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 02:00:00'
ENDS '2025-12-31 23:59:59'
DO
    DELETE FROM temp_data WHERE created_at < NOW() - INTERVAL 1 DAY;
```

**Complex Schedule:**

```sql
-- Every 6 hours
CREATE EVENT sync_data
ON SCHEDULE EVERY 6 HOUR
DO
    CALL sync_external_data();

-- Every Friday at 6 PM
CREATE EVENT weekly_backup
ON SCHEDULE EVERY 1 WEEK
STARTS '2025-10-25 18:00:00'  -- Next Friday
DO
    CALL backup_database();
```

---

#### Manage Events

**Drop Event:**

```sql
DROP EVENT IF EXISTS archive_old_orders;
```

**Disable/Enable Event:**

```sql
ALTER EVENT archive_old_orders DISABLE;
ALTER EVENT archive_old_orders ENABLE;
```

**Modify Event Schedule:**

```sql
ALTER EVENT archive_old_orders
ON SCHEDULE EVERY 2 MONTH;
```

---

### 11. Advantages

**Automation:**
- ✅ Reduces manual work for repetitive actions
- ✅ Eliminates human error
- ✅ 24/7 operation without supervision
- ✅ Immediate response to data changes

**Consistency:**
- ✅ Enforces business rules automatically
- ✅ Same logic applied every time
- ✅ Cannot be bypassed by applications
- ✅ Centralized rule management

**Security:**
- ✅ Prevents unauthorized or incorrect changes
- ✅ Validates data at database level
- ✅ Audit trail for compliance
- ✅ Reduces application-level vulnerabilities

**Auditability:**
- ✅ Keeps track of all modifications
- ✅ Who, what, when tracking
- ✅ Compliance with regulations (SOX, GDPR)
- ✅ Historical data for analysis

**Performance:**
- ✅ Real-time reaction to events
- ✅ No network round-trips for validations
- ✅ Database-level optimizations
- ✅ Batch operations via events

---

### 12. Disadvantages

**Debugging Difficulty:**
- 🚫 Hard to trace logic errors inside triggers
- 🚫 No step-by-step debugging tools
- 🚫 Errors can be cryptic
- 🚫 Testing requires actual database operations

**Hidden Logic:**
- 🚫 Too many triggers make systems unpredictable
- 🚫 Logic not visible in application code
- 🚫 New developers may miss trigger effects
- 🚫 Difficult to document and maintain

**Performance Overhead:**
- 🚫 Frequent triggers slow heavy transaction systems
- 🚫 Each row operation may fire multiple triggers
- 🚫 Cascading triggers can cause exponential slowdown
- 🚫 Locks held longer during trigger execution

**Portability Issues:**
- 🚫 Trigger syntax varies between databases
- 🚫 Not all databases support same features
- 🚫 Migration to different DBMS is complex

**Best Practice Tips:**
> 💡 **Tip:** Use triggers wisely — only when the logic truly belongs in the database layer. Keep triggers simple, well-documented, and avoid complex business logic that's better suited for application code.

---

### 13. Real-World Use Cases

| Use Case                | Description                                          | Example                                  |
| ----------------------- | ---------------------------------------------------- | ---------------------------------------- |
| **Audit Trail**         | Log every INSERT/UPDATE/DELETE in a separate table   | Track all changes to customer data       |
| **Data Validation**     | Prevent invalid data updates                         | Ensure email format, age > 0             |
| **Cascading Actions**   | Delete related records automatically                 | Delete order items when order is deleted |
| **Notification System** | Trigger email/SMS after specific events              | Alert admin on large transactions        |
| **Automated Maintenance** | Run cleanup or recalculation events                | Archive old data, rebuild indexes        |
| **Inventory Management** | Update stock levels on sales                        | Reduce quantity after purchase           |
| **Computed Columns**    | Calculate derived values automatically               | Total = quantity * price                 |
| **Replication**         | Sync data across tables or databases                 | Mirror changes to reporting database     |
| **Security Enforcement** | Prevent unauthorized deletions or updates           | Block deletion of locked records         |
| **Data Archival**       | Move historical data to archive tables               | Monthly archive of old transactions      |

---

### 14. Example: Automatic Timestamp Update

**Scenario:**
Automatically update `updated_at` timestamp whenever a record is modified, ensuring accurate modification tracking without application code changes.

**Implementation:**

```sql
-- Add timestamp columns
ALTER TABLE students
ADD COLUMN created_at DATETIME DEFAULT NOW(),
ADD COLUMN updated_at DATETIME DEFAULT NOW();

-- Create trigger
DELIMITER //
CREATE TRIGGER update_timestamp
BEFORE UPDATE
ON students
FOR EACH ROW
BEGIN
    SET NEW.updated_at = NOW();
END //
DELIMITER ;
```

**Testing:**

```sql
-- Insert record
INSERT INTO students (student_id, name, age) VALUES (1, 'Alice', 20);

-- Check timestamps
SELECT student_id, name, created_at, updated_at FROM students WHERE student_id = 1;
-- created_at: 2025-10-18 10:00:00
-- updated_at: 2025-10-18 10:00:00

-- Update record
UPDATE students SET age = 21 WHERE student_id = 1;

-- Check updated timestamp
SELECT student_id, name, created_at, updated_at FROM students WHERE student_id = 1;
-- created_at: 2025-10-18 10:00:00 (unchanged)
-- updated_at: 2025-10-18 10:05:00 (automatically updated)
```

**Benefits:**
> 💡 Keeps "last modified" data always accurate without manual effort, perfect for audit trails and data synchronization.

---

### 15. Summary

| Concept                | Description                                          |
| ---------------------- | ---------------------------------------------------- |
| **Trigger**            | Auto executes SQL code when data changes (INSERT/UPDATE/DELETE) |
| **Event**              | Scheduled task for automated execution (time-based)  |
| **BEFORE Trigger**     | Executes before operation, can modify data or prevent operation |
| **AFTER Trigger**      | Executes after operation, used for logging and cascading |
| **OLD / NEW**          | Pseudo-records to access before/after row data       |
| **Event Scheduler**    | Database-managed cron-like scheduler                 |
| **Best Practice**      | Use for validation, auditing, automation; avoid overuse |
| **Caution**            | Can be hard to debug; avoid complex logic            |

**Key Takeaways:**

**Triggers:**
- ✅ Automatic, event-driven, data-centric
- ✅ Enforce business rules at database level
- ✅ Perfect for auditing and validation
- ⚠️ Use sparingly to maintain clarity

**Events:**
- ✅ Scheduled, time-based, maintenance-focused
- ✅ Automate routine database tasks
- ✅ No external scheduler required
- ⚠️ Monitor execution and performance

**When to Use:**
- **Triggers:** Data integrity, auditing, cascading operations
- **Events:** Maintenance, archival, scheduled reports, cleanup

**When to Avoid:**
- Complex business logic (better in application)
- Heavy computations (performance impact)
- Extensive cascading (unpredictable behavior)

**Final Thought:**
> Triggers act like *database guardians* — silently watching your data and reacting instantly. When combined with Events, they make your database **intelligent, self-healing, and automated**.
''',
    codeSnippet: '''
-- ========================================
-- 1. AUDIT LOG TRIGGER (AFTER INSERT)
-- ========================================

-- Create audit log table
CREATE TABLE audit_log (
    log_id INT AUTO_INCREMENT PRIMARY KEY,
    action VARCHAR(50),
    username VARCHAR(100),
    action_time DATETIME,
    details TEXT
);

-- Create audit trigger
DELIMITER //
CREATE TRIGGER after_employee_insert
AFTER INSERT
ON employees
FOR EACH ROW
BEGIN
    INSERT INTO audit_log(action, username, action_time, details)
    VALUES('INSERT', NEW.name, NOW(), CONCAT('New employee: ', NEW.name, ' (ID: ', NEW.employee_id, ')'));
END //
DELIMITER ;

-- Test
INSERT INTO employees (employee_id, name, department, salary)
VALUES (101, 'John Doe', 'Engineering', 75000);

SELECT * FROM audit_log;

-- ========================================
-- 2. PREVENT SALARY REDUCTION (BEFORE UPDATE)
-- ========================================

DELIMITER //
CREATE TRIGGER prevent_salary_cut
BEFORE UPDATE
ON employees
FOR EACH ROW
BEGIN
    IF NEW.salary < OLD.salary THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Salary reduction not allowed! Salary can only be increased.';
    END IF;
END //
DELIMITER ;

-- Test (will succeed)
UPDATE employees SET salary = 80000 WHERE employee_id = 101;

-- Test (will fail)
UPDATE employees SET salary = 70000 WHERE employee_id = 101;

-- ========================================
-- 3. BACKUP DELETED RECORDS (AFTER DELETE)
-- ========================================

-- Create backup table
CREATE TABLE deleted_employees (
    backup_id INT AUTO_INCREMENT PRIMARY KEY,
    employee_id INT,
    name VARCHAR(100),
    department VARCHAR(50),
    salary DECIMAL(10,2),
    deleted_on DATETIME,
    deleted_by VARCHAR(100)
);

-- Create backup trigger
DELIMITER //
CREATE TRIGGER backup_deleted_employee
AFTER DELETE
ON employees
FOR EACH ROW
BEGIN
    INSERT INTO deleted_employees(employee_id, name, department, salary, deleted_on, deleted_by)
    VALUES(OLD.employee_id, OLD.name, OLD.department, OLD.salary, NOW(), USER());
END //
DELIMITER ;

-- Test
DELETE FROM employees WHERE employee_id = 101;
SELECT * FROM deleted_employees;

-- ========================================
-- 4. AUTOMATIC STOCK UPDATE (AFTER INSERT)
-- ========================================

-- Create tables
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    quantity INT,
    price DECIMAL(10,2)
);

CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity_sold INT,
    sale_date DATETIME DEFAULT NOW(),
    FOREIGN KEY (product_id) REFERENCES products(product_id)
);

CREATE TABLE alerts (
    alert_id INT AUTO_INCREMENT PRIMARY KEY,
    message TEXT,
    created_at DATETIME
);

-- Create trigger
DELIMITER //
CREATE TRIGGER update_stock_after_sale
AFTER INSERT
ON sales
FOR EACH ROW
BEGIN
    -- Reduce product stock
    UPDATE products
    SET quantity = quantity - NEW.quantity_sold
    WHERE product_id = NEW.product_id;
    
    -- Alert if stock is low
    IF (SELECT quantity FROM products WHERE product_id = NEW.product_id) < 10 THEN
        INSERT INTO alerts (message, created_at)
        VALUES (CONCAT('Low stock alert for product ID: ', NEW.product_id), NOW());
    END IF;
END //
DELIMITER ;

-- Test
INSERT INTO products VALUES (1, 'Laptop', 50, 999.99);
INSERT INTO sales (product_id, quantity_sold) VALUES (1, 3);
SELECT * FROM products WHERE product_id = 1;  -- quantity = 47

-- ========================================
-- 5. AUTOMATIC TIMESTAMP UPDATE (BEFORE UPDATE)
-- ========================================

-- Add timestamp columns
ALTER TABLE students
ADD COLUMN created_at DATETIME DEFAULT NOW(),
ADD COLUMN updated_at DATETIME DEFAULT NOW();

-- Create trigger
DELIMITER //
CREATE TRIGGER update_timestamp
BEFORE UPDATE
ON students
FOR EACH ROW
BEGIN
    SET NEW.updated_at = NOW();
END //
DELIMITER ;

-- Test
INSERT INTO students (student_id, name, age) VALUES (1, 'Alice', 20);
UPDATE students SET age = 21 WHERE student_id = 1;
SELECT student_id, name, created_at, updated_at FROM students WHERE student_id = 1;

-- ========================================
-- 6. DATA VALIDATION (BEFORE INSERT)
-- ========================================

DELIMITER //
CREATE TRIGGER validate_product_price
BEFORE INSERT
ON products
FOR EACH ROW
BEGIN
    IF NEW.price < 0 THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Price cannot be negative';
    END IF;
    
    IF NEW.quantity < 0 THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Quantity cannot be negative';
    END IF;
    
    -- Set default created_at if not provided
    IF NEW.created_at IS NULL THEN
        SET NEW.created_at = NOW();
    END IF;
END //
DELIMITER ;

-- ========================================
-- 7. CASCADE DELETE (AFTER DELETE)
-- ========================================

DELIMITER //
CREATE TRIGGER cascade_delete_order_items
AFTER DELETE
ON orders
FOR EACH ROW
BEGIN
    -- Delete all items associated with deleted order
    DELETE FROM order_items WHERE order_id = OLD.order_id;
    
    -- Log the cascade deletion
    INSERT INTO audit_log (action, details, action_time)
    VALUES ('CASCADE_DELETE', CONCAT('Deleted items for order: ', OLD.order_id), NOW());
END //
DELIMITER ;

-- ========================================
-- 8. COMPUTED COLUMN (BEFORE INSERT/UPDATE)
-- ========================================

DELIMITER //
CREATE TRIGGER calculate_total_price
BEFORE INSERT
ON order_items
FOR EACH ROW
BEGIN
    -- Calculate total based on quantity and unit price
    SET NEW.total_price = NEW.quantity * NEW.unit_price;
    
    -- Apply discount if applicable
    IF NEW.quantity > 10 THEN
        SET NEW.total_price = NEW.total_price * 0.9;  -- 10% discount
    END IF;
END //
DELIMITER ;

-- ========================================
-- 9. VIEWING AND MANAGING TRIGGERS
-- ========================================

-- Show all triggers
SHOW TRIGGERS;

-- Show triggers for specific table
SHOW TRIGGERS WHERE \`Table\` = 'employees';

-- Show trigger definition
SHOW CREATE TRIGGER after_employee_insert;

-- Drop trigger
DROP TRIGGER IF EXISTS after_employee_insert;

-- Query trigger metadata
SELECT 
    TRIGGER_NAME,
    EVENT_MANIPULATION,
    EVENT_OBJECT_TABLE,
    ACTION_TIMING,
    ACTION_STATEMENT
FROM information_schema.TRIGGERS
WHERE TRIGGER_SCHEMA = 'mydb';

-- ========================================
-- 10. EVENT HANDLING - ENABLE SCHEDULER
-- ========================================

-- Enable event scheduler
SET GLOBAL event_scheduler = ON;

-- Check status
SHOW VARIABLES LIKE 'event_scheduler';

-- View all events
SHOW EVENTS;

-- ========================================
-- 11. EVENT - AUTO ARCHIVE OLD ORDERS
-- ========================================

-- Create archive table
CREATE TABLE archive_orders LIKE orders;

-- Create maintenance log
CREATE TABLE maintenance_log (
    log_id INT AUTO_INCREMENT PRIMARY KEY,
    action VARCHAR(100),
    rows_affected INT,
    timestamp DATETIME
);

-- Create event
DELIMITER //
CREATE EVENT archive_old_orders
ON SCHEDULE EVERY 1 MONTH
STARTS '2025-11-01 00:00:00'
DO
BEGIN
    -- Archive old orders
    INSERT INTO archive_orders 
    SELECT * FROM orders 
    WHERE order_date < NOW() - INTERVAL 6 MONTH;
    
    -- Delete archived orders
    DELETE FROM orders 
    WHERE order_date < NOW() - INTERVAL 6 MONTH;
    
    -- Log archival
    INSERT INTO maintenance_log (action, rows_affected, timestamp)
    VALUES ('Archive old orders', ROW_COUNT(), NOW());
END //
DELIMITER ;

-- ========================================
-- 12. EVENT - ONE-TIME EXECUTION
-- ========================================

CREATE EVENT send_monthly_report
ON SCHEDULE AT '2025-11-01 09:00:00'
DO
    CALL generate_monthly_report();

-- ========================================
-- 13. EVENT - RECURRING WITH END DATE
-- ========================================

CREATE EVENT daily_cleanup
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 02:00:00'
ENDS '2025-12-31 23:59:59'
DO
    DELETE FROM temp_data WHERE created_at < NOW() - INTERVAL 1 DAY;

-- ========================================
-- 14. EVENT - COMPLEX SCHEDULES
-- ========================================

-- Every 6 hours
CREATE EVENT sync_data
ON SCHEDULE EVERY 6 HOUR
DO
    CALL sync_external_data();

-- Every week (Friday at 6 PM)
CREATE EVENT weekly_backup
ON SCHEDULE EVERY 1 WEEK
STARTS '2025-10-25 18:00:00'
DO
    CALL backup_database();

-- ========================================
-- 15. MANAGING EVENTS
-- ========================================

-- Drop event
DROP EVENT IF EXISTS archive_old_orders;

-- Disable event
ALTER EVENT archive_old_orders DISABLE;

-- Enable event
ALTER EVENT archive_old_orders ENABLE;

-- Modify event schedule
ALTER EVENT archive_old_orders
ON SCHEDULE EVERY 2 MONTH;

-- Show event definition
SHOW CREATE EVENT archive_old_orders;

-- ========================================
-- 16. MULTIPLE TRIGGERS ON SAME TABLE
-- ========================================

-- First trigger: Validate data
DELIMITER //
CREATE TRIGGER validate_employee_data
BEFORE INSERT
ON employees
FOR EACH ROW
BEGIN
    IF NEW.age < 18 THEN
        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Employee must be at least 18 years old';
    END IF;
END //
DELIMITER ;

-- Second trigger: Log insertion
DELIMITER //
CREATE TRIGGER log_employee_insert
AFTER INSERT
ON employees
FOR EACH ROW
BEGIN
    INSERT INTO audit_log (action, username, action_time)
    VALUES ('NEW_EMPLOYEE', NEW.name, NOW());
END //
DELIMITER ;

-- ========================================
-- 17. TRIGGER WITH CONDITIONAL LOGIC
-- ========================================

DELIMITER //
CREATE TRIGGER apply_discount_on_update
BEFORE UPDATE
ON orders
FOR EACH ROW
BEGIN
    DECLARE customer_type VARCHAR(20);
    
    -- Get customer type
    SELECT type INTO customer_type FROM customers WHERE customer_id = NEW.customer_id;
    
    -- Apply discount based on customer type
    IF customer_type = 'VIP' THEN
        SET NEW.discount = 0.15;  -- 15% discount
    ELSEIF customer_type = 'Regular' THEN
        SET NEW.discount = 0.05;  -- 5% discount
    ELSE
        SET NEW.discount = 0;
    END IF;
    
    -- Recalculate total
    SET NEW.total_amount = NEW.subtotal * (1 - NEW.discount);
END //
DELIMITER ;
''',
    revisionPoints: [
      'Triggers are special stored programs that automatically execute when INSERT, UPDATE, or DELETE occurs',
      'BEFORE triggers execute before the operation and can modify NEW values or prevent the operation',
      'AFTER triggers execute after the operation and are used for logging and cascading actions',
      'OLD pseudo-record contains original row data (available in UPDATE and DELETE)',
      'NEW pseudo-record contains new row data (available in INSERT and UPDATE)',
      'Triggers enforce business rules and maintain data integrity automatically at database level',
      'Use DELIMITER to change statement terminator when creating multi-line triggers in MySQL',
      'SIGNAL raises custom errors with SQLSTATE and MESSAGE_TEXT to prevent invalid operations',
      'Triggers are row-level (FOR EACH ROW) - execute once per affected row',
      'DROP TRIGGER IF EXISTS safely removes triggers without error if not found',
      'Events are scheduled tasks that run automatically at specified time intervals',
      'Event Scheduler must be enabled with SET GLOBAL event_scheduler = ON',
      'Events can run one-time (ON SCHEDULE AT) or recurring (ON SCHEDULE EVERY)',
      'Use events for automated maintenance: archiving old data, cleanup, generating reports',
      'Advantages: automation, consistency, security, auditability, real-time reaction',
      'Disadvantages: debugging difficulty, hidden logic, performance overhead on heavy transactions',
      'SHOW TRIGGERS displays all triggers; SHOW EVENTS displays all scheduled events',
      'Triggers cannot be bypassed by applications - logic always enforced at database level',
      'Use triggers for: audit trails, data validation, cascading actions, automatic timestamps',
      'Use events for: scheduled backups, data archival, maintenance tasks, periodic reports',
      'Avoid complex business logic in triggers - keep them simple and focused',
      'Triggers can call stored procedures for complex operations',
      'Multiple triggers can exist on same table with different timing (BEFORE/AFTER) and events',
      'Common use cases: inventory management, audit logging, computed columns, cascade operations',
      'Best practice: Use triggers wisely - only when logic truly belongs in database layer',
    ],
    quizQuestions: [
      Question(
        question: 'When does a BEFORE trigger execute?',
        options: ['After the operation', 'Before the operation', 'During the operation', 'Never automatically'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which pseudo-record is available in an INSERT trigger?',
        options: ['Only OLD', 'Only NEW', 'Both OLD and NEW', 'Neither OLD nor NEW'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does SIGNAL SQLSTATE do in a trigger?',
        options: [
          'Logs a message',
          'Raises a custom error and stops execution',
          'Sends a notification',
          'Commits the transaction'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Can BEFORE triggers modify NEW values?',
        options: ['Yes', 'No', 'Only for UPDATE', 'Only for INSERT'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which command enables the MySQL Event Scheduler?',
        options: [
          'START EVENT_SCHEDULER',
          'SET GLOBAL event_scheduler = ON',
          'ENABLE EVENT_SCHEDULER',
          'CREATE EVENT_SCHEDULER'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main difference between triggers and events?',
        options: [
          'Triggers run on schedule, events run on data changes',
          'Triggers run on data changes, events run on schedule',
          'Both run on schedule',
          'Both run on data changes'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Can AFTER triggers modify the data being inserted/updated?',
        options: ['Yes, always', 'No, data is already committed', 'Only UPDATE operations', 'Only if using SIGNAL'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is a common use case for AFTER DELETE triggers?',
        options: [
          'Validate data before deletion',
          'Prevent deletion',
          'Backup deleted data to archive table',
          'Modify the data being deleted'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What does FOR EACH ROW mean in a trigger?',
        options: [
          'Executes once for entire operation',
          'Executes once per affected row',
          'Executes for first row only',
          'Executes for last row only'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which event schedule type runs only once?',
        options: ['EVERY 1 DAY', 'AT specific_time', 'EVERY 1 MONTH', 'STARTS date_time'],
        correctIndex: 1,
      ),
      Question(
        question: 'How do you drop a trigger safely without error if it does not exist?',
        options: [
          'DELETE TRIGGER',
          'DROP TRIGGER',
          'DROP TRIGGER IF EXISTS',
          'REMOVE TRIGGER IF EXISTS'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a disadvantage of using too many triggers?',
        options: [
          'Improved performance',
          'Better security',
          'Hidden logic that makes systems unpredictable',
          'Easier debugging'
        ],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'views_materialized',
    title: '16. Views & Materialized Views',
    explanation: '''## Views & Materialized Views in Databases

### 1. Introduction

#### What is a View?

**Definition:**

A **View** is a **virtual table** created from one or more real tables using a SQL query. It **does not store data** physically — instead, it shows the **result of a stored SELECT query** each time you access it.

**Key Characteristics:**
- **Virtual existence:** No physical storage of data
- **Dynamic data:** Always reflects current data from base tables
- **Saved query:** Behaves like a table but is actually a query
- **Security layer:** Can hide sensitive columns from users
- **Query simplification:** Complex JOINs made simple

**Analogy:**
> 💡 Think of a View as a *saved query* that behaves like a table. Like a window, it provides a view into the underlying data without duplicating it.

**Example Scenario:**

Instead of writing this complex query repeatedly:
```sql
SELECT e.emp_name, d.dept_name, s.salary
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
JOIN salaries s ON e.emp_id = s.emp_id
WHERE s.salary > 50000;
```

Create a view once:
```sql
CREATE VIEW high_earners AS
SELECT e.emp_name, d.dept_name, s.salary
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
JOIN salaries s ON e.emp_id = s.emp_id
WHERE s.salary > 50000;
```

Then simply query:
```sql
SELECT * FROM high_earners;
```

---

#### What is a Materialized View?

**Definition:**

A **Materialized View** is similar to a regular view but **stores the query result physically** in the database. It creates a cached copy of the data that must be **refreshed manually or automatically** to update its contents.

**Key Characteristics:**
- **Physical storage:** Data is actually stored on disk
- **Precomputed results:** Query runs once, results cached
- **Faster queries:** No recalculation needed
- **Stale data risk:** May not reflect latest changes until refreshed
- **Storage overhead:** Requires disk space

**Analogy:**
> 💡 **Regular Views = Virtual** (like streaming live TV)
> 💡 **Materialized Views = Cached & Stored** (like recorded TV shows)

**Performance Comparison:**

| Operation                    | Regular View           | Materialized View |
| ---------------------------- | ---------------------- | ----------------- |
| **First Query**              | Slow (executes query)  | Fast (cached)     |
| **Subsequent Queries**       | Slow (re-executes)     | Fast (cached)     |
| **Data Freshness**           | Always current         | May be stale      |
| **Complex Aggregations**     | Slow every time        | Fast (precomputed)|
| **Storage Space**            | None                   | Significant       |

---

### 2. Why Use Views

**Benefits:**

**Simplify Complex Queries:**
- ✅ Hide complex JOINs behind simple SELECT statements
- ✅ Reduce code duplication across applications
- ✅ Make SQL more readable and maintainable
- ✅ Standardize data access patterns

**Enhance Data Security:**
- ✅ Hide sensitive columns (salary, SSN, passwords)
- ✅ Grant access to views, not base tables
- ✅ Row-level security through WHERE clauses
- ✅ Column-level security by selecting specific columns

**Provide Customized Data Access:**
- ✅ Different views for different user roles
- ✅ Department-specific data views
- ✅ Regional data filtering
- ✅ Personalized dashboards

**Make Reporting Easier:**
- ✅ Pre-joined tables for reports
- ✅ Calculated fields readily available
- ✅ Business-friendly column names
- ✅ Aggregated metrics for dashboards

**Improve Code Maintainability:**
- ✅ Change view definition without changing application code
- ✅ Centralized business logic
- ✅ Database schema abstraction
- ✅ Easier testing and debugging

---

### 3. Creating a View

#### Basic Syntax

```sql
CREATE VIEW view_name AS
SELECT columns
FROM table_name
WHERE condition;
```

**Components:**
- **view_name:** Unique identifier for the view
- **AS:** Defines the query that creates the view
- **SELECT query:** Any valid SELECT statement

---

#### Example 1: Simple View

**Create View:**

```sql
CREATE VIEW employee_info AS
SELECT emp_id, emp_name, department, salary
FROM employees
WHERE department = 'Sales';
```

**Query View:**

```sql
SELECT * FROM employee_info;
```

**Output:**

| emp_id | emp_name | department | salary |
| ------ | -------- | ---------- | ------ |
| 101    | Alex     | Sales      | 50000  |
| 103    | Maya     | Sales      | 55000  |
| 105    | John     | Sales      | 52000  |

**Benefits:**
- Users see only Sales department employees
- Simplified access to frequently needed data
- Can grant SELECT on view without base table access

---

#### Example 2: View with JOIN

```sql
CREATE VIEW employee_details AS
SELECT 
    e.emp_id,
    e.emp_name,
    d.dept_name,
    p.project_name,
    e.salary
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
JOIN projects p ON e.project_id = p.project_id;
```

**Usage:**

```sql
-- Instead of writing complex JOIN every time
SELECT * FROM employee_details WHERE dept_name = 'Engineering';
```

---

#### Example 3: View with Calculated Columns

```sql
CREATE VIEW employee_annual_salary AS
SELECT 
    emp_id,
    emp_name,
    salary AS monthly_salary,
    salary * 12 AS annual_salary,
    salary * 12 * 0.1 AS annual_bonus
FROM employees;
```

---

### 4. Updating Data Through a View

**When Views Are Updatable:**

If the view is based on a **single table** and does **not use** aggregations, DISTINCT, GROUP BY, or UNION, you can modify the base table through the view.

**Example:**

```sql
-- Update through view
UPDATE employee_info
SET salary = 60000
WHERE emp_id = 101;

-- This updates the employees table automatically
SELECT * FROM employees WHERE emp_id = 101;
-- salary is now 60000
```

**Insert Through View:**

```sql
INSERT INTO employee_info (emp_id, emp_name, department, salary)
VALUES (110, 'Sarah', 'Sales', 58000);

-- This inserts into the employees table
```

**Delete Through View:**

```sql
DELETE FROM employee_info WHERE emp_id = 110;

-- This deletes from the employees table
```

---

### 5. When Views Are Not Updatable

**A view is NOT updatable if it contains:**

**Aggregate Functions:**
```sql
-- Not updatable
CREATE VIEW dept_summary AS
SELECT department, COUNT(*) AS emp_count, AVG(salary) AS avg_salary
FROM employees
GROUP BY department;
```

**GROUP BY or HAVING:**
```sql
-- Not updatable
CREATE VIEW high_salary_depts AS
SELECT department, AVG(salary) AS avg_salary
FROM employees
GROUP BY department
HAVING AVG(salary) > 60000;
```

**DISTINCT:**
```sql
-- Not updatable
CREATE VIEW unique_departments AS
SELECT DISTINCT department FROM employees;
```

**UNION:**
```sql
-- Not updatable
CREATE VIEW all_people AS
SELECT name FROM employees
UNION
SELECT name FROM contractors;
```

**Multiple Tables (JOINs):**
```sql
-- Not updatable (in most databases)
CREATE VIEW emp_dept AS
SELECT e.emp_name, d.dept_name
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id;
```

**Subqueries in SELECT:**
```sql
-- Not updatable
CREATE VIEW emp_with_avg AS
SELECT emp_name, salary, (SELECT AVG(salary) FROM employees) AS avg_salary
FROM employees;
```

---

### 6. Dropping and Modifying Views

**Drop View:**

```sql
-- Drop view if exists (safe)
DROP VIEW IF EXISTS employee_info;

-- Drop view (error if not exists)
DROP VIEW employee_info;
```

**Modify View (Replace):**

```sql
-- Replace existing view or create new
CREATE OR REPLACE VIEW employee_info AS
SELECT emp_id, emp_name, department, salary, hire_date
FROM employees
WHERE department = 'Sales';
```

**Rename View:**

```sql
-- MySQL
RENAME TABLE old_view_name TO new_view_name;

-- PostgreSQL
ALTER VIEW old_view_name RENAME TO new_view_name;
```

**View Metadata:**

```sql
-- Show all views
SHOW FULL TABLES WHERE Table_type = 'VIEW';

-- Show view definition
SHOW CREATE VIEW employee_info;

-- Query view information
SELECT * FROM information_schema.views WHERE table_name = 'employee_info';
```

---

### 7. Real-World Example: Securing Sensitive Data

**Scenario:**
HR managers should see employee information but **not salaries**.

**Solution: Create Restricted View**

```sql
-- Create view without sensitive columns
CREATE VIEW employee_basic AS
SELECT emp_id, emp_name, department, hire_date, email
FROM employees;

-- Grant access only to the view
GRANT SELECT ON employee_basic TO hr_user;

-- HR users cannot access the base table
REVOKE SELECT ON employees FROM hr_user;
```

**HR User Queries:**

```sql
-- HR user can query this
SELECT * FROM employee_basic;

-- Output: No salary column visible
```

**Output:**

| emp_id | emp_name | department  | hire_date  | email              |
| ------ | -------- | ----------- | ---------- | ------------------ |
| 101    | Alex     | Sales       | 2020-01-15 | alex@company.com   |
| 102    | Maya     | Engineering | 2019-06-20 | maya@company.com   |

**Benefits:**
> 🧠 This hides sensitive columns like `salary`, `ssn`, and `bank_account` while allowing HR to perform their duties.

---

### 8. Example: Simplifying Complex Queries

**Problem:**
Application needs to repeatedly query student performance data involving multiple JOINs.

**Without View (Complex Query):**

```sql
-- Must write this complex query repeatedly
SELECT 
    s.student_id,
    s.student_name,
    c.course_name,
    g.grade,
    g.exam_date
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
JOIN grades g ON e.enrollment_id = g.enrollment_id;
```

**With View (Simple Solution):**

```sql
-- Create view once
CREATE VIEW student_performance AS
SELECT 
    s.student_id,
    s.student_name,
    c.course_name,
    g.grade,
    g.exam_date
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
JOIN grades g ON e.enrollment_id = g.enrollment_id;
```

**Simple Queries:**

```sql
-- Query high performers
SELECT * FROM student_performance WHERE grade > 80;

-- Query by course
SELECT * FROM student_performance WHERE course_name = 'Database Systems';

-- Query recent exams
SELECT * FROM student_performance WHERE exam_date >= CURDATE() - INTERVAL 30 DAY;
```

**Benefits:**
- 90% reduction in query complexity
- Consistent data access across applications
- Easier to maintain and debug
- Single point of change for business logic

---

### 9. Materialized Views

#### Definition

A **Materialized View (MV)** stores the **result set physically** on disk, improving performance for complex queries that involve:
- Large data volumes
- Complex aggregations (SUM, AVG, COUNT)
- Multiple table JOINs
- Expensive calculations

**Key Difference:**

**Regular View:**
- Query executes **every time** you SELECT from it
- Always shows **current data**
- **Slower** for complex queries

**Materialized View:**
- Query executes **once**, result stored
- Shows **cached data** (may be stale)
- **Much faster** for complex queries

---

#### Syntax (PostgreSQL/Oracle)

**PostgreSQL:**

```sql
CREATE MATERIALIZED VIEW view_name AS
SELECT columns
FROM tables
WHERE conditions;
```

**Oracle:**

```sql
CREATE MATERIALIZED VIEW view_name
BUILD IMMEDIATE
REFRESH FAST
AS
SELECT columns
FROM tables;
```

---

#### Example 1: Sales Summary

**Scenario:**
Dashboard needs monthly sales by region, calculated from millions of transactions.

**Create Materialized View:**

```sql
CREATE MATERIALIZED VIEW monthly_sales AS
SELECT 
    region,
    DATE_TRUNC('month', order_date) AS month,
    SUM(amount) AS total_sales,
    COUNT(*) AS order_count,
    AVG(amount) AS avg_order_value
FROM orders
GROUP BY region, DATE_TRUNC('month', order_date);
```

**Query Materialized View:**

```sql
-- Instant results (no recalculation)
SELECT * FROM monthly_sales
WHERE month = '2025-10-01'
ORDER BY total_sales DESC;
```

**Performance:**
- **Without MV:** 10 seconds (scans millions of rows, aggregates)
- **With MV:** 0.1 seconds (reads cached results)
- **100x faster!**

---

#### Example 2: Product Analytics

```sql
CREATE MATERIALIZED VIEW product_analytics AS
SELECT 
    p.product_id,
    p.product_name,
    COUNT(DISTINCT o.customer_id) AS unique_customers,
    SUM(oi.quantity) AS total_units_sold,
    SUM(oi.quantity * oi.unit_price) AS total_revenue,
    AVG(r.rating) AS avg_rating,
    COUNT(r.review_id) AS review_count
FROM products p
LEFT JOIN order_items oi ON p.product_id = oi.product_id
LEFT JOIN orders o ON oi.order_id = o.order_id
LEFT JOIN reviews r ON p.product_id = r.product_id
GROUP BY p.product_id, p.product_name;
```

---

### 10. Refreshing a Materialized View

**Manual Refresh (PostgreSQL):**

```sql
-- Full refresh (regenerate entire view)
REFRESH MATERIALIZED VIEW monthly_sales;

-- Concurrent refresh (allows queries during refresh)
REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales;
```

**Automatic Refresh (Oracle):**

```sql
-- Refresh on commit (whenever base table changes)
CREATE MATERIALIZED VIEW monthly_sales
REFRESH FAST ON COMMIT
AS
SELECT region, SUM(amount) AS total_sales
FROM orders
GROUP BY region;

-- Refresh on schedule (every day at 2 AM)
CREATE MATERIALIZED VIEW monthly_sales
REFRESH FAST START WITH SYSDATE NEXT SYSDATE + 1
AS
SELECT region, SUM(amount) AS total_sales
FROM orders
GROUP BY region;
```

**Refresh Strategies:**

**COMPLETE Refresh:**
- Regenerates entire materialized view
- Slower but always accurate
- Use when: Data changes significantly

**FAST Refresh:**
- Only updates changed data (incremental)
- Faster but requires materialized view logs
- Use when: Small incremental changes

---

### 11. Benefits of Materialized Views

**Improved Performance:**
- ✅ Precomputed data = **faster queries** (10-100x speedup)
- ✅ No recalculation of complex aggregations
- ✅ Instant dashboard loading
- ✅ Reduced CPU and I/O load

**Reduced Database Load:**
- ✅ Fewer complex query executions
- ✅ Lower CPU usage during peak hours
- ✅ Better resource allocation
- ✅ Improved concurrency

**Great for Reports/Dashboards:**
- ✅ Keeps summarized data ready
- ✅ Real-time dashboard performance
- ✅ Executive reports load instantly
- ✅ Analytics queries don't slow production

**Offline Analytics:**
- ✅ Even if base tables are locked
- ✅ Independent of source data availability
- ✅ Point-in-time snapshots
- ✅ Historical analysis

---

### 12. Limitations of Materialized Views

**Storage Overhead:**
- 🚫 Requires extra **disk space** for cached data
- 🚫 Can be significant for large datasets
- 🚫 Duplicate storage of same data
- 🚫 Costs increase with data volume

**Stale Data Risk:**
- 🚫 Data can become **outdated** if not refreshed
- 🚫 May show incorrect results
- 🚫 User confusion if not aware of refresh schedule
- 🚫 Compliance issues with real-time requirements

**Maintenance Overhead:**
- 🚫 Slight **refresh logic** complexity
- 🚫 Monitoring refresh failures
- 🚫 Managing refresh schedules
- 🚫 Handling failed refreshes

**Refresh Time:**
- 🚫 Full refresh can be slow for large views
- 🚫 Locks may affect concurrent queries
- 🚫 Resource-intensive during refresh
- 🚫 Scheduling challenges

---

### 13. Real-World Example: Sales Dashboard

**Scenario:**
A retail company wants a real-time dashboard showing monthly revenue per region, calculated from 50 million transaction records.

---

**Step 1: Create Materialized View**

```sql
CREATE MATERIALIZED VIEW revenue_summary AS
SELECT 
    region,
    DATE_TRUNC('month', sale_date) AS month,
    SUM(total_amount) AS monthly_revenue,
    COUNT(*) AS transaction_count,
    AVG(total_amount) AS avg_transaction,
    MAX(total_amount) AS largest_sale
FROM sales
WHERE sale_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY region, DATE_TRUNC('month', sale_date);
```

---

**Step 2: Schedule Daily Refresh**

```sql
-- PostgreSQL: Create cron job or use pg_cron extension
SELECT cron.schedule('refresh-revenue', '0 2 * * *', 
    'REFRESH MATERIALIZED VIEW revenue_summary');

-- Oracle: Automatic refresh
CREATE MATERIALIZED VIEW revenue_summary
REFRESH FAST START WITH SYSDATE NEXT SYSDATE + 1;
```

---

**Step 3: Dashboard Queries**

```sql
-- Current month revenue by region
SELECT region, monthly_revenue
FROM revenue_summary
WHERE month = DATE_TRUNC('month', CURRENT_DATE)
ORDER BY monthly_revenue DESC;

-- Year-over-year comparison
SELECT 
    region,
    SUM(CASE WHEN month >= CURRENT_DATE - INTERVAL '1 year' 
             THEN monthly_revenue ELSE 0 END) AS current_year,
    SUM(CASE WHEN month < CURRENT_DATE - INTERVAL '1 year' 
             THEN monthly_revenue ELSE 0 END) AS previous_year
FROM revenue_summary
GROUP BY region;
```

---

**Step 4: Output**

| region | monthly_revenue | transaction_count | avg_transaction |
| ------ | --------------- | ----------------- | --------------- |
| North  | 1,200,000       | 5,432             | 220.90          |
| South  | 980,000         | 4,123             | 237.71          |
| East   | 1,150,000       | 5,890             | 195.25          |
| West   | 1,420,000       | 6,234             | 227.79          |

**Performance Improvement:**
- **Without MV:** 15 seconds (full table scan + aggregation)
- **With MV:** 0.05 seconds (cached results)
- **300x faster!**

---

### 14. Comparison Table

| Feature                | View                             | Materialized View           |
| ---------------------- | -------------------------------- | --------------------------- |
| **Storage**            | Virtual (no storage)             | Physical (stored on disk)   |
| **Performance**        | Slower (computed on each query)  | Faster (precomputed)        |
| **Data Freshness**     | Always up-to-date                | May be outdated             |
| **Refresh Required**   | No (automatic)                   | Yes (manual or scheduled)   |
| **Maintenance**        | None                             | Needs refresh strategy      |
| **Disk Space**         | None                             | Significant                 |
| **Use Case**           | Security, abstraction            | Reporting, analytics        |
| **Query Complexity**   | Any complexity                   | Complex aggregations        |
| **Best For**           | Simple queries, current data     | Complex queries, historical data |
| **Update Frequency**   | Real-time                        | Batch/scheduled             |

---

### 15. Security with Views

**Row-Level Security:**

```sql
-- Sales reps see only their own region
CREATE VIEW my_sales AS
SELECT * FROM sales
WHERE region = (SELECT region FROM employees WHERE emp_id = CURRENT_USER());
```

**Column-Level Security:**

```sql
-- Hide sensitive columns
CREATE VIEW customer_public AS
SELECT customer_id, name, email, city
FROM customers;
-- Excludes: ssn, credit_card, income
```

**Grant Permissions:**

```sql
-- Grant access to view, not base table
GRANT SELECT ON customer_public TO sales_team;
REVOKE SELECT ON customers FROM sales_team;
```

**Benefits:**
> ✅ Keeps underlying tables protected while allowing controlled data access.

---

### 16. Best Practices

**Naming Conventions:**

```sql
-- Prefix views with 'v_'
CREATE VIEW v_employee_basic AS ...

-- Prefix materialized views with 'mv_'
CREATE MATERIALIZED VIEW mv_sales_summary AS ...
```

**Use Views for:**
1. ✅ **Simplifying complex JOINs** (DRY principle)
2. ✅ **Security and access control** (hide sensitive data)
3. ✅ **Backward compatibility** (schema changes)
4. ✅ **Consistent data access** (standardized queries)

**Use Materialized Views for:**
1. ✅ **Pre-aggregated reports** (monthly summaries)
2. ✅ **Heavy analytics** (data warehouse queries)
3. ✅ **Dashboard data** (instant loading)
4. ✅ **Historical snapshots** (point-in-time analysis)

**Refresh Strategy:**
- **High volatility data:** Refresh frequently (hourly, daily)
- **Low volatility data:** Refresh less often (weekly, monthly)
- **Critical dashboards:** Use regular views for real-time data
- **Reports:** Use materialized views for performance

**Performance Tips:**
- ✅ Index materialized views for faster queries
- ✅ Partition large materialized views
- ✅ Monitor refresh time and adjust schedule
- ✅ Use CONCURRENTLY for PostgreSQL refreshes
- ✅ Drop unused views to save resources

**Security Tips:**
- ✅ Combine views with **roles and permissions**
- ✅ Audit view usage for compliance
- ✅ Document view purpose and data sources
- ✅ Regularly review and update view definitions

---

### 17. Summary

| Concept                    | Key Idea                                              |
| -------------------------- | ----------------------------------------------------- |
| **View**                   | Virtual table derived from SQL query (no storage)     |
| **Materialized View**      | Physical copy of query result for faster access       |
| **View Benefits**          | Simplify queries, secure data, code maintainability   |
| **MV Benefits**            | Improved performance (10-100x), reduced load          |
| **Use Cases**              | Views for security/abstraction, MVs for analytics     |
| **Refresh**                | Manual or scheduled for materialized views            |
| **Performance**            | Views = slower but current, MVs = faster but may be stale |
| **Best Practice**          | Use views for access control, MVs for heavy reporting |

**Key Takeaways:**

**Views:**
- ✅ No storage overhead
- ✅ Always current data
- ✅ Perfect for security and simplification
- ⚠️ Slower for complex queries

**Materialized Views:**
- ✅ Dramatically faster queries
- ✅ Great for analytics and dashboards
- ✅ Reduces database load
- ⚠️ Requires storage and refresh strategy

**When to Use:**
- **Views:** Real-time data, security, query simplification
- **Materialized Views:** Reports, dashboards, complex aggregations, historical analysis

**Final Thought:**
> *Views bring clarity. Materialized Views bring speed.*
> Together, they make data **cleaner, faster, and smarter** for every application — from analytics dashboards to enterprise reporting.
''',
    codeSnippet: '''
-- ========================================
-- 1. BASIC VIEW CREATION
-- ========================================

-- Simple view
CREATE VIEW employee_info AS
SELECT emp_id, emp_name, department, salary
FROM employees
WHERE department = 'Sales';

-- Query the view
SELECT * FROM employee_info;

-- Filter view results
SELECT * FROM employee_info WHERE salary > 50000;

-- ========================================
-- 2. VIEW WITH JOIN
-- ========================================

CREATE VIEW employee_details AS
SELECT 
    e.emp_id,
    e.emp_name,
    d.dept_name,
    p.project_name,
    e.salary
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
JOIN projects p ON e.project_id = p.project_id;

-- Usage
SELECT * FROM employee_details WHERE dept_name = 'Engineering';

-- ========================================
-- 3. VIEW WITH CALCULATED COLUMNS
-- ========================================

CREATE VIEW employee_annual_salary AS
SELECT 
    emp_id,
    emp_name,
    salary AS monthly_salary,
    salary * 12 AS annual_salary,
    salary * 12 * 0.1 AS annual_bonus,
    CASE 
        WHEN salary > 70000 THEN 'High'
        WHEN salary > 50000 THEN 'Medium'
        ELSE 'Low'
    END AS salary_grade
FROM employees;

SELECT * FROM employee_annual_salary WHERE salary_grade = 'High';

-- ========================================
-- 4. UPDATING DATA THROUGH VIEW
-- ========================================

-- Update through view (single table, no aggregation)
UPDATE employee_info
SET salary = 60000
WHERE emp_id = 101;

-- Insert through view
INSERT INTO employee_info (emp_id, emp_name, department, salary)
VALUES (110, 'Sarah', 'Sales', 58000);

-- Delete through view
DELETE FROM employee_info WHERE emp_id = 110;

-- ========================================
-- 5. SECURITY VIEW (HIDE SENSITIVE DATA)
-- ========================================

-- Create view without sensitive columns
CREATE VIEW employee_basic AS
SELECT emp_id, emp_name, department, hire_date, email
FROM employees;
-- Excludes: salary, ssn, bank_account

-- Grant access only to view
GRANT SELECT ON employee_basic TO hr_user;
REVOKE SELECT ON employees FROM hr_user;

-- ========================================
-- 6. VIEW WITH AGGREGATION (NOT UPDATABLE)
-- ========================================

CREATE VIEW dept_summary AS
SELECT 
    department,
    COUNT(*) AS emp_count,
    AVG(salary) AS avg_salary,
    MIN(salary) AS min_salary,
    MAX(salary) AS max_salary
FROM employees
GROUP BY department;

SELECT * FROM dept_summary ORDER BY avg_salary DESC;

-- ========================================
-- 7. VIEW FOR COMPLEX QUERY SIMPLIFICATION
-- ========================================

CREATE VIEW student_performance AS
SELECT 
    s.student_id,
    s.student_name,
    c.course_name,
    g.grade,
    g.exam_date,
    CASE 
        WHEN g.grade >= 90 THEN 'A'
        WHEN g.grade >= 80 THEN 'B'
        WHEN g.grade >= 70 THEN 'C'
        WHEN g.grade >= 60 THEN 'D'
        ELSE 'F'
    END AS letter_grade
FROM students s
JOIN enrollments e ON s.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
JOIN grades g ON e.enrollment_id = g.enrollment_id;

-- Simple queries on complex view
SELECT * FROM student_performance WHERE grade > 80;
SELECT * FROM student_performance WHERE course_name = 'Database Systems';

-- ========================================
-- 8. MODIFYING AND DROPPING VIEWS
-- ========================================

-- Replace existing view
CREATE OR REPLACE VIEW employee_info AS
SELECT emp_id, emp_name, department, salary, hire_date
FROM employees
WHERE department = 'Sales';

-- Drop view
DROP VIEW IF EXISTS employee_info;

-- Show all views
SHOW FULL TABLES WHERE Table_type = 'VIEW';

-- Show view definition
SHOW CREATE VIEW employee_info;

-- ========================================
-- 9. MATERIALIZED VIEW (PostgreSQL)
-- ========================================

-- Create materialized view
CREATE MATERIALIZED VIEW monthly_sales AS
SELECT 
    region,
    DATE_TRUNC('month', order_date) AS month,
    SUM(amount) AS total_sales,
    COUNT(*) AS order_count,
    AVG(amount) AS avg_order_value
FROM orders
GROUP BY region, DATE_TRUNC('month', order_date);

-- Query materialized view (fast!)
SELECT * FROM monthly_sales
WHERE month = '2025-10-01'
ORDER BY total_sales DESC;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW monthly_sales;

-- Concurrent refresh (allows queries during refresh)
REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales;

-- ========================================
-- 10. MATERIALIZED VIEW - PRODUCT ANALYTICS
-- ========================================

CREATE MATERIALIZED VIEW product_analytics AS
SELECT 
    p.product_id,
    p.product_name,
    p.category,
    COUNT(DISTINCT o.customer_id) AS unique_customers,
    SUM(oi.quantity) AS total_units_sold,
    SUM(oi.quantity * oi.unit_price) AS total_revenue,
    AVG(r.rating) AS avg_rating,
    COUNT(r.review_id) AS review_count
FROM products p
LEFT JOIN order_items oi ON p.product_id = oi.product_id
LEFT JOIN orders o ON oi.order_id = o.order_id
LEFT JOIN reviews r ON p.product_id = r.product_id
GROUP BY p.product_id, p.product_name, p.category;

-- Fast analytics queries
SELECT * FROM product_analytics 
WHERE total_revenue > 100000 
ORDER BY avg_rating DESC;

-- ========================================
-- 11. MATERIALIZED VIEW - SALES DASHBOARD
-- ========================================

CREATE MATERIALIZED VIEW revenue_summary AS
SELECT 
    region,
    DATE_TRUNC('month', sale_date) AS month,
    SUM(total_amount) AS monthly_revenue,
    COUNT(*) AS transaction_count,
    AVG(total_amount) AS avg_transaction,
    MAX(total_amount) AS largest_sale,
    MIN(total_amount) AS smallest_sale
FROM sales
WHERE sale_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY region, DATE_TRUNC('month', sale_date);

-- Dashboard queries (instant results)
SELECT region, monthly_revenue
FROM revenue_summary
WHERE month = DATE_TRUNC('month', CURRENT_DATE)
ORDER BY monthly_revenue DESC;

-- Year-over-year comparison
SELECT 
    region,
    SUM(CASE WHEN month >= CURRENT_DATE - INTERVAL '1 year' 
             THEN monthly_revenue ELSE 0 END) AS current_year,
    SUM(CASE WHEN month < CURRENT_DATE - INTERVAL '1 year' 
             THEN monthly_revenue ELSE 0 END) AS previous_year
FROM revenue_summary
GROUP BY region;

-- ========================================
-- 12. ROW-LEVEL SECURITY WITH VIEWS
-- ========================================

-- Sales reps see only their own region
CREATE VIEW my_sales AS
SELECT * FROM sales
WHERE region = (SELECT region FROM employees WHERE emp_id = CURRENT_USER());

-- Managers see all regions
CREATE VIEW all_sales AS
SELECT * FROM sales;

-- Grant appropriate access
GRANT SELECT ON my_sales TO sales_rep_role;
GRANT SELECT ON all_sales TO manager_role;

-- ========================================
-- 13. VIEW WITH FILTERING CONDITIONS
-- ========================================

-- Active employees only
CREATE VIEW active_employees AS
SELECT * FROM employees
WHERE status = 'active' AND termination_date IS NULL;

-- Recent orders (last 90 days)
CREATE VIEW recent_orders AS
SELECT * FROM orders
WHERE order_date >= CURRENT_DATE - INTERVAL '90 days';

-- High-value customers
CREATE VIEW vip_customers AS
SELECT 
    customer_id,
    customer_name,
    SUM(order_total) AS total_spent
FROM orders
GROUP BY customer_id, customer_name
HAVING SUM(order_total) > 10000;

-- ========================================
-- 14. INDEXING MATERIALIZED VIEWS
-- ========================================

-- Create materialized view
CREATE MATERIALIZED VIEW customer_summary AS
SELECT 
    customer_id,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_spent,
    MAX(order_date) AS last_order_date
FROM orders
GROUP BY customer_id;

-- Create indexes for better performance
CREATE INDEX idx_customer_summary_spent ON customer_summary(total_spent);
CREATE INDEX idx_customer_summary_count ON customer_summary(order_count);

-- Fast queries on indexed columns
SELECT * FROM customer_summary WHERE total_spent > 5000;

-- ========================================
-- 15. DROP MATERIALIZED VIEW
-- ========================================

DROP MATERIALIZED VIEW IF EXISTS monthly_sales;
DROP MATERIALIZED VIEW IF EXISTS product_analytics;

-- ========================================
-- 16. VIEW METADATA QUERIES
-- ========================================

-- List all views
SELECT table_name, table_type
FROM information_schema.tables
WHERE table_schema = 'mydb' AND table_type = 'VIEW';

-- View definition
SELECT view_definition
FROM information_schema.views
WHERE table_name = 'employee_info';

-- Check view dependencies
SELECT 
    table_name AS view_name,
    view_definition
FROM information_schema.views
WHERE view_definition LIKE '%employees%';

-- ========================================
-- 17. COMPLETE EXAMPLE: E-COMMERCE VIEWS
-- ========================================

-- Customer order history view
CREATE VIEW customer_order_history AS
SELECT 
    c.customer_id,
    c.customer_name,
    o.order_id,
    o.order_date,
    o.status,
    COUNT(oi.order_item_id) AS items_count,
    SUM(oi.quantity * oi.unit_price) AS order_total
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
JOIN order_items oi ON o.order_id = oi.order_id
GROUP BY c.customer_id, c.customer_name, o.order_id, o.order_date, o.status;

-- Product inventory view
CREATE VIEW product_inventory_status AS
SELECT 
    p.product_id,
    p.product_name,
    p.quantity_in_stock,
    COALESCE(SUM(oi.quantity), 0) AS pending_orders,
    p.quantity_in_stock - COALESCE(SUM(oi.quantity), 0) AS available_stock,
    CASE 
        WHEN p.quantity_in_stock - COALESCE(SUM(oi.quantity), 0) <= 10 THEN 'Low Stock'
        WHEN p.quantity_in_stock - COALESCE(SUM(oi.quantity), 0) <= 50 THEN 'Medium Stock'
        ELSE 'High Stock'
    END AS stock_status
FROM products p
LEFT JOIN order_items oi ON p.product_id = oi.product_id
LEFT JOIN orders o ON oi.order_id = o.order_id AND o.status = 'pending'
GROUP BY p.product_id, p.product_name, p.quantity_in_stock;
''',
    revisionPoints: [
      'Views are virtual tables created from SQL queries that do not store data physically',
      'Materialized Views store query results physically on disk for faster access',
      'Views always show current data; materialized views may be stale until refreshed',
      'Views simplify complex queries by hiding JOINs and calculations behind simple SELECT statements',
      'Views enhance security by hiding sensitive columns from specific users',
      'Updatable views must be based on single table without aggregations, DISTINCT, or GROUP BY',
      'CREATE OR REPLACE VIEW modifies existing view definition without dropping it',
      'DROP VIEW IF EXISTS safely removes views without error if they do not exist',
      'Materialized views improve performance 10-100x for complex aggregations and reports',
      'REFRESH MATERIALIZED VIEW updates cached data (CONCURRENTLY allows queries during refresh)',
      'Materialized views require extra disk space to store cached results',
      'Use views for: security, query simplification, backward compatibility, standardized access',
      'Use materialized views for: dashboards, analytics, pre-aggregated reports, historical snapshots',
      'Views cannot be updated if they contain: GROUP BY, aggregates (SUM, AVG), DISTINCT, UNION, or multiple tables',
      'Best practice: Prefix views with v_ and materialized views with mv_ for clarity',
      'Grant permissions on views, not base tables, to control data access (row and column security)',
      'Refresh strategies: high volatility data (hourly/daily), low volatility (weekly/monthly)',
      'Index materialized views for better query performance on filtered columns',
      'Regular views have no storage overhead; materialized views trade storage for speed',
      'Views provide data abstraction layer between applications and database schema',
      'Materialized views reduce database load by avoiding repeated execution of complex queries',
      'Use ROW-LEVEL security in views to filter data by user role or region',
      'SHOW FULL TABLES WHERE Table_type = VIEW lists all views in database',
      'Materialized views are ideal for data warehouse and OLAP queries',
      'Final principle: Views bring clarity, Materialized Views bring speed',
    ],
    quizQuestions: [
      Question(
        question: 'What is the main difference between views and materialized views?',
        options: [
          'Views store data, materialized views don\'t',
          'Materialized views store data physically, views don\'t',
          'No difference',
          'Views are faster'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Can you update data through a view that uses GROUP BY?',
        options: ['Yes, always', 'No, views with GROUP BY are not updatable', 'Only in PostgreSQL', 'Only if using DISTINCT'],
        correctIndex: 1,
      ),
      Question(
        question: 'What command refreshes a PostgreSQL materialized view?',
        options: [
          'UPDATE MATERIALIZED VIEW',
          'REFRESH MATERIALIZED VIEW',
          'RELOAD MATERIALIZED VIEW',
          'REBUILD MATERIALIZED VIEW'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is a primary advantage of regular views?',
        options: [
          'Faster query performance',
          'Always shows current data',
          'Uses less CPU',
          'Requires no refresh'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a key use case for materialized views?',
        options: [
          'Real-time transaction processing',
          'Complex analytics and dashboard queries',
          'Hiding sensitive columns',
          'Simplifying simple SELECT queries'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'How do you modify an existing view without dropping it?',
        options: [
          'ALTER VIEW',
          'UPDATE VIEW',
          'CREATE OR REPLACE VIEW',
          'MODIFY VIEW'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a disadvantage of materialized views?',
        options: [
          'Always shows outdated data',
          'Cannot be queried',
          'Requires extra storage and may have stale data',
          'Slower than regular views'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which view type is best for hiding sensitive columns?',
        options: [
          'Materialized View',
          'Indexed View',
          'Regular View',
          'Temporary View'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Can a view be based on multiple tables using JOINs?',
        options: ['Yes', 'No', 'Only in Oracle', 'Only for materialized views'],
        correctIndex: 0,
      ),
      Question(
        question: 'What is the performance improvement range for materialized views?',
        options: ['2-5x faster', '10-100x faster', '1000x faster', 'No improvement'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which command shows all views in a MySQL database?',
        options: [
          'LIST VIEWS',
          'SHOW VIEWS',
          'SHOW FULL TABLES WHERE Table_type = VIEW',
          'SELECT * FROM views'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'What is a best practice for naming materialized views?',
        options: [
          'No specific convention',
          'Prefix with mv_',
          'Suffix with _view',
          'Use ALL CAPS'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'data_integrity',
    title: '17. Data Integrity & Constraints',
    explanation: '''## Data Integrity & Constraints

### 1. Introduction

#### What is Data Integrity?

**Definition:**

**Data Integrity** ensures that data in a database is **accurate, consistent, and reliable** throughout its life cycle. It prevents invalid, incomplete, or conflicting data from entering the system.

**Key Characteristics:**
- **Accuracy:** Data is correct and error-free
- **Consistency:** Data remains uniform across tables
- **Reliability:** Data can be trusted for decisions
- **Validity:** Data meets defined rules and constraints
- **Protection:** Guards against corruption and unauthorized changes

**Analogy:**
> 💡 Think of Data Integrity as the *immune system* of your database — it keeps data clean and trustworthy, just like your immune system keeps your body healthy by fighting infections.

**Why It Matters:**

✅ **Ensures accuracy** of data (no duplicates or wrong entries)
✅ **Maintains consistency** across related tables
✅ **Prevents accidental deletion or corruption**
✅ **Improves data quality and reliability** for analytics
✅ **Builds trust** in business intelligence and reporting
✅ **Reduces debugging time** by catching errors at insertion

**Without Data Integrity:**
- ❌ Orders without valid customers
- ❌ Students enrolled in non-existent courses
- ❌ Negative account balances in banking systems
- ❌ Duplicate user registrations
- ❌ Orphaned records with broken relationships

---

### 2. Types of Data Integrity

#### Overview Table

| Type                       | Description                                            | Example                                                | Constraint Type     |
| -------------------------- | ------------------------------------------------------ | ------------------------------------------------------ | ------------------- |
| **Entity Integrity**       | Each table must have a unique identifier (Primary Key) | student_id in students table                           | PRIMARY KEY         |
| **Referential Integrity**  | Relationships between tables must be valid             | Foreign key in orders references valid customer_id     | FOREIGN KEY         |
| **Domain Integrity**       | Data values must be valid and within specific range    | Age must be between 0-120, status IN ('active', 'inactive') | CHECK, NOT NULL, DEFAULT |
| **User-Defined Integrity** | Custom rules defined by the user                       | Salary cannot exceed manager's salary, start_date < end_date | CHECK, TRIGGER      |

---

#### Entity Integrity

**Definition:**
Each row in a table must be **uniquely identifiable** using a **Primary Key**. No two rows can have the same primary key, and it cannot be NULL.

**Rules:**
- ✅ Primary Key must be UNIQUE
- ✅ Primary Key cannot be NULL
- ✅ Each table should have exactly one Primary Key
- ✅ Can be single column or composite (multiple columns)

**Example:**
```sql
CREATE TABLE students (
    student_id INT PRIMARY KEY,  -- Unique identifier
    name VARCHAR(50),
    email VARCHAR(100)
);
```

**Violation Example:**
```sql
-- ❌ Error: Duplicate primary key
INSERT INTO students VALUES (1, 'Alex', 'alex@test.com');
INSERT INTO students VALUES (1, 'Maya', 'maya@test.com');  -- FAILS

-- ❌ Error: NULL primary key
INSERT INTO students VALUES (NULL, 'John', 'john@test.com');  -- FAILS
```

---

#### Referential Integrity

**Definition:**
Ensures that **relationships between tables remain valid**. A foreign key in one table must reference an existing primary key in another table.

**Rules:**
- ✅ Foreign Key must reference existing Primary Key
- ✅ Prevents orphaned records (child without parent)
- ✅ Maintains parent-child relationships
- ✅ Can be NULL (optional relationship)

**Example:**
```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);
```

**Violation Example:**
```sql
-- ❌ Error: customer_id = 999 doesn't exist in customers table
INSERT INTO orders VALUES (1, 999, 100.00);  -- FAILS
```

---

#### Domain Integrity

**Definition:**
Ensures that column values are **valid and within acceptable range** based on data type, format, and business rules.

**Enforced By:**
- ✅ Data types (INT, VARCHAR, DATE)
- ✅ NOT NULL constraint
- ✅ CHECK constraint
- ✅ DEFAULT values
- ✅ ENUM/SET types

**Example:**
```sql
CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    age INT CHECK (age >= 18 AND age <= 65),
    salary DECIMAL(10,2) CHECK (salary > 0),
    status ENUM('active', 'inactive', 'suspended')
);
```

---

#### User-Defined Integrity

**Definition:**
Custom business rules enforced through **CHECK constraints** or **triggers** that are specific to the application.

**Example:**
```sql
-- Salary cannot exceed manager's salary
CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    emp_name VARCHAR(50),
    salary DECIMAL(10,2),
    manager_id INT,
    CHECK (salary <= (SELECT salary FROM employees WHERE emp_id = manager_id))
);

-- Start date must be before end date
CREATE TABLE projects (
    project_id INT PRIMARY KEY,
    start_date DATE,
    end_date DATE,
    CHECK (start_date < end_date)
);
```

---

### 3. Constraints Overview

**Definition:**
Constraints are **rules applied on table columns** to enforce data integrity automatically at the database level.

**Benefits:**
- ✅ **Automatic validation** (no app code needed)
- ✅ **Centralized enforcement** (all apps follow same rules)
- ✅ **Performance** (database-level checking is faster)
- ✅ **Data protection** (even direct SQL can't violate rules)
- ✅ **Self-documenting** (constraints explain data rules)

---

#### Types of Constraints

| Constraint      | Description                                    | Example Usage                              |
| --------------- | ---------------------------------------------- | ------------------------------------------ |
| **PRIMARY KEY** | Ensures unique and non-null identification     | student_id, order_id, emp_id               |
| **FOREIGN KEY** | Maintains referential integrity                | customer_id references customers table     |
| **UNIQUE**      | Prevents duplicate values in a column          | email, phone, username                     |
| **NOT NULL**    | Ensures a column cannot store NULL values      | name, email, required fields               |
| **CHECK**       | Restricts values based on a condition          | age >= 18, salary > 0, status IN (...)     |
| **DEFAULT**     | Provides a default value when none is supplied | created_at = CURRENT_TIMESTAMP, status = 'active' |

---

### 4. Creating Constraints in SQL

#### Example Table with All Constraints

```sql
CREATE TABLE students (
    student_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    age INT CHECK (age >= 5 AND age <= 25),
    grade CHAR(1) DEFAULT 'A',
    email VARCHAR(100) UNIQUE,
    enrollment_date DATE DEFAULT CURRENT_DATE
);
```

#### Explanation

**PRIMARY KEY:**
- ✅ `student_id` uniquely identifies each student
- ✅ Cannot be NULL or duplicate
- ✅ Automatically creates an index for fast lookups

**NOT NULL:**
- ✅ `name` cannot be left empty
- ✅ Forces users to provide a value
- ✅ Prevents incomplete records

**CHECK:**
- ✅ Restricts `age` range between 5 and 25
- ✅ Validates data before insertion
- ✅ Rejects invalid values automatically

**DEFAULT:**
- ✅ Assigns grade 'A' by default
- ✅ Assigns current date to enrollment_date
- ✅ Reduces NULL values and simplifies inserts

**UNIQUE:**
- ✅ Ensures each email address is different
- ✅ Allows NULL values (unlike PRIMARY KEY)
- ✅ Prevents duplicate registrations

---

#### Test Insertions

```sql
-- ✅ Valid insertion
INSERT INTO students (student_id, name, age, email)
VALUES (1, 'Alex', 20, 'alex@school.com');
-- grade becomes 'A' (default), enrollment_date = today

-- ❌ Error: age out of range
INSERT INTO students VALUES (2, 'Maya', 30, 'B', 'maya@school.com');
-- CHECK constraint failed: age must be between 5 and 25

-- ❌ Error: duplicate email
INSERT INTO students VALUES (3, 'John', 18, 'A', 'alex@school.com');
-- UNIQUE constraint failed: email already exists

-- ❌ Error: name is NULL
INSERT INTO students (student_id, age, email)
VALUES (4, 19, 'john@school.com');
-- NOT NULL constraint failed: name cannot be NULL
```

---

### 5. Referential Integrity with FOREIGN KEY

**Definition:**
FOREIGN KEY establishes and enforces relationships between tables, ensuring that referenced data exists.

#### Example: Students and Courses

```sql
-- Parent table
CREATE TABLE courses (
    course_id INT PRIMARY KEY,
    course_name VARCHAR(50) NOT NULL
);

-- Child table
CREATE TABLE enrollments (
    enrollment_id INT PRIMARY KEY,
    student_id INT,
    course_id INT,
    enrollment_date DATE DEFAULT CURRENT_DATE,
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id)
);
```

#### Explanation

**FOREIGN KEY Behavior:**
- ✅ `student_id` in enrollments **must exist** in students table
- ✅ `course_id` in enrollments **must exist** in courses table
- ✅ Prevents insertion of records that violate relationships
- ✅ Maintains parent-child data consistency

**Real-World Analogy:**
> 💡 You can't enroll a student who doesn't exist in the students table, just like you can't assign a book to a non-existent library member.

---

#### Test Referential Integrity

```sql
-- Insert parent records first
INSERT INTO students (student_id, name, age, email)
VALUES (101, 'Alex', 20, 'alex@school.com');

INSERT INTO courses (course_id, course_name)
VALUES (1, 'Database Systems');

-- ✅ Valid enrollment (both student and course exist)
INSERT INTO enrollments (enrollment_id, student_id, course_id)
VALUES (1, 101, 1);

-- ❌ Error: student_id 999 doesn't exist
INSERT INTO enrollments (enrollment_id, student_id, course_id)
VALUES (2, 999, 1);
-- FOREIGN KEY constraint failed

-- ❌ Error: course_id 999 doesn't exist
INSERT INTO enrollments (enrollment_id, student_id, course_id)
VALUES (3, 101, 999);
-- FOREIGN KEY constraint failed
```

---

### 6. Cascade Actions in Foreign Keys

**Definition:**
Cascade actions automatically propagate changes from parent table to child tables, maintaining referential integrity.

#### Cascade Options Table

| Action                 | Description                                     | Use Case                                   |
| ---------------------- | ----------------------------------------------- | ------------------------------------------ |
| **ON DELETE CASCADE**  | Deletes dependent rows automatically            | Delete customer → delete all their orders  |
| **ON UPDATE CASCADE**  | Updates child table when parent changes         | Update customer_id → update in all orders  |
| **ON DELETE SET NULL** | Sets foreign key to NULL when parent is deleted | Delete department → employee.dept_id = NULL |
| **ON DELETE RESTRICT** | Prevents deletion if child records exist        | Can't delete customer with existing orders |
| **ON DELETE NO ACTION**| Same as RESTRICT (default behavior)             | Prevents accidental cascading deletes      |

---

#### Example: ON DELETE CASCADE

```sql
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL
);

CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE,
    FOREIGN KEY (customer_id)
        REFERENCES customers(customer_id)
        ON DELETE CASCADE
);
```

**Behavior:**
```sql
-- Insert data
INSERT INTO customers VALUES (1, 'Alex');
INSERT INTO orders VALUES (101, 1, '2025-01-01');
INSERT INTO orders VALUES (102, 1, '2025-01-15');

-- Delete customer
DELETE FROM customers WHERE customer_id = 1;

-- ✅ All orders with customer_id = 1 are automatically deleted
SELECT * FROM orders;  -- Empty result
```

**Warning:**
> ⚠️ Use CASCADE carefully! Deleting one customer could delete thousands of orders.

---

#### Example: ON DELETE SET NULL

```sql
CREATE TABLE departments (
    dept_id INT PRIMARY KEY,
    dept_name VARCHAR(50)
);

CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    emp_name VARCHAR(50),
    dept_id INT,
    FOREIGN KEY (dept_id)
        REFERENCES departments(dept_id)
        ON DELETE SET NULL
);
```

**Behavior:**
```sql
-- Insert data
INSERT INTO departments VALUES (10, 'HR');
INSERT INTO employees VALUES (1, 'Alex', 10);

-- Delete department
DELETE FROM departments WHERE dept_id = 10;

-- ✅ Employee record remains, but dept_id becomes NULL
SELECT * FROM employees WHERE emp_id = 1;
-- Result: (1, 'Alex', NULL)
```

---

### 7. Example: Combining Multiple Constraints

```sql
CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    emp_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    salary DECIMAL(10,2) CHECK (salary > 0),
    hire_date DATE DEFAULT CURRENT_DATE,
    department_id INT,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    FOREIGN KEY (department_id) REFERENCES departments(dept_id)
        ON DELETE SET NULL
);
```

#### Enforced Rules:

✅ **Entity Integrity:** `emp_id` is PRIMARY KEY (unique, non-null)
✅ **Domain Integrity:** 
  - `emp_name` cannot be NULL
  - `email` must be unique
  - `salary` must be positive
  - `status` must be one of three valid values
✅ **Referential Integrity:** `department_id` must exist in departments table
✅ **Default Values:** `hire_date` = today, `status` = 'active'

---

### 8. Adding Constraints After Table Creation

#### Syntax: ALTER TABLE

```sql
-- Add CHECK constraint
ALTER TABLE students
ADD CONSTRAINT chk_age CHECK (age BETWEEN 5 AND 25);

-- Add UNIQUE constraint
ALTER TABLE students
ADD CONSTRAINT unique_email UNIQUE (email);

-- Add FOREIGN KEY constraint
ALTER TABLE students
ADD CONSTRAINT fk_course FOREIGN KEY (course_id)
    REFERENCES courses(course_id)
    ON DELETE CASCADE;

-- Add NOT NULL constraint
ALTER TABLE students
MODIFY name VARCHAR(50) NOT NULL;

-- Add DEFAULT constraint
ALTER TABLE students
ALTER COLUMN grade SET DEFAULT 'A';
```

#### When to Use ALTER TABLE:

✅ **Existing tables** that need constraints added
✅ **Legacy databases** being migrated to better practices
✅ **Testing** constraints before making them permanent
✅ **Gradual enforcement** to avoid breaking existing apps

---

### 9. Violating Constraints

#### Example Violations:

**PRIMARY KEY Violation:**
```sql
INSERT INTO students VALUES (1, 'Alex', 20, 'A', 'alex@test.com');
INSERT INTO students VALUES (1, 'Maya', 19, 'B', 'maya@test.com');
-- ❌ Error: Duplicate entry '1' for key 'PRIMARY'
```

**CHECK Constraint Violation:**
```sql
INSERT INTO students (student_id, name, age) VALUES (2, 'John', 2);
-- ❌ Error: CHECK constraint failed: age must be >= 5 and <= 25
```

**NOT NULL Violation:**
```sql
INSERT INTO students (student_id, age, grade) VALUES (3, 20, 'A');
-- ❌ Error: Column 'name' cannot be null
```

**FOREIGN KEY Violation:**
```sql
INSERT INTO enrollments (enrollment_id, student_id, course_id)
VALUES (1, 999, 1);
-- ❌ Error: Cannot add or update: foreign key constraint fails
```

**UNIQUE Constraint Violation:**
```sql
INSERT INTO students VALUES (4, 'Sarah', 21, 'A', 'alex@test.com');
-- ❌ Error: Duplicate entry 'alex@test.com' for key 'unique_email'
```

**Benefits of Constraint Errors:**
> ✅ Constraints protect your database from invalid data, even if the application code has bugs or is bypassed through direct SQL.

---

### 10. Disabling & Dropping Constraints

#### Drop Constraint

```sql
-- Drop CHECK constraint
ALTER TABLE students DROP CONSTRAINT chk_age;

-- Drop FOREIGN KEY constraint
ALTER TABLE enrollments DROP FOREIGN KEY fk_student;

-- Drop UNIQUE constraint
ALTER TABLE students DROP INDEX unique_email;
```

#### Disable/Enable (MySQL/Oracle)

```sql
-- Disable constraint (Oracle)
ALTER TABLE students DISABLE CONSTRAINT chk_age;

-- Enable constraint (Oracle)
ALTER TABLE students ENABLE CONSTRAINT chk_age;

-- MySQL: Must drop and recreate
```

**Warning:**
> ⚠️ Use carefully — dropping constraints can lead to inconsistent or invalid data. Always backup before modifying constraints in production.

---

### 11. CHECK Constraint Examples

#### Banking Account Constraints

```sql
CREATE TABLE accounts (
    acc_id INT PRIMARY KEY,
    acc_number VARCHAR(20) UNIQUE NOT NULL,
    balance DECIMAL(10,2) CHECK (balance >= 0),
    acc_type VARCHAR(10) CHECK (acc_type IN ('SAVINGS', 'CURRENT', 'FIXED')),
    interest_rate DECIMAL(5,2) CHECK (interest_rate >= 0 AND interest_rate <= 15),
    opened_date DATE DEFAULT CURRENT_DATE,
    status ENUM('active', 'closed', 'frozen') DEFAULT 'active'
);
```

#### Enforced Rules:

✅ **No negative balances** (prevents overdrafts without approval)
✅ **Only valid account types** (SAVINGS, CURRENT, FIXED)
✅ **Interest rate range** (0% to 15%)
✅ **Valid status** (active, closed, frozen)

---

#### E-Commerce Product Constraints

```sql
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) CHECK (price > 0),
    discount_pct DECIMAL(5,2) CHECK (discount_pct >= 0 AND discount_pct <= 100),
    stock INT CHECK (stock >= 0),
    rating DECIMAL(3,2) CHECK (rating >= 0 AND rating <= 5),
    category ENUM('electronics', 'clothing', 'food', 'books')
);
```

#### Enforced Rules:

✅ **Positive prices** (no free or negative prices)
✅ **Valid discount range** (0% to 100%)
✅ **Non-negative stock** (inventory cannot be negative)
✅ **Rating range** (0 to 5 stars)
✅ **Valid categories** (predefined list)

---

### 12. DEFAULT Constraint Example

#### Products Table

```sql
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(50) NOT NULL,
    stock INT DEFAULT 100,
    status VARCHAR(20) DEFAULT 'available',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

#### Automatic Value Assignment:

```sql
-- Insert without providing defaults
INSERT INTO products (product_id, product_name)
VALUES (1, 'Laptop');

-- Automatic assignment:
-- stock = 100
-- status = 'available'
-- created_at = current timestamp
-- updated_at = current timestamp

SELECT * FROM products WHERE product_id = 1;
```

**Output:**

| product_id | product_name | stock | status    | created_at          | updated_at          |
| ---------- | ------------ | ----- | --------- | ------------------- | ------------------- |
| 1          | Laptop       | 100   | available | 2025-10-18 10:30:00 | 2025-10-18 10:30:00 |

---

#### User Registration Defaults

```sql
CREATE TABLE users (
    user_id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    role VARCHAR(20) DEFAULT 'user',
    credits INT DEFAULT 0,
    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP NULL
);
```

**Benefits:**
- ✅ New users automatically get 'active' status
- ✅ Default role is 'user' (non-admin)
- ✅ Credits start at 0
- ✅ Registration time captured automatically

---

### 13. Importance of Data Integrity in Real-World Systems

#### Industry Examples

| System              | Example of Integrity                       | Impact if Violated                         |
| ------------------- | ------------------------------------------ | ------------------------------------------ |
| **Banking**         | Prevent withdrawal > balance               | Financial loss, fraud, legal issues        |
| **E-commerce**      | Prevent order without a valid product      | Broken orders, customer complaints         |
| **University DB**   | Ensure student_id exists before enrollment | Orphaned records, reporting errors         |
| **Healthcare**      | Ensure patient's age & ID are valid        | Medical errors, legal liability            |
| **Government Data** | Prevent duplicate identity registration    | Fraud, security breaches                   |
| **Flight Booking**  | Prevent overbooking beyond seat capacity   | Angry customers, compensation costs        |
| **Payroll System**  | Salary > 0, employee exists                | Payment errors, financial audits fail      |

---

#### Banking Example

```sql
-- Prevent negative balance
CREATE TABLE bank_accounts (
    account_id INT PRIMARY KEY,
    balance DECIMAL(10,2) CHECK (balance >= 0),
    overdraft_limit DECIMAL(10,2) DEFAULT 0
);

-- Prevent withdrawal exceeding balance + overdraft
-- (enforced by trigger or application logic + CHECK constraint)
```

**Without Integrity:**
- ❌ Customer withdraws more than available balance
- ❌ Account goes negative without authorization
- ❌ Bank loses money

**With Integrity:**
- ✅ Database rejects invalid withdrawals
- ✅ Application logic is backed by database rules
- ✅ Even if app has bug, database protects data

---

### 14. Data Integrity Best Practices

#### Golden Rules

**1. Always define PRIMARY KEYS for each table**
```sql
-- ✅ Good
CREATE TABLE orders (order_id INT PRIMARY KEY, ...);

-- ❌ Bad: No primary key
CREATE TABLE orders (order_id INT, ...);
```

**2. Use FOREIGN KEYS to maintain relationships**
```sql
-- ✅ Good: Enforces referential integrity
FOREIGN KEY (customer_id) REFERENCES customers(customer_id)

-- ❌ Bad: No relationship enforcement
-- customer_id INT (just a number, no validation)
```

**3. Enforce NOT NULL on mandatory fields**
```sql
-- ✅ Good
CREATE TABLE users (
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100) NOT NULL
);

-- ❌ Bad: Allows incomplete records
CREATE TABLE users (username VARCHAR(50), email VARCHAR(100));
```

**4. Add CHECK constraints to prevent invalid values**
```sql
-- ✅ Good
age INT CHECK (age >= 0 AND age <= 120),
status ENUM('active', 'inactive')

-- ❌ Bad: No validation
age INT,
status VARCHAR(20)
```

**5. Use DEFAULT values to handle missing data**
```sql
-- ✅ Good
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
status VARCHAR(20) DEFAULT 'active'

-- ❌ Bad: NULL values everywhere
created_at TIMESTAMP,
status VARCHAR(20)
```

**6. Apply CASCADE actions carefully to avoid data loss**
```sql
-- ✅ Good: Use ON DELETE SET NULL for optional relationships
FOREIGN KEY (manager_id) REFERENCES employees(emp_id)
    ON DELETE SET NULL

-- ⚠️ Careful: ON DELETE CASCADE can delete many records
FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
    ON DELETE CASCADE
```

**7. Regularly validate data consistency using scripts or audits**
```sql
-- Find orphaned records (violated referential integrity)
SELECT * FROM orders o
WHERE NOT EXISTS (SELECT 1 FROM customers c WHERE c.customer_id = o.customer_id);

-- Find invalid domain values (violated CHECK constraints)
SELECT * FROM employees WHERE salary < 0;
```

---

#### Naming Conventions

**Constraint Names:**
- **Primary Key:** `pk_tablename`
- **Foreign Key:** `fk_tablename_columnname`
- **Unique:** `uq_tablename_columnname`
- **Check:** `chk_tablename_columnname`

**Example:**
```sql
CREATE TABLE employees (
    emp_id INT,
    email VARCHAR(100),
    dept_id INT,
    salary DECIMAL(10,2),
    CONSTRAINT pk_employees PRIMARY KEY (emp_id),
    CONSTRAINT uq_employees_email UNIQUE (email),
    CONSTRAINT fk_employees_dept_id FOREIGN KEY (dept_id) REFERENCES departments(dept_id),
    CONSTRAINT chk_employees_salary CHECK (salary > 0)
);
```

---

### 15. Example: Full Integrity Implementation

#### Complete E-Commerce Database

```sql
-- Customers table
CREATE TABLE customers (
    customer_id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    phone CHAR(10) CHECK (phone REGEXP '^[0-9]{10}\$'),
    status ENUM('active', 'inactive') DEFAULT 'active',
    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Products table
CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    product_name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) CHECK (price > 0),
    stock INT CHECK (stock >= 0) DEFAULT 0,
    category VARCHAR(50) NOT NULL
);

-- Orders table
CREATE TABLE orders (
    order_id INT PRIMARY KEY AUTO_INCREMENT,
    customer_id INT NOT NULL,
    order_date DATE DEFAULT CURRENT_DATE,
    total_amount DECIMAL(10,2) CHECK (total_amount >= 0),
    status ENUM('pending', 'shipped', 'delivered', 'cancelled') DEFAULT 'pending',
    FOREIGN KEY (customer_id)
        REFERENCES customers(customer_id)
        ON DELETE CASCADE
);

-- Order Items table
CREATE TABLE order_items (
    item_id INT PRIMARY KEY AUTO_INCREMENT,
    order_id INT NOT NULL,
    product_id INT NOT NULL,
    quantity INT CHECK (quantity > 0),
    unit_price DECIMAL(10,2) CHECK (unit_price > 0),
    FOREIGN KEY (order_id)
        REFERENCES orders(order_id)
        ON DELETE CASCADE,
    FOREIGN KEY (product_id)
        REFERENCES products(product_id)
        ON DELETE RESTRICT
);
```

#### Enforced Integrity Rules:

✅ **Entity Integrity:**
  - All tables have PRIMARY KEYs with AUTO_INCREMENT
  - Unique identification for every record

✅ **Referential Integrity:**
  - Orders must have valid customer_id
  - Order items must have valid order_id and product_id
  - CASCADE delete: Delete customer → delete their orders → delete order items
  - RESTRICT delete: Cannot delete product if it's in an order

✅ **Domain Integrity:**
  - Valid phone number format (10 digits)
  - Positive prices and quantities
  - Non-negative stock and order amounts
  - Valid status values (ENUM)

✅ **Default Values:**
  - New customers are 'active'
  - Order date defaults to today
  - Orders start as 'pending'

---

### 16. SQL Query to View Constraints

#### MySQL

```sql
-- View all constraints for a table
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    CONSTRAINT_TYPE
FROM information_schema.TABLE_CONSTRAINTS
WHERE TABLE_SCHEMA = 'your_database_name'
    AND TABLE_NAME = 'customers';

-- View foreign key relationships
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    COLUMN_NAME,
    REFERENCED_TABLE_NAME,
    REFERENCED_COLUMN_NAME
FROM information_schema.KEY_COLUMN_USAGE
WHERE TABLE_SCHEMA = 'your_database_name'
    AND REFERENCED_TABLE_NAME IS NOT NULL;

-- View CHECK constraints
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    CHECK_CLAUSE
FROM information_schema.CHECK_CONSTRAINTS
WHERE CONSTRAINT_SCHEMA = 'your_database_name';
```

#### PostgreSQL

```sql
-- View all constraints
SELECT 
    conname AS constraint_name,
    contype AS constraint_type,
    conrelid::regclass AS table_name
FROM pg_constraint
WHERE connamespace = 'public'::regnamespace;

-- Constraint types:
-- 'p' = PRIMARY KEY
-- 'f' = FOREIGN KEY
-- 'u' = UNIQUE
-- 'c' = CHECK
```

**Benefits:**
> ✅ Helps in auditing, debugging, and documentation of database constraints.

---

### 17. Common Interview Questions

**1. Difference between PRIMARY KEY and UNIQUE Key?**

| Feature        | PRIMARY KEY               | UNIQUE                      |
| -------------- | ------------------------- | --------------------------- |
| NULL allowed   | No (NOT NULL required)    | Yes (can have one NULL)     |
| Per table      | Only ONE                  | Multiple allowed            |
| Clustered      | Creates clustered index   | Creates non-clustered index |
| Purpose        | Entity identification     | Prevent duplicates          |

**Example:**
```sql
CREATE TABLE users (
    user_id INT PRIMARY KEY,        -- Only one PRIMARY KEY
    email VARCHAR(100) UNIQUE,      -- Can have NULL
    phone VARCHAR(15) UNIQUE,       -- Multiple UNIQUE allowed
    username VARCHAR(50) UNIQUE
);
```

---

**2. What is Referential Integrity?**

**Answer:** Referential Integrity ensures that relationships between tables remain valid. A FOREIGN KEY in one table must reference an existing PRIMARY KEY in another table, preventing orphaned records.

**Example:**
```sql
-- Can't create order for non-existent customer
FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
```

---

**3. Can we have multiple foreign keys in one table?**

**Answer:** Yes! A table can have multiple FOREIGN KEYs referencing different parent tables.

**Example:**
```sql
CREATE TABLE enrollments (
    enrollment_id INT PRIMARY KEY,
    student_id INT,
    course_id INT,
    instructor_id INT,
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id),
    FOREIGN KEY (instructor_id) REFERENCES instructors(instructor_id)
);
```

---

**4. When do you use ON DELETE SET NULL?**

**Answer:** Use ON DELETE SET NULL when the relationship is **optional** and you want to keep child records even if parent is deleted.

**Example:**
```sql
-- Keep employee record even if department is deleted
CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    dept_id INT,
    FOREIGN KEY (dept_id) REFERENCES departments(dept_id)
        ON DELETE SET NULL
);
```

**Use Cases:**
- ✅ Employee without department (freelancer, contractor)
- ✅ Product without category (uncategorized)
- ✅ Order without assigned delivery person

---

**5. Difference between View and Constraint in enforcing security?**

| Feature    | View                          | Constraint                   |
| ---------- | ----------------------------- | ---------------------------- |
| Purpose    | Control data visibility       | Control data validity        |
| Security   | Hide sensitive columns/rows   | Enforce data rules           |
| Example    | Hide salary column from users | Prevent negative salary      |
| Enforcement| Application/query level       | Database level               |
| Can violate?| Yes (direct table access)    | No (always enforced)         |

**Example:**
```sql
-- View: Security through visibility control
CREATE VIEW employee_public AS
SELECT emp_id, name, department FROM employees;
-- Hides: salary, ssn

-- Constraint: Security through data validation
CREATE TABLE employees (
    salary DECIMAL(10,2) CHECK (salary > 0)  -- Can't insert negative
);
```

---

### 18. Summary

#### Key Concepts Table

| Concept            | Purpose                                           | Constraint Type     |
| ------------------ | ------------------------------------------------- | ------------------- |
| **Data Integrity** | Ensures correctness, consistency, and reliability | All constraints     |
| **Primary Key**    | Unique identification of rows                     | PRIMARY KEY         |
| **Foreign Key**    | Maintains table relationships                     | FOREIGN KEY         |
| **CHECK**          | Restricts column values                           | CHECK               |
| **DEFAULT**        | Auto-assigns values                               | DEFAULT             |
| **NOT NULL**       | Prevents missing data                             | NOT NULL            |
| **UNIQUE**         | Prevents duplicates                               | UNIQUE              |
| **CASCADE**        | Maintains referential integrity automatically     | ON DELETE/UPDATE CASCADE |

---

#### Integrity Types

**Entity Integrity:** PRIMARY KEY (unique row identification)
**Referential Integrity:** FOREIGN KEY (valid relationships)
**Domain Integrity:** CHECK, NOT NULL, DEFAULT (valid values)
**User-Defined Integrity:** Custom CHECK constraints and triggers

---

#### Best Practices Checklist

✅ Define PRIMARY KEY for every table
✅ Use FOREIGN KEYs for all relationships
✅ Add NOT NULL to mandatory fields
✅ Use CHECK constraints for business rules
✅ Provide DEFAULT values where appropriate
✅ Use CASCADE carefully (test thoroughly)
✅ Name constraints descriptively
✅ Document integrity rules
✅ Regular audits to find violations
✅ Test constraints before production

---

### Final Thought

> **"Data without integrity is just noise."**
> 
> Constraints are not limitations — they are **guardrails** that keep your database consistent, secure, and meaningful. Just as traffic rules prevent accidents, database constraints prevent data corruption and maintain trust in your system.

**Remember:**
- 🛡️ **Integrity protects** your data from corruption
- 🎯 **Constraints enforce** rules automatically
- 🔒 **Security begins** with valid, consistent data
- 📊 **Quality data** leads to quality decisions
- ⚡ **Prevention** is better than correction
''',
    codeSnippet: '''
-- ========================================
-- 1. BASIC CONSTRAINTS EXAMPLE
-- ========================================

CREATE TABLE students (
    student_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    age INT CHECK (age >= 5 AND age <= 25),
    grade CHAR(1) DEFAULT 'A',
    email VARCHAR(100) UNIQUE,
    enrollment_date DATE DEFAULT CURRENT_DATE
);

-- Test insertions
INSERT INTO students (student_id, name, age, email)
VALUES (1, 'Alex', 20, 'alex@school.com');
-- grade becomes 'A' (default), enrollment_date = today

-- ❌ Error: age out of range
-- INSERT INTO students VALUES (2, 'Maya', 30, 'B', 'maya@school.com');

-- ❌ Error: duplicate email
-- INSERT INTO students VALUES (3, 'John', 18, 'A', 'alex@school.com');

-- ========================================
-- 2. REFERENTIAL INTEGRITY (FOREIGN KEY)
-- ========================================

-- Parent tables
CREATE TABLE courses (
    course_id INT PRIMARY KEY,
    course_name VARCHAR(50) NOT NULL
);

CREATE TABLE students (
    student_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE
);

-- Child table with foreign keys
CREATE TABLE enrollments (
    enrollment_id INT PRIMARY KEY,
    student_id INT,
    course_id INT,
    enrollment_date DATE DEFAULT CURRENT_DATE,
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id)
);

-- Valid insertion (references exist)
INSERT INTO students VALUES (101, 'Alex', 'alex@school.com');
INSERT INTO courses VALUES (1, 'Database Systems');
INSERT INTO enrollments VALUES (1, 101, 1, CURRENT_DATE);

-- ❌ Error: student_id 999 doesn't exist
-- INSERT INTO enrollments VALUES (2, 999, 1, CURRENT_DATE);

-- ========================================
-- 3. CASCADE ACTIONS
-- ========================================

-- ON DELETE CASCADE example
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE
);

CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE DEFAULT CURRENT_DATE,
    total_amount DECIMAL(10,2),
    FOREIGN KEY (customer_id)
        REFERENCES customers(customer_id)
        ON DELETE CASCADE
);

-- Insert data
INSERT INTO customers VALUES (1, 'Alex', 'alex@shop.com');
INSERT INTO orders VALUES (101, 1, '2025-01-01', 250.00);
INSERT INTO orders VALUES (102, 1, '2025-01-15', 180.00);

-- Delete customer (cascades to orders)
DELETE FROM customers WHERE customer_id = 1;
-- All orders with customer_id = 1 are automatically deleted

-- ========================================
-- 4. ON DELETE SET NULL
-- ========================================

CREATE TABLE departments (
    dept_id INT PRIMARY KEY,
    dept_name VARCHAR(50) NOT NULL
);

CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    emp_name VARCHAR(50) NOT NULL,
    salary DECIMAL(10,2) CHECK (salary > 0),
    dept_id INT,
    FOREIGN KEY (dept_id)
        REFERENCES departments(dept_id)
        ON DELETE SET NULL
);

-- Insert data
INSERT INTO departments VALUES (10, 'HR');
INSERT INTO employees VALUES (1, 'Alex', 50000, 10);

-- Delete department
DELETE FROM departments WHERE dept_id = 10;

-- Employee record remains, but dept_id becomes NULL
SELECT * FROM employees WHERE emp_id = 1;
-- Result: (1, 'Alex', 50000, NULL)

-- ========================================
-- 5. MULTIPLE CONSTRAINTS ON SINGLE TABLE
-- ========================================

CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    emp_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    salary DECIMAL(10,2) CHECK (salary > 0),
    hire_date DATE DEFAULT CURRENT_DATE,
    department_id INT,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    FOREIGN KEY (department_id) 
        REFERENCES departments(dept_id)
        ON DELETE SET NULL
);

-- Valid insertion
INSERT INTO departments VALUES (20, 'Engineering');
INSERT INTO employees (emp_id, emp_name, email, salary, department_id)
VALUES (1, 'Maya', 'maya@company.com', 60000, 20);
-- hire_date = today, status = 'active' (defaults applied)

-- ========================================
-- 6. CHECK CONSTRAINT EXAMPLES
-- ========================================

-- Banking account
CREATE TABLE accounts (
    acc_id INT PRIMARY KEY,
    acc_number VARCHAR(20) UNIQUE NOT NULL,
    balance DECIMAL(10,2) CHECK (balance >= 0),
    acc_type VARCHAR(10) CHECK (acc_type IN ('SAVINGS', 'CURRENT', 'FIXED')),
    interest_rate DECIMAL(5,2) CHECK (interest_rate >= 0 AND interest_rate <= 15),
    opened_date DATE DEFAULT CURRENT_DATE,
    status ENUM('active', 'closed', 'frozen') DEFAULT 'active'
);

-- Valid insertion
INSERT INTO accounts (acc_id, acc_number, balance, acc_type, interest_rate)
VALUES (1, 'ACC001', 5000.00, 'SAVINGS', 4.5);

-- ❌ Error: negative balance
-- INSERT INTO accounts VALUES (2, 'ACC002', -100, 'SAVINGS', 3.0);

-- ❌ Error: invalid account type
-- INSERT INTO accounts VALUES (3, 'ACC003', 1000, 'CHECKING', 2.0);

-- ========================================
-- 7. E-COMMERCE PRODUCT CONSTRAINTS
-- ========================================

CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) CHECK (price > 0),
    discount_pct DECIMAL(5,2) CHECK (discount_pct >= 0 AND discount_pct <= 100),
    stock INT CHECK (stock >= 0) DEFAULT 0,
    rating DECIMAL(3,2) CHECK (rating >= 0 AND rating <= 5),
    category ENUM('electronics', 'clothing', 'food', 'books') NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Valid insertion
INSERT INTO products (product_id, product_name, price, discount_pct, stock, rating, category)
VALUES (1, 'Laptop', 999.99, 10.00, 50, 4.5, 'electronics');

-- ========================================
-- 8. DEFAULT CONSTRAINT EXAMPLES
-- ========================================

CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    product_name VARCHAR(50) NOT NULL,
    stock INT DEFAULT 100,
    status VARCHAR(20) DEFAULT 'available',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- Insert without providing defaults
INSERT INTO products (product_name) VALUES ('Laptop');

-- Automatic assignment: stock=100, status='available', timestamps set
SELECT * FROM products;

-- ========================================
-- 9. USER REGISTRATION WITH DEFAULTS
-- ========================================

CREATE TABLE users (
    user_id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    role VARCHAR(20) DEFAULT 'user',
    credits INT DEFAULT 0,
    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP NULL
);

INSERT INTO users (username, email, password_hash)
VALUES ('alex123', 'alex@example.com', 'hashed_password');
-- Defaults: status='active', role='user', credits=0, registered_at=now

-- ========================================
-- 10. ADDING CONSTRAINTS AFTER TABLE CREATION
-- ========================================

-- Create table without constraints
CREATE TABLE students (
    student_id INT,
    name VARCHAR(50),
    age INT,
    email VARCHAR(100),
    course_id INT
);

-- Add PRIMARY KEY
ALTER TABLE students
ADD CONSTRAINT pk_students PRIMARY KEY (student_id);

-- Add CHECK constraint
ALTER TABLE students
ADD CONSTRAINT chk_age CHECK (age BETWEEN 5 AND 25);

-- Add UNIQUE constraint
ALTER TABLE students
ADD CONSTRAINT uq_email UNIQUE (email);

-- Add FOREIGN KEY constraint
ALTER TABLE students
ADD CONSTRAINT fk_course FOREIGN KEY (course_id)
    REFERENCES courses(course_id)
    ON DELETE CASCADE;

-- Add NOT NULL constraint
ALTER TABLE students
MODIFY name VARCHAR(50) NOT NULL;

-- ========================================
-- 11. FULL E-COMMERCE DATABASE
-- ========================================

-- Customers table
CREATE TABLE customers (
    customer_id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    phone CHAR(10) CHECK (phone REGEXP '^[0-9]{10}\$'),
    status ENUM('active', 'inactive') DEFAULT 'active',
    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Products table
CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    product_name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) CHECK (price > 0),
    stock INT CHECK (stock >= 0) DEFAULT 0,
    category VARCHAR(50) NOT NULL
);

-- Orders table
CREATE TABLE orders (
    order_id INT PRIMARY KEY AUTO_INCREMENT,
    customer_id INT NOT NULL,
    order_date DATE DEFAULT CURRENT_DATE,
    total_amount DECIMAL(10,2) CHECK (total_amount >= 0),
    status ENUM('pending', 'shipped', 'delivered', 'cancelled') DEFAULT 'pending',
    FOREIGN KEY (customer_id)
        REFERENCES customers(customer_id)
        ON DELETE CASCADE
);

-- Order Items table
CREATE TABLE order_items (
    item_id INT PRIMARY KEY AUTO_INCREMENT,
    order_id INT NOT NULL,
    product_id INT NOT NULL,
    quantity INT CHECK (quantity > 0),
    unit_price DECIMAL(10,2) CHECK (unit_price > 0),
    FOREIGN KEY (order_id)
        REFERENCES orders(order_id)
        ON DELETE CASCADE,
    FOREIGN KEY (product_id)
        REFERENCES products(product_id)
        ON DELETE RESTRICT
);

-- ========================================
-- 12. VIEW CONSTRAINTS METADATA
-- ========================================

-- View all constraints for a table (MySQL)
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    CONSTRAINT_TYPE
FROM information_schema.TABLE_CONSTRAINTS
WHERE TABLE_SCHEMA = DATABASE()
    AND TABLE_NAME = 'customers';

-- View foreign key relationships
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    COLUMN_NAME,
    REFERENCED_TABLE_NAME,
    REFERENCED_COLUMN_NAME
FROM information_schema.KEY_COLUMN_USAGE
WHERE TABLE_SCHEMA = DATABASE()
    AND REFERENCED_TABLE_NAME IS NOT NULL;

-- View CHECK constraints
SELECT 
    CONSTRAINT_NAME,
    TABLE_NAME,
    CHECK_CLAUSE
FROM information_schema.CHECK_CONSTRAINTS
WHERE CONSTRAINT_SCHEMA = DATABASE();

-- ========================================
-- 13. DROPPING CONSTRAINTS
-- ========================================

-- Drop CHECK constraint
ALTER TABLE students DROP CONSTRAINT chk_age;

-- Drop FOREIGN KEY constraint
ALTER TABLE enrollments DROP FOREIGN KEY fk_student;

-- Drop UNIQUE constraint
ALTER TABLE students DROP INDEX uq_email;

-- Drop PRIMARY KEY (rarely done)
ALTER TABLE students DROP PRIMARY KEY;

-- ========================================
-- 14. CONSTRAINT NAMING CONVENTIONS
-- ========================================

CREATE TABLE employees (
    emp_id INT,
    email VARCHAR(100),
    dept_id INT,
    salary DECIMAL(10,2),
    CONSTRAINT pk_employees PRIMARY KEY (emp_id),
    CONSTRAINT uq_employees_email UNIQUE (email),
    CONSTRAINT fk_employees_dept_id FOREIGN KEY (dept_id) 
        REFERENCES departments(dept_id),
    CONSTRAINT chk_employees_salary CHECK (salary > 0)
);

-- ========================================
-- 15. USER-DEFINED INTEGRITY EXAMPLES
-- ========================================

-- Start date must be before end date
CREATE TABLE projects (
    project_id INT PRIMARY KEY,
    project_name VARCHAR(100) NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    budget DECIMAL(12,2) CHECK (budget > 0),
    CHECK (start_date < end_date)
);

-- Discount price must be less than original price
CREATE TABLE product_discounts (
    discount_id INT PRIMARY KEY,
    product_id INT,
    original_price DECIMAL(10,2),
    discount_price DECIMAL(10,2),
    CHECK (discount_price < original_price),
    FOREIGN KEY (product_id) REFERENCES products(product_id)
);

-- ========================================
-- 16. FINDING CONSTRAINT VIOLATIONS
-- ========================================

-- Find orphaned records (referential integrity violations)
SELECT o.order_id, o.customer_id
FROM orders o
WHERE NOT EXISTS (
    SELECT 1 FROM customers c 
    WHERE c.customer_id = o.customer_id
);

-- Find domain integrity violations (negative values)
SELECT * FROM employees WHERE salary < 0;
SELECT * FROM products WHERE stock < 0;

-- Find duplicate emails (UNIQUE violations)
SELECT email, COUNT(*) as count
FROM customers
GROUP BY email
HAVING COUNT(*) > 1;
''',
    revisionPoints: [
      'Data Integrity ensures data is accurate, consistent, and reliable throughout its life cycle',
      'Entity Integrity: Each table must have a PRIMARY KEY for unique row identification',
      'Referential Integrity: FOREIGN KEYs maintain valid relationships between tables',
      'Domain Integrity: CHECK, NOT NULL, and DEFAULT constraints ensure valid column values',
      'User-Defined Integrity: Custom business rules enforced through CHECK constraints and triggers',
      'PRIMARY KEY constraint ensures unique and non-null identification (only one per table)',
      'FOREIGN KEY constraint prevents orphaned records and maintains parent-child relationships',
      'UNIQUE constraint prevents duplicate values but allows NULL (multiple per table allowed)',
      'NOT NULL constraint ensures mandatory fields cannot be left empty',
      'CHECK constraint restricts values based on conditions (e.g., age >= 0, salary > 0)',
      'DEFAULT constraint provides automatic values when none is supplied (e.g., status = active)',
      'ON DELETE CASCADE automatically deletes dependent rows when parent is deleted',
      'ON DELETE SET NULL sets foreign key to NULL when parent is deleted (optional relationships)',
      'ON DELETE RESTRICT prevents deletion if child records exist (default behavior)',
      'Constraints enforce rules at database level, protecting data even if application has bugs',
      'ALTER TABLE ADD CONSTRAINT adds constraints to existing tables',
      'ALTER TABLE DROP CONSTRAINT removes constraints (use carefully to avoid data corruption)',
      'Constraint violations return errors preventing invalid data insertion or updates',
      'Best practice: Name constraints descriptively (pk_, fk_, uq_, chk_ prefixes)',
      'Banking systems use CHECK constraints to prevent negative balances and withdrawals exceeding limits',
      'E-commerce systems use FOREIGN KEYs to ensure orders reference valid customers and products',
      'Multiple FOREIGN KEYs can exist in one table referencing different parent tables',
      'UNIQUE allows one NULL value, PRIMARY KEY does not allow any NULLs',
      'View constraints metadata using information_schema.TABLE_CONSTRAINTS and KEY_COLUMN_USAGE',
      'Final principle: Constraints are guardrails that keep databases consistent, secure, and meaningful',
    ],
    quizQuestions: [
      Question(
        question: 'Which constraint ensures column values are unique?',
        options: ['PRIMARY KEY', 'UNIQUE', 'CHECK', 'NOT NULL'],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of integrity does PRIMARY KEY enforce?',
        options: ['Domain Integrity', 'Entity Integrity', 'Referential Integrity', 'User-Defined Integrity'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which CASCADE action deletes dependent rows automatically?',
        options: ['ON DELETE CASCADE', 'ON DELETE SET NULL', 'ON DELETE RESTRICT', 'ON UPDATE CASCADE'],
        correctIndex: 0,
      ),
      Question(
        question: 'Can a table have multiple UNIQUE constraints?',
        options: ['Yes, unlimited', 'No, only one', 'Only if PRIMARY KEY exists', 'Only in PostgreSQL'],
        correctIndex: 0,
      ),
      Question(
        question: 'What does NOT NULL constraint prevent?',
        options: ['Duplicate values', 'Missing/empty values', 'Negative values', 'Invalid formats'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which constraint enforces referential integrity?',
        options: ['PRIMARY KEY', 'UNIQUE', 'FOREIGN KEY', 'CHECK'],
        correctIndex: 2,
      ),
      Question(
        question: 'What happens with ON DELETE SET NULL?',
        options: [
          'Deletes child records',
          'Sets foreign key to NULL',
          'Prevents deletion',
          'Updates foreign key'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which constraint restricts values based on conditions?',
        options: ['DEFAULT', 'CHECK', 'UNIQUE', 'NOT NULL'],
        correctIndex: 1,
      ),
      Question(
        question: 'How many PRIMARY KEYs can a table have?',
        options: ['Unlimited', 'Only one', 'One per column', 'Depends on database'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does DEFAULT constraint do?',
        options: [
          'Validates data',
          'Provides automatic value when none supplied',
          'Prevents duplicates',
          'Creates relationships'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Can UNIQUE constraint allow NULL values?',
        options: ['Yes, one NULL', 'No, never', 'Yes, unlimited NULLs', 'Only in MySQL'],
        correctIndex: 0,
      ),
      Question(
        question: 'Which constraint is best for preventing negative salaries?',
        options: ['NOT NULL', 'UNIQUE', 'CHECK (salary > 0)', 'DEFAULT'],
        correctIndex: 2,
      ),
    ],
  ),
  Topic(
    id: 'replication_clustering',
    title: '18. Replication & Clustering',
    explanation: '''## Replication & Clustering

### 1. Introduction

#### What is Replication?

**Definition:**

**Replication** means creating **copies of your database** and storing them on **multiple servers** to improve **availability, reliability, and performance**.

**Key Characteristics:**
- **Data redundancy:** Multiple copies of the same data
- **High availability:** Service continues if one server fails
- **Read scalability:** Distribute read queries across replicas
- **Disaster recovery:** Quick recovery from server failures
- **Geographical distribution:** Serve users from nearest location

**Analogy:**
> 💡 Think of replication like having backup copies of important documents stored in different safe locations. If one location is inaccessible, you can retrieve the document from another location.

**Example Scenario:**

If your main database (primary) fails, another replica (secondary) can take over — ensuring users never face downtime.

**Real-World Example:**
- **Netflix:** Replicates user data across multiple regions
- **Facebook:** Uses replicas to serve billions of read requests
- **Banks:** Maintain replicas for 24/7 availability

---

#### What is Clustering?

**Definition:**

**Clustering** means connecting **multiple database servers** to work as a **single system**, sharing the load and providing high availability.

**Key Characteristics:**
- **Unified system:** Multiple servers act as one database
- **Load sharing:** Distribute workload across nodes
- **Fault tolerance:** Automatic failover if node crashes
- **Horizontal scaling:** Add more nodes to increase capacity
- **Seamless operation:** Users don't notice individual nodes

**Analogy:**
> 💡 Think of clustering as teamwork — multiple servers cooperate to ensure continuous service and faster query handling, just like a team of doctors working together in an emergency room.

**Real-World Example:**
- **E-commerce sites:** Use clusters to handle Black Friday traffic
- **Stock exchanges:** Require zero downtime for transactions
- **Healthcare systems:** Need continuous availability for patient data

---

### 2. Why Replication & Clustering Are Important

#### Comparison Table

| Feature              | Replication                 | Clustering                               |
| -------------------- | --------------------------- | ---------------------------------------- |
| **Goal**             | Copy data across servers    | Combine multiple servers into one system |
| **Focus**            | Data availability & backups | Load balancing & fault tolerance         |
| **Failure Handling** | Failover to replica         | Automatic node takeover                  |
| **Example Use**      | Read replicas for analytics | E-commerce systems with heavy traffic    |
| **Write Operations** | Usually single master       | Can support multi-master                 |
| **Read Operations**  | Distributed across replicas | Distributed across cluster nodes         |
| **Consistency**      | Eventual (async) or Strong (sync) | Strong consistency within cluster   |
| **Cost**             | Lower (simple setup)        | Higher (complex infrastructure)          |

---

#### Business Benefits

**Replication Benefits:**
- ✅ **Zero downtime** for maintenance or failures
- ✅ **Faster reads** by serving from multiple locations
- ✅ **Data protection** through redundancy
- ✅ **Geographic distribution** for global users
- ✅ **Backup without performance impact**

**Clustering Benefits:**
- ✅ **High availability** (99.99% uptime)
- ✅ **Load balancing** across multiple nodes
- ✅ **Automatic failover** (no manual intervention)
- ✅ **Horizontal scalability** (add more nodes easily)
- ✅ **Better resource utilization**

---

### 3. Database Replication – Deep Dive

#### Types of Replication

| Type                               | Description                                       | Example                               | Use Case                           |
| ---------------------------------- | ------------------------------------------------- | ------------------------------------- | ---------------------------------- |
| **Master-Slave (Primary-Replica)** | Writes go to the master; reads can go to replicas | MySQL Replication                     | Read-heavy applications            |
| **Master-Master**                  | Each node can read/write and syncs changes        | PostgreSQL Bi-directional Replication | Multi-region write requirements    |
| **Snapshot Replication**           | Copies data at specific intervals                 | Used for reporting databases          | Periodic reporting, data warehousing |
| **Transactional Replication**      | Copies changes in real-time                       | SQL Server Transactional Replication  | Real-time analytics, low-latency   |
| **Merge Replication**              | Combines data from multiple nodes                 | Used in distributed systems           | Mobile apps, offline-first systems |

---

#### Master-Slave Replication (Primary-Replica)

**Architecture:**
```
[Master/Primary] 
    ↓ (writes)
    ↓ (replication stream)
[Slave/Replica 1] [Slave/Replica 2] [Slave/Replica 3]
    ↑ (reads)       ↑ (reads)         ↑ (reads)
```

**How It Works:**
1. **Master** receives all write operations (INSERT, UPDATE, DELETE)
2. **Binary log** records all changes
3. **Replicas** connect to master and request changes
4. **Replication stream** sends changes to replicas
5. **Replicas** apply changes to their local data
6. **Applications** read from replicas to reduce master load

**Advantages:**
- ✅ Simple setup and management
- ✅ Clear separation of read/write workloads
- ✅ Reduced load on master server
- ✅ Multiple replicas for redundancy

**Disadvantages:**
- ❌ Single point of failure (master)
- ❌ Replication lag (eventual consistency)
- ❌ Write operations limited to master capacity

---

#### Master-Master Replication (Multi-Master)

**Architecture:**
```
[Master 1] ↔ (bidirectional sync) ↔ [Master 2]
    ↓ (reads/writes)                    ↓ (reads/writes)
```

**How It Works:**
1. Both nodes accept **read and write** operations
2. Each node replicates changes to the other
3. **Conflict resolution** required for simultaneous updates
4. Applications can connect to either master

**Advantages:**
- ✅ No single point of failure
- ✅ Better write scalability
- ✅ Lower latency for geographically distributed users
- ✅ High availability

**Disadvantages:**
- ❌ Complex conflict resolution
- ❌ Risk of data inconsistency
- ❌ More difficult to manage

---

#### Asynchronous vs Synchronous Replication

**Asynchronous Replication:**
- Master **does not wait** for replicas to confirm
- **Faster writes** (no waiting)
- **Eventual consistency** (slight delay)
- Risk of **data loss** if master fails before replication
- **Use case:** Read-heavy applications, analytics

**Synchronous Replication:**
- Master **waits** for at least one replica to confirm
- **Slower writes** (waiting overhead)
- **Strong consistency** (no data loss)
- **Guaranteed durability**
- **Use case:** Financial transactions, critical data

---

### 4. Example: MySQL Master-Slave Replication

#### Setup Overview

**Components:**
- **Master Server:** Handles all write operations
- **Slave Server(s):** Receive updates and serve read requests
- **Binary Log:** Records all changes on master
- **Relay Log:** Stores changes on slave before applying

---

#### Step-by-Step Configuration

**Step 1: Enable Binary Logging on Master**

Edit `/etc/mysql/my.cnf`:
```ini
[mysqld]
server-id = 1
log_bin = mysql-bin
binlog_do_db = mydb
binlog_format = ROW
```

**Explanation:**
- `server-id`: Unique identifier for this server
- `log_bin`: Enable binary logging
- `binlog_do_db`: Database to replicate (optional)
- `binlog_format`: ROW (recommended for consistency)

Restart MySQL:
```bash
sudo systemctl restart mysql
```

---

**Step 2: Configure Slave Server**

Edit `/etc/mysql/my.cnf`:
```ini
[mysqld]
server-id = 2
relay-log = /var/log/mysql/mysql-relay-bin
log_bin = mysql-bin
read_only = 1
```

**Explanation:**
- `server-id`: Must be different from master
- `relay-log`: Location of relay log files
- `read_only`: Prevent writes to slave (optional)

---

**Step 3: Create Replication User on Master**

```sql
-- Create replication user
CREATE USER 'repl'@'%' IDENTIFIED BY 'SecurePassword123';

-- Grant replication privileges
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

-- Apply changes
FLUSH PRIVILEGES;
```

**Security Note:**
> 🔒 Use strong passwords and restrict IP addresses (replace '%' with specific IPs)

---

**Step 4: Get Master Log Position**

```sql
-- Lock tables to get consistent snapshot
FLUSH TABLES WITH READ LOCK;

-- Get current binary log file and position
SHOW MASTER STATUS;
```

**Output:**
```
+------------------+----------+--------------+------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+------------------+
| mysql-bin.000001 |      120 | mydb         |                  |
+------------------+----------+--------------+------------------+
```

**Important:** Note the `File` and `Position` values!

---

**Step 5: Configure Slave to Connect to Master**

On slave server:
```sql
-- Configure master connection
CHANGE MASTER TO
    MASTER_HOST='192.168.1.10',
    MASTER_USER='repl',
    MASTER_PASSWORD='SecurePassword123',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=120;

-- Start replication
START SLAVE;
```

**Unlock tables on master:**
```sql
UNLOCK TABLES;
```

---

**Step 6: Check Replication Status**

On slave server:
```sql
SHOW SLAVE STATUS\\G
```

**Key Fields to Check:**
- `Slave_IO_Running: Yes` ✅ (connected to master)
- `Slave_SQL_Running: Yes` ✅ (applying changes)
- `Seconds_Behind_Master: 0` ✅ (no lag)
- `Last_Error:` (should be empty)

**Result:**
> ✅ All changes in the master automatically replicate to the slave!

---

#### Testing Replication

**On Master:**
```sql
USE mydb;
CREATE TABLE test_replication (
    id INT PRIMARY KEY,
    name VARCHAR(50)
);

INSERT INTO test_replication VALUES (1, 'Test Data');
```

**On Slave:**
```sql
USE mydb;
SELECT * FROM test_replication;
-- Should show the inserted data
```

---

### 5. Advantages of Replication

#### Detailed Benefits

**1. Increases Data Availability**
- 📊 **Uptime:** 99.9% → 99.99% with replicas
- 🔄 **Failover:** Automatic promotion of replica
- 🌐 **Global access:** Data available in multiple regions

**2. Provides Read Scalability**
- 📈 **Performance:** Distribute read load across replicas
- ⚡ **Speed:** Each replica handles subset of read queries
- 💰 **Cost-effective:** Scale reads without scaling writes

**3. Allows Backup Without Affecting Live Data**
- 💾 **Zero impact:** Backup from replica, not master
- ⏰ **Anytime:** No need for maintenance windows
- 🛡️ **Safety:** Master continues serving users

**4. Improves Disaster Recovery**
- 🚨 **Fast recovery:** Promote replica to master in seconds
- 🗺️ **Geographic redundancy:** Replicas in different data centers
- 📉 **RTO/RPO:** Recovery Time/Point Objective < 1 minute

**5. Supports Geo-Distributed Systems**
- 🌍 **Latency:** Users connect to nearest replica
- 🚀 **Performance:** Local reads instead of cross-continent queries
- 📍 **Compliance:** Data residency requirements (GDPR)

---

### 6. Challenges in Replication

#### Common Issues and Solutions

| Challenge               | Description                                   | Solution                                |
| ----------------------- | --------------------------------------------- | --------------------------------------- |
| **Data Latency**        | Replication delay between master and replica  | Use synchronous replication for critical data |
| **Conflict Resolution** | In multi-master systems, updates may conflict | Implement last-write-wins or CRDT      |
| **Network Overhead**    | Data sync consumes bandwidth                  | Compress replication stream, use local replicas |
| **Consistency Issues**  | Eventual consistency in async replication     | Use read-your-writes consistency       |
| **Monitoring**          | Detecting replication failures                | Use monitoring tools (Prometheus, Nagios) |
| **Failover Complexity** | Manual promotion can be error-prone           | Use automatic failover tools (MHA, Orchestrator) |

---

#### Replication Lag

**Definition:**
Time delay between a write on master and its appearance on replica.

**Causes:**
- ❌ High write load on master
- ❌ Network latency
- ❌ Slow disk I/O on replica
- ❌ Long-running queries on replica

**Monitoring:**
```sql
-- Check replication lag (seconds)
SHOW SLAVE STATUS\\G
-- Look at: Seconds_Behind_Master
```

**Solutions:**
- ✅ Upgrade replica hardware
- ✅ Optimize network connection
- ✅ Use parallel replication
- ✅ Reduce write load on master

---

### 7. Database Clustering – Deep Dive

#### What is Database Clustering?

**Definition:**

Clustering connects multiple database instances (nodes) so they behave as **one unified database system**.

**Key Concepts:**
- **Nodes:** Individual database servers in the cluster
- **Quorum:** Minimum nodes needed to make decisions
- **Split-brain:** When nodes can't communicate (dangerous)
- **Fencing:** Preventing split-brain by isolating nodes
- **Leader election:** Choosing primary node automatically

**How It Works:**

Each node can process queries, and if one fails — another node continues the service seamlessly.

**Cluster Components:**
1. **Database nodes:** Store and process data
2. **Coordinator:** Manages cluster state (Etcd, Consul, ZooKeeper)
3. **Load balancer:** Distributes client connections
4. **Monitoring:** Detects node failures

---

#### Types of Clustering

**1. Active-Passive (Failover)**

**Architecture:**
```
[Active Node] ← (clients) → [Load Balancer]
[Passive Node (standby)]
```

**Characteristics:**
- One node **active**, another **on standby**
- Standby node takes over if active fails
- **No load sharing** (passive node idle)
- **Fast failover** (seconds to minutes)
- **Simplest setup**

**Example:** MySQL InnoDB Cluster with single primary

---

**2. Active-Active (Multi-Master)**

**Architecture:**
```
[Active Node 1] ← (clients) → [Load Balancer] ← (clients) → [Active Node 2]
        ↑                                                          ↑
        └──────────────── (synchronization) ──────────────────────┘
```

**Characteristics:**
- **All nodes serve requests** concurrently
- **Load sharing** across nodes
- **Better resource utilization**
- **Complex conflict resolution**
- **Higher availability**

**Examples:** Oracle RAC, PostgreSQL with Patroni

---

**3. Shared Disk Cluster**

**Architecture:**
```
[Node 1] ──→ [Shared Storage] ←── [Node 2]
```

**Characteristics:**
- All nodes access **same physical storage**
- **Single source of truth**
- **No data duplication**
- **Storage contention** possible
- **Expensive** (SAN/NAS required)

**Example:** Oracle Real Application Clusters (RAC)

---

**4. Shared Nothing Cluster**

**Architecture:**
```
[Node 1 + Storage] ← (network) → [Node 2 + Storage] ← (network) → [Node 3 + Storage]
```

**Characteristics:**
- Each node has **own storage**
- **No shared storage**
- **Data replication** between nodes
- **Horizontal scalability**
- **Cost-effective** (commodity hardware)

**Examples:** Cassandra, MongoDB, Couchbase

---

### 8. Example: PostgreSQL High Availability Cluster

#### Setup Using Patroni

**What is Patroni?**

Patroni is a template for **high availability PostgreSQL** solutions using:
- **Python** for cluster management
- **Etcd/Consul/ZooKeeper** for distributed consensus
- **Automatic failover** and leader election
- **REST API** for monitoring and control

---

#### Architecture

```
[Patroni + PostgreSQL Primary] 
    ↓ (streaming replication)
[Patroni + PostgreSQL Replica 1] [Patroni + PostgreSQL Replica 2]
    ↑                                 ↑
    └──────── [Etcd Cluster] ─────────┘
    ↑
[HAProxy Load Balancer]
    ↑
[Applications]
```

---

#### Configuration Example

**Patroni Configuration (patroni.yml):**

```yaml
scope: postgres
namespace: /db/
name: node1

restapi:
  listen: 0.0.0.0:8008
  connect_address: 192.168.1.10:8008

etcd:
  host: 127.0.0.1:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576

postgresql:
  listen: 0.0.0.0:5432
  connect_address: 192.168.1.10:5432
  data_dir: /var/lib/postgresql/data
  
  parameters:
    wal_level: replica
    hot_standby: on
    max_wal_senders: 10
    max_replication_slots: 10
    wal_keep_segments: 8
    
  authentication:
    replication:
      username: replicator
      password: SecurePassword123
    superuser:
      username: postgres
      password: AdminPassword123
```

**Key Settings:**
- `scope`: Cluster identifier
- `wal_level: replica`: Enable streaming replication
- `max_wal_senders`: Number of simultaneous replicas
- `ttl`: Leader lease time (30 seconds)

---

#### How Patroni Works

**Normal Operation:**
1. **Etcd** stores cluster state
2. **Patroni** on each node monitors PostgreSQL
3. **Primary node** holds leader lock in Etcd
4. **Replicas** stream changes from primary

**Failover Process:**
1. **Primary fails** (crash, network issue)
2. **Leader lock expires** in Etcd
3. **Patroni agents compete** for new leader lock
4. **Healthiest replica wins** (lowest lag)
5. **New primary promoted** automatically
6. **Other replicas reconnect** to new primary

**Failover Time:** Typically 10-30 seconds

---

### 9. Load Balancing in Clusters

#### Purpose

**Load balancing** distributes client connections and queries across multiple database nodes to:
- ✅ Prevent overload on single node
- ✅ Improve response times
- ✅ Maximize resource utilization
- ✅ Provide connection pooling

---

#### HAProxy Configuration Example

**HAProxy Configuration (haproxy.cfg):**

```bash
global
    maxconn 100

defaults
    log global
    mode tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

# Frontend for write operations (primary only)
frontend pgsql_write
    bind *:5432
    default_backend pgsql_primary

# Backend for write operations
backend pgsql_primary
    option httpchk
    http-check expect status 200
    server db1 192.168.1.10:5432 maxconn 100 check port 8008
    
# Frontend for read operations (replicas)
frontend pgsql_read
    bind *:5433
    default_backend pgsql_replicas

# Backend for read operations
backend pgsql_replicas
    balance roundrobin
    option httpchk
    http-check expect status 200
    server db2 192.168.1.11:5432 maxconn 100 check port 8008
    server db3 192.168.1.12:5432 maxconn 100 check port 8008
```

**Load Balancing Algorithms:**
- **Round Robin:** Each server in turn
- **Least Connections:** Server with fewest active connections
- **Source IP Hash:** Same client always routes to same server

**Benefits:**
> ✅ Helps in handling large user traffic efficiently by distributing load across multiple nodes.

---

### 10. Replication vs Clustering

#### Comprehensive Comparison

| Feature              | Replication              | Clustering                      |
| -------------------- | ------------------------ | ------------------------------- |
| **Data Copies**      | Multiple copies          | Shared or distributed nodes     |
| **Write Operations** | Usually one master       | Often multiple active nodes     |
| **Consistency**      | Eventual (Async)         | Strong (Synchronous)            |
| **Use Case**         | Backup, read scalability | Fault tolerance, load sharing   |
| **Example**          | MySQL Replication        | Oracle RAC, MongoDB Replica Set |
| **Complexity**       | Low to Medium            | Medium to High                  |
| **Cost**             | Lower                    | Higher                          |
| **Failover**         | Manual or semi-automatic | Automatic                       |
| **Performance**      | Excellent for reads      | Balanced read/write             |
| **Setup Time**       | Hours                    | Days to weeks                   |

---

#### When to Use Each

**Use Replication When:**
- ✅ Read-heavy workload (90% reads, 10% writes)
- ✅ Need geographic distribution
- ✅ Want simple disaster recovery
- ✅ Budget-conscious
- ✅ Analytics and reporting queries

**Use Clustering When:**
- ✅ Need zero downtime (99.99%+ uptime)
- ✅ Automatic failover required
- ✅ Balanced read/write workload
- ✅ High transaction volume
- ✅ Mission-critical applications

**Use Both When:**
- ✅ Global e-commerce platform
- ✅ Social media applications
- ✅ Banking systems
- ✅ Real-time analytics platforms

---

### 11. Real-World Examples

#### Database-Specific Implementations

| Database       | Replication                    | Clustering                      | Best For                        |
| -------------- | ------------------------------ | ------------------------------- | ------------------------------- |
| **MySQL**      | Master-Slave, GTID replication | InnoDB Cluster                  | Web applications, e-commerce    |
| **PostgreSQL** | Streaming Replication          | Patroni / Citus Cluster         | Enterprise apps, analytics      |
| **MongoDB**    | Replica Sets                   | Sharded Clusters                | Document storage, real-time     |
| **Cassandra**  | Peer-to-peer replication       | Ring-based clustering           | Time-series, IoT data           |
| **Oracle**     | Data Guard                     | RAC (Real Application Clusters) | Enterprise, mission-critical    |
| **Redis**      | Master-Slave + Sentinel        | Redis Cluster                   | Caching, session storage        |
| **SQL Server** | Always On Availability Groups  | Failover Cluster Instances      | Windows environments            |

---

#### Industry Use Cases

**E-Commerce Platform (Amazon-scale):**
- **Replication:** Product catalog replicated globally (read-heavy)
- **Clustering:** Order processing cluster (write-heavy, zero downtime)
- **Result:** Handle millions of transactions during Black Friday

**Social Media (Facebook-scale):**
- **Replication:** User feeds replicated across data centers
- **Clustering:** Friend graph database cluster
- **Result:** Billions of users served with low latency

**Banking System:**
- **Replication:** Synchronous replication for transactions
- **Clustering:** Active-active cluster for ATM network
- **Result:** 99.999% uptime (5 minutes downtime per year)

---

### 12. Failover and High Availability

#### Automatic Failover Process

**Detection Phase:**
1. **Health checks** fail on primary node
2. **Timeout period** expires (10-30 seconds)
3. **Cluster manager** detects failure

**Election Phase:**
4. **Quorum check:** Ensure majority of nodes agree
5. **Replica selection:** Choose healthiest replica (lowest lag)
6. **Promotion:** Elected replica becomes new primary

**Recovery Phase:**
7. **DNS/VIP update:** Point applications to new primary
8. **Client reconnection:** Applications reconnect automatically
9. **Old primary fenced:** Prevent split-brain

**Total Failover Time:** 10-60 seconds

---

#### MongoDB Replica Set Failover Example

**Normal State:**
```
PRIMARY (mongo1) ← [Heartbeat] → SECONDARY (mongo2)
                ← [Heartbeat] → SECONDARY (mongo3)
```

**Primary Failure:**
```
FAILED (mongo1)     SECONDARY (mongo2) ← [Election]
                    SECONDARY (mongo3) ← [Votes]
```

**After Election:**
```
FAILED (mongo1)     PRIMARY (mongo2) ← [New writes]
                    SECONDARY (mongo3) ← [Replication]
```

**MongoDB Automatic Failover:**
> 💡 In MongoDB, a secondary replica automatically becomes the new primary if the current one fails. Election completes in 2-12 seconds.

---

### 13. Monitoring Replication and Cluster Health

#### Key Metrics to Monitor

**Replication Metrics:**
- 📊 **Replication Lag:** Time delay behind master
- 📈 **Throughput:** Transactions per second
- 🔗 **Connection Status:** Replica connected to master
- 💾 **Binary Log Size:** Disk space usage
- ⚠️ **Errors:** Replication errors and warnings

**Cluster Metrics:**
- 🖥️ **Node Status:** Online, offline, recovering
- 🔄 **Quorum Status:** Majority available
- ⚡ **Response Time:** Query latency per node
- 💻 **Resource Usage:** CPU, memory, disk
- 🌐 **Network:** Bandwidth, packet loss

---

#### Monitoring Commands

**MySQL Replication Status:**
```sql
-- Check replication status
SHOW SLAVE STATUS\\G;

-- Key fields:
-- Slave_IO_Running: Yes/No
-- Slave_SQL_Running: Yes/No
-- Seconds_Behind_Master: 0 (ideal)
-- Last_Error: (should be empty)
```

**PostgreSQL Replication Lag:**
```sql
-- Check replication lag
SELECT 
    client_addr,
    state,
    sent_lsn,
    write_lsn,
    flush_lsn,
    replay_lsn,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes
FROM pg_stat_replication;
```

**MongoDB Replica Set Status:**
```javascript
// Check replica set status
rs.status()

// Check replication lag
rs.printSlaveReplicationInfo()
```

---

#### Monitoring Tools

**Popular Tools:**
- **Prometheus + Grafana:** Time-series metrics and dashboards
- **Percona Monitoring and Management (PMM):** MySQL/PostgreSQL monitoring
- **MongoDB Cloud Manager:** MongoDB-specific monitoring
- **Nagios/Zabbix:** General-purpose monitoring with alerting
- **Datadog/New Relic:** Cloud-based APM with database monitoring

**Alert Thresholds:**
- ⚠️ **Replication Lag > 10 seconds:** Warning
- 🚨 **Replication Lag > 60 seconds:** Critical
- 🚨 **Replica connection lost:** Critical
- ⚠️ **Node CPU > 80%:** Warning
- 🚨 **Quorum lost:** Critical

---

### 14. Replication in NoSQL Databases

#### NoSQL Replication Models

| Database          | Replication Style                    | Consistency Model         | Failover         |
| ----------------- | ------------------------------------ | ------------------------- | ---------------- |
| **MongoDB**       | Replica Sets with automatic failover | Strong (primary reads)    | Automatic (10s)  |
| **Cassandra**     | Peer-to-peer, eventually consistent  | Tunable consistency       | No single master |
| **Redis**         | Master-Slave + Sentinel for HA       | Eventual (async)          | Sentinel-managed |
| **Elasticsearch** | Shards with replicated copies        | Eventual                  | Automatic        |
| **Couchbase**     | Cross-datacenter replication (XDCR)  | Eventually consistent     | Automatic        |

---

#### MongoDB Replica Set Configuration

**Initialize Replica Set:**

```javascript
// Connect to MongoDB
mongo --host mongo1:27017

// Initialize replica set
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017", priority: 2 },
    { _id: 1, host: "mongo2:27017", priority: 1 },
    { _id: 2, host: "mongo3:27017", priority: 1 }
  ]
});

// Check replica set status
rs.status();

// Check current primary
rs.isMaster();
```

**Benefits:**
- ✅ Automatic failover (10-12 seconds)
- ✅ Read preference options (primary, secondary, nearest)
- ✅ Write concern levels (acknowledged, majority)
- ✅ Geographic distribution support

---

#### Cassandra Peer-to-Peer Replication

**Architecture:**
```
[Node 1] ↔ [Node 2] ↔ [Node 3] ↔ [Node 4]
   ↑                                  ↓
   └──────────────────────────────────┘
         (Ring topology)
```

**Key Concepts:**
- **Ring topology:** All nodes equal (no master)
- **Replication factor:** Number of copies (RF=3 typical)
- **Consistency level:** Quorum, ONE, ALL
- **Hinted handoff:** Store writes for offline nodes

**Tunable Consistency:**
```sql
-- Write to majority, read from one
CONSISTENCY QUORUM;
INSERT INTO users (id, name) VALUES (1, 'Alice');

CONSISTENCY ONE;
SELECT * FROM users WHERE id = 1;
```

---

### 15. Real-World Architecture Example

#### E-Commerce Database Architecture

**Global E-Commerce Platform:**

**Requirements:**
- 📈 **Traffic:** 1 million concurrent users
- 🌍 **Global:** Serve users in NA, EU, APAC
- 🛒 **Orders:** 10,000 orders per minute
- ⚡ **Latency:** < 100ms response time
- 🔒 **Uptime:** 99.99% availability

---

**Architecture Design:**

**Replication Layer (Read Scalability):**
```
[Region: North America]
  Primary: MySQL Master
  Replicas: 5 read replicas (product catalog, user data)

[Region: Europe]
  Replicas: 3 read replicas (local reads)

[Region: Asia-Pacific]
  Replicas: 3 read replicas (local reads)
```

**Benefits:**
- ✅ Product catalog reads served locally (low latency)
- ✅ User profile reads distributed across replicas
- ✅ Analytics queries run on dedicated replicas

---

**Clustering Layer (High Availability):**
```
[Order Processing Cluster]
  Node 1: Active (handles orders)
  Node 2: Active (handles payments)
  Node 3: Standby (automatic failover)
  
[Shopping Cart Cluster (Redis)]
  Node 1, Node 2, Node 3: Active-Active
  Sentinel: Monitors health
```

**Benefits:**
- ✅ Zero downtime for order processing
- ✅ Automatic failover if node crashes
- ✅ Load balanced across multiple nodes
- ✅ Session data always available

---

**Result:**
- 📊 **Performance:** 50ms average response time
- 🎯 **Availability:** 99.995% uptime achieved
- 💰 **Cost:** 60% reduction in master load
- 🚀 **Scalability:** Handle Black Friday traffic (10x normal)

---

### 16. Advantages Summary

#### Replication Advantages

| Benefit               | Description                             | Business Impact                   |
| --------------------- | --------------------------------------- | --------------------------------- |
| **High Availability** | Service continues despite server failure | Minimize revenue loss             |
| **Scalability**       | Handle more users easily                | Support business growth           |
| **Data Redundancy**   | Protects against data loss              | Prevent catastrophic failures     |
| **Performance**       | Faster reads and queries                | Improve user experience           |
| **Disaster Recovery** | Replica can be promoted instantly       | Reduce RTO/RPO                    |
| **Geographic Distribution** | Serve users from nearest location | Lower latency globally            |
| **Load Distribution** | Spread read queries across replicas     | Prevent master overload           |

---

#### Clustering Advantages

| Benefit               | Description                             | Business Impact                   |
| --------------------- | --------------------------------------- | --------------------------------- |
| **Zero Downtime**     | Automatic failover in seconds           | 99.99%+ uptime SLA                |
| **Load Balancing**    | Distribute queries across nodes         | Better resource utilization       |
| **Horizontal Scaling**| Add nodes to increase capacity          | Cost-effective scalability        |
| **Fault Tolerance**   | No single point of failure              | Mission-critical reliability      |
| **Automatic Recovery**| Self-healing architecture               | Reduce operational overhead       |

---

### 17. Best Practices

#### Replication Best Practices

**1. Always monitor replication lag**
```sql
-- Set up alerts for lag > 10 seconds
SELECT Seconds_Behind_Master FROM SHOW SLAVE STATUS;
```

**2. Use asynchronous replication for read-heavy workloads**
- ✅ Better performance
- ✅ Lower latency
- ⚠️ Risk of data loss (acceptable for reads)

**3. For critical systems, prefer synchronous replication or clustering**
- ✅ Zero data loss
- ✅ Strong consistency
- ⚠️ Higher latency

**4. Secure all nodes with SSL and access control**
```sql
-- Enable SSL for replication
CHANGE MASTER TO 
    MASTER_SSL=1,
    MASTER_SSL_CA='/path/to/ca.pem';
```

**5. Regularly test failover scenarios**
- 🧪 Monthly failover drills
- 📋 Document failover procedures
- ⏱️ Measure actual failover time

**6. Use consistent backups across replicas**
- 💾 Backup from replica (not master)
- ⏰ Schedule during low-traffic periods
- 🔍 Verify backup integrity

---

#### Clustering Best Practices

**1. Use odd number of nodes for quorum**
- ✅ 3, 5, or 7 nodes (avoid split-brain)
- ❌ Never use 2 nodes (no quorum possible)

**2. Separate coordinator nodes from data nodes**
- ✅ Dedicated Etcd/Consul cluster
- ✅ Prevents coordinator failure affecting data

**3. Network redundancy**
- ✅ Multiple network paths between nodes
- ✅ Monitor network latency and packet loss

**4. Regular health checks**
- ✅ HTTP health endpoints
- ✅ TCP connection checks
- ✅ Query response time monitoring

**5. Capacity planning**
- ✅ Monitor resource usage trends
- ✅ Add nodes before reaching 80% capacity
- ✅ Test performance under load

---

### 18. Common Interview Questions

**1. Difference between replication and clustering?**

**Answer:**
- **Replication:** Creates copies of data across multiple servers for redundancy and read scalability. Typically has one master for writes.
- **Clustering:** Connects multiple servers to work as one unified system, providing automatic failover and load balancing. Can support multi-master writes.

---

**2. What is synchronous vs asynchronous replication?**

**Answer:**
- **Synchronous:** Master waits for replica to confirm write before responding to client. Ensures zero data loss but slower.
- **Asynchronous:** Master responds immediately without waiting. Faster but risk of data loss if master fails.

---

**3. How does MySQL handle replication lag?**

**Answer:**
MySQL uses binary log positions to track replication progress. Lag is measured by comparing master's binary log position with replica's applied position. Solutions include parallel replication, optimizing replica hardware, and reducing write load.

---

**4. What is a read replica?**

**Answer:**
A read replica is a copy of the database that receives updates from the master and serves only read queries. Used to offload read traffic from the master, enabling horizontal read scalability.

---

**5. Explain failover in PostgreSQL.**

**Answer:**
PostgreSQL failover involves promoting a standby replica to primary when the current primary fails. Tools like Patroni automate this process by:
1. Detecting primary failure (health checks)
2. Electing new primary (consensus via Etcd)
3. Promoting chosen replica (pg_ctl promote)
4. Updating connection routing (load balancer)

---

**6. What is an active-active cluster?**

**Answer:**
An active-active cluster has all nodes serving requests concurrently (no idle standbys). Provides better resource utilization and load distribution. Requires conflict resolution for multi-master writes. Examples: Oracle RAC, PostgreSQL with BDR.

---

**7. How does MongoDB ensure data consistency in replica sets?**

**Answer:**
MongoDB uses:
- **Write Concern:** Specify acknowledgment level (majority, all)
- **Read Concern:** Specify data freshness (local, majority)
- **Oplog:** Operation log for replication
- **Election:** Automatic primary election on failure
- **Heartbeat:** Nodes monitor each other's health

---

### 19. Summary Table

| Concept               | Description                   | Key Benefit                  |
| --------------------- | ----------------------------- | ---------------------------- |
| **Replication**       | Copy data to multiple servers | Read scalability, redundancy |
| **Clustering**        | Group servers to act as one   | High availability, failover  |
| **Master-Slave**      | Writes to master, reads from slaves | Simple, scalable reads   |
| **Master-Master**     | All nodes accept writes       | Write scalability            |
| **Failover**          | Auto switch to backup node    | Zero downtime                |
| **Read Replica**      | Optimizes read-heavy queries  | Reduced master load          |
| **Load Balancer**     | Distributes query load        | Better resource utilization  |
| **High Availability** | Prevents downtime             | 99.99%+ uptime               |
| **Synchronous Replication** | Zero data loss          | Strong consistency           |
| **Asynchronous Replication** | Better performance     | Eventual consistency         |

---

### 20. Final Thought

> **"Replication keeps your data alive,**
> **Clustering keeps your system alive."**
> 
> Together, they form the **backbone of reliable, scalable database infrastructure**. Modern applications demand both:
> - 🌍 **Replication** for global reach and performance
> - 🛡️ **Clustering** for resilience and availability
> 
> Master these concepts, and you'll build systems that never sleep, never fail, and always deliver.
''',
    codeSnippet: '''
-- ========================================
-- 1. MYSQL MASTER-SLAVE REPLICATION SETUP
-- ========================================

-- MASTER SERVER CONFIGURATION (my.cnf)
-- [mysqld]
-- server-id = 1
-- log_bin = mysql-bin
-- binlog_do_db = mydb
-- binlog_format = ROW

-- Create replication user on master
CREATE USER 'repl'@'%' IDENTIFIED BY 'SecurePassword123';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';
FLUSH PRIVILEGES;

-- Lock tables and get master status
FLUSH TABLES WITH READ LOCK;
SHOW MASTER STATUS;
-- Note: File = mysql-bin.000001, Position = 120

-- SLAVE SERVER CONFIGURATION (my.cnf)
-- [mysqld]
-- server-id = 2
-- relay-log = /var/log/mysql/mysql-relay-bin
-- read_only = 1

-- Configure slave to connect to master
CHANGE MASTER TO
    MASTER_HOST='192.168.1.10',
    MASTER_USER='repl',
    MASTER_PASSWORD='SecurePassword123',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=120;

-- Start replication
START SLAVE;

-- Check replication status
SHOW SLAVE STATUS\\G;
-- Look for:
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
-- Seconds_Behind_Master: 0

-- Unlock tables on master
UNLOCK TABLES;

-- ========================================
-- 2. TESTING REPLICATION
-- ========================================

-- On MASTER: Create test data
USE mydb;
CREATE TABLE test_replication (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

INSERT INTO test_replication (id, name) VALUES 
(1, 'Test Data 1'),
(2, 'Test Data 2'),
(3, 'Test Data 3');

-- On SLAVE: Verify replication
USE mydb;
SELECT * FROM test_replication;
-- Should show all 3 rows

-- ========================================
-- 3. MONITORING REPLICATION LAG
-- ========================================

-- Check replication lag (MySQL)
SHOW SLAVE STATUS\\G;
-- Key field: Seconds_Behind_Master

-- Find replication errors
SELECT * FROM performance_schema.replication_connection_status;
SELECT * FROM performance_schema.replication_applier_status;

-- ========================================
-- 4. POSTGRESQL STREAMING REPLICATION
-- ========================================

-- PRIMARY SERVER (postgresql.conf)
-- wal_level = replica
-- max_wal_senders = 10
-- max_replication_slots = 10
-- hot_standby = on

-- Create replication user on primary
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'SecurePassword123';

-- Configure pg_hba.conf (allow replication connections)
-- host replication replicator 192.168.1.0/24 md5

-- STANDBY SERVER (recovery.conf or postgresql.auto.conf)
-- standby_mode = on
-- primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator password=SecurePassword123'
-- trigger_file = '/tmp/postgresql.trigger.5432'

-- Check replication status on primary
SELECT 
    client_addr,
    state,
    sent_lsn,
    write_lsn,
    flush_lsn,
    replay_lsn,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes
FROM pg_stat_replication;

-- ========================================
-- 5. MONGODB REPLICA SET CONFIGURATION
-- ========================================

-- Initialize replica set
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017", priority: 2 },
    { _id: 1, host: "mongo2:27017", priority: 1 },
    { _id: 2, host: "mongo3:27017", priority: 1 }
  ]
});

-- Check replica set status
rs.status();

-- Check current primary
rs.isMaster();

-- Check replication lag
rs.printSlaveReplicationInfo();

-- Add new member to replica set
rs.add("mongo4:27017");

-- Remove member from replica set
rs.remove("mongo4:27017");

-- Set replica priority (higher = more likely to become primary)
cfg = rs.conf();
cfg.members[1].priority = 0.5;
rs.reconfig(cfg);

-- ========================================
-- 6. HAPROXY LOAD BALANCER CONFIGURATION
-- ========================================

-- haproxy.cfg configuration file
-- global
--     maxconn 100
-- 
-- defaults
--     mode tcp
--     timeout connect 4s
--     timeout client 30m
--     timeout server 30m
-- 
-- # Write operations (primary only)
-- frontend pgsql_write
--     bind *:5432
--     default_backend pgsql_primary
-- 
-- backend pgsql_primary
--     option httpchk
--     http-check expect status 200
--     server db1 192.168.1.10:5432 check port 8008
-- 
-- # Read operations (replicas)
-- frontend pgsql_read
--     bind *:5433
--     default_backend pgsql_replicas
-- 
-- backend pgsql_replicas
--     balance roundrobin
--     server db2 192.168.1.11:5432 check
--     server db3 192.168.1.12:5432 check

-- ========================================
-- 7. PATRONI CONFIGURATION (YAML)
-- ========================================

-- patroni.yml
-- scope: postgres
-- namespace: /db/
-- name: node1
-- 
-- restapi:
--   listen: 0.0.0.0:8008
--   connect_address: 192.168.1.10:8008
-- 
-- etcd:
--   host: 127.0.0.1:2379
-- 
-- bootstrap:
--   dcs:
--     ttl: 30
--     loop_wait: 10
--     retry_timeout: 10
-- 
-- postgresql:
--   listen: 0.0.0.0:5432
--   connect_address: 192.168.1.10:5432
--   data_dir: /var/lib/postgresql/data
--   
--   parameters:
--     wal_level: replica
--     hot_standby: on
--     max_wal_senders: 10
--     max_replication_slots: 10

-- ========================================
-- 8. REPLICATION MONITORING QUERIES
-- ========================================

-- MySQL: Check replication health
SHOW SLAVE STATUS\\G;
SELECT * FROM performance_schema.replication_connection_status;

-- PostgreSQL: Check replication lag
SELECT 
    client_addr AS replica_ip,
    state,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes,
    EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds
FROM pg_stat_replication;

-- PostgreSQL: Check if server is in recovery (replica)
SELECT pg_is_in_recovery();

-- MongoDB: Check replica set status
rs.status();
rs.printReplicationInfo();

-- ========================================
-- 9. FAILOVER SIMULATION AND TESTING
-- ========================================

-- MySQL: Stop master to test failover
-- sudo systemctl stop mysql

-- On slave: Promote to master
STOP SLAVE;
RESET SLAVE ALL;

-- Make slave writable
SET GLOBAL read_only = 0;

-- PostgreSQL: Promote standby to primary
SELECT pg_promote();
-- Or use trigger file: touch /tmp/postgresql.trigger.5432

-- MongoDB: Force election (step down current primary)
rs.stepDown(60);  -- Step down for 60 seconds

-- ========================================
-- 10. REPLICATION USER AND PERMISSIONS
-- ========================================

-- MySQL: Create replication user with SSL
CREATE USER 'repl_ssl'@'%' IDENTIFIED BY 'SecurePassword123' REQUIRE SSL;
GRANT REPLICATION SLAVE ON *.* TO 'repl_ssl'@'%';

-- PostgreSQL: Create replication user
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'SecurePassword123';

-- MongoDB: Create replication admin user
use admin;
db.createUser({
  user: "replicator",
  pwd: "SecurePassword123",
  roles: [ 
    { role: "clusterAdmin", db: "admin" },
    { role: "readWriteAnyDatabase", db: "admin" }
  ]
});

-- ========================================
-- 11. GTID-BASED REPLICATION (MySQL)
-- ========================================

-- Enable GTID on master (my.cnf)
-- gtid_mode = ON
-- enforce_gtid_consistency = ON

-- Configure slave with GTID
CHANGE MASTER TO
    MASTER_HOST='192.168.1.10',
    MASTER_USER='repl',
    MASTER_PASSWORD='SecurePassword123',
    MASTER_AUTO_POSITION=1;

START SLAVE;

-- Check GTID execution
SHOW MASTER STATUS;
-- Shows: Executed_Gtid_Set

-- ========================================
-- 12. MULTI-MASTER REPLICATION (PostgreSQL BDR)
-- ========================================

-- Create BDR extension
CREATE EXTENSION bdr;

-- Initialize BDR on first node
SELECT bdr.bdr_group_create(
    local_node_name := 'node1',
    node_external_dsn := 'host=192.168.1.10 dbname=mydb'
);

-- Join additional nodes
SELECT bdr.bdr_group_join(
    local_node_name := 'node2',
    node_external_dsn := 'host=192.168.1.11 dbname=mydb',
    join_using_dsn := 'host=192.168.1.10 dbname=mydb'
);

-- ========================================
-- 13. CASSANDRA REPLICATION
-- ========================================

-- Create keyspace with replication
CREATE KEYSPACE ecommerce
WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'datacenter1': 3,
  'datacenter2': 2
};

-- Check replication status
nodetool status ecommerce;

-- ========================================
-- 14. REDIS REPLICATION WITH SENTINEL
-- ========================================

-- redis.conf (master)
-- bind 192.168.1.10
-- port 6379

-- redis.conf (slave)
-- replicaof 192.168.1.10 6379
-- replica-read-only yes

-- sentinel.conf
-- sentinel monitor mymaster 192.168.1.10 6379 2
-- sentinel down-after-milliseconds mymaster 5000
-- sentinel parallel-syncs mymaster 1
-- sentinel failover-timeout mymaster 10000

-- Check replication info
INFO replication;

-- ========================================
-- 15. AUTOMATED FAILOVER SCRIPTS
-- ========================================

-- MySQL: Check if slave is ready for promotion
SELECT 
    CASE 
        WHEN Slave_IO_Running = 'Yes' AND 
             Slave_SQL_Running = 'Yes' AND 
             Seconds_Behind_Master = 0 
        THEN 'READY' 
        ELSE 'NOT READY' 
    END AS failover_ready
FROM (SHOW SLAVE STATUS) AS s;

-- ========================================
-- 16. BACKUP FROM REPLICA
-- ========================================

-- MySQL: Backup from slave without affecting master
-- On slave server
mysqldump --single-transaction --master-data=2 mydb > backup.sql

-- PostgreSQL: Backup from standby
pg_basebackup -h 192.168.1.11 -D /backup/pgdata -U replicator -v -P

-- ========================================
-- 17. REPLICATION TROUBLESHOOTING
-- ========================================

-- MySQL: Skip replication error (use carefully!)
STOP SLAVE;
SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1;
START SLAVE;

-- PostgreSQL: Check replication slots
SELECT * FROM pg_replication_slots;

-- Clean up inactive slots
SELECT pg_drop_replication_slot('slot_name');

-- MongoDB: Resync replica (nuclear option)
rs.syncFrom("mongo1:27017");
''',
    revisionPoints: [
      'Replication creates copies of database across multiple servers for availability, reliability, and performance',
      'Clustering connects multiple database servers to work as a single unified system',
      'Master-Slave replication: writes go to master, reads distributed to replicas (slaves)',
      'Master-Master replication: all nodes accept read/write operations with bidirectional sync',
      'Asynchronous replication: faster writes, eventual consistency, risk of data loss',
      'Synchronous replication: slower writes, strong consistency, zero data loss',
      'Replication provides read scalability by distributing queries across replicas',
      'Clustering provides high availability through automatic failover and load balancing',
      'Active-Passive clustering: one node active, standby waits for failover',
      'Active-Active clustering: all nodes serve requests concurrently with load sharing',
      'Replication lag is the time delay between write on master and appearance on replica',
      'Failover automatically promotes replica to primary when master fails (10-60 seconds)',
      'Binary log (MySQL) records all changes for replication to slaves',
      'Streaming replication (PostgreSQL) uses WAL (Write-Ahead Log) for continuous data transfer',
      'MongoDB Replica Sets provide automatic failover with leader election (10-12 seconds)',
      'Load balancers (HAProxy, PgBouncer) distribute client connections across database nodes',
      'Read replicas optimize read-heavy workloads by offloading queries from master',
      'Shared disk clustering: all nodes access same storage (Oracle RAC)',
      'Shared nothing clustering: each node has own storage (Cassandra, MongoDB)',
      'GTID (Global Transaction ID) simplifies MySQL replication by auto-positioning',
      'Quorum requires odd number of nodes (3, 5, 7) to prevent split-brain scenarios',
      'Patroni automates PostgreSQL high availability using Etcd/Consul for coordination',
      'Monitor replication with SHOW SLAVE STATUS (MySQL), pg_stat_replication (PostgreSQL)',
      'Best practice: backup from replica to avoid impacting live master performance',
      'Final principle: Replication keeps data alive, Clustering keeps system alive',
    ],
    quizQuestions: [
      Question(
        question: 'What is the purpose of database replication?',
        options: ['Data backup only', 'Redundancy and read scaling', 'Faster writes', 'Data encryption'],
        correctIndex: 1,
      ),
      Question(
        question: 'In Master-Slave replication, where do write operations go?',
        options: ['Any replica', 'Master only', 'Distributed across all', 'Load balancer decides'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is replication lag?',
        options: [
          'Network delay between servers',
          'Time delay between write on master and appearance on replica',
          'Database startup time',
          'Query execution time'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which type of clustering has all nodes serving requests concurrently?',
        options: ['Active-Passive', 'Active-Active', 'Shared Disk', 'Master-Slave'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the main advantage of asynchronous replication?',
        options: ['Zero data loss', 'Faster writes', 'Strong consistency', 'Automatic failover'],
        correctIndex: 1,
      ),
      Question(
        question: 'What tool automates PostgreSQL high availability?',
        options: ['MySQL Router', 'Patroni', 'Redis Sentinel', 'MongoDB Replica Set'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the typical failover time in MongoDB Replica Sets?',
        options: ['1-2 seconds', '10-12 seconds', '1-2 minutes', '10 minutes'],
        correctIndex: 1,
      ),
      Question(
        question: 'What does a load balancer do in database clusters?',
        options: [
          'Encrypts data',
          'Distributes client connections across nodes',
          'Backs up data',
          'Compresses data'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which MySQL command shows replication status?',
        options: ['SHOW MASTER STATUS', 'SHOW SLAVE STATUS', 'SHOW REPLICATION', 'SHOW BINARY LOG'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a read replica used for?',
        options: [
          'Write operations only',
          'Offloading read queries from master',
          'Data encryption',
          'Backup storage'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Why use odd number of nodes in clustering?',
        options: [
          'Better performance',
          'Prevent split-brain with quorum',
          'Easier configuration',
          'Lower cost'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does GTID stand for in MySQL replication?',
        options: [
          'Global Transaction ID',
          'General Transfer ID',
          'Group Transaction Index',
          'Global Transfer Index'
        ],
        correctIndex: 0,
      ),
    ],
  ),
  Topic(
    id: 'partitioning_sharding',
    title: '19. Partitioning & Sharding',
    explanation: '''## Partitioning & Sharding in Databases

### 1. Introduction

As databases grow to millions or even billions of rows, performance starts to slow down. Query execution time increases, indexes become massive, and maintenance becomes challenging.

To handle this efficiently, we divide the data into **smaller, more manageable pieces** — this process is called **Partitioning** or **Sharding**.

**The Problem:**
- 📊 **Large tables:** 100M+ rows slow down queries
- 💾 **Index bloat:** Indexes become too large to fit in memory
- 🐌 **Maintenance:** VACUUM, ANALYZE take hours or days
- 🔒 **Lock contention:** Updates block large portions of table
- 💰 **Cost:** Vertical scaling (bigger servers) is expensive

**The Solution:**
- ✅ **Divide and conquer:** Split data into smaller chunks
- ✅ **Parallel processing:** Query multiple chunks simultaneously
- ✅ **Horizontal scaling:** Add more servers instead of bigger servers
- ✅ **Targeted queries:** Access only relevant data subset

**Analogy:**
> 💡 Think of it like organizing books in a library — instead of one giant shelf with millions of books, you have many smaller shelves (sections) organized by subject, author, or year. This makes finding and managing books much faster and easier.

---

### 2. What Is Partitioning?

#### Definition

**Partitioning** means dividing a **large database table** into smaller logical parts (called *partitions*) within the **same database server**.

**Key Characteristics:**
- 📂 **Logical division:** Table split into smaller pieces
- 🖥️ **Single server:** All partitions on same database instance
- 🔍 **Transparent:** Application sees single table
- ⚡ **Performance:** Queries scan only relevant partitions
- 🧹 **Maintenance:** Manage partitions independently

**How It Works:**

Each partition holds a subset of data based on a partitioning key, but **logically behaves like a single table** to applications.

**Physical vs Logical:**
- **Physically:** Data stored in separate table spaces/files
- **Logically:** Applications query as if it's one table

---

#### Example Scenario

**Problem:**
You have a table `Orders` with **50 million records** spanning 5 years. Queries searching for recent orders are slow because the database scans all 50M rows.

**Solution: Partition by Year**

| Partition      | Data Stored        | Row Count |
| -------------- | ------------------ | --------- |
| Orders_2020    | All 2020 orders    | 8M rows   |
| Orders_2021    | All 2021 orders    | 9M rows   |
| Orders_2022    | All 2022 orders    | 10M rows  |
| Orders_2023    | All 2023 orders    | 11M rows  |
| Orders_2024    | All 2024 orders    | 12M rows  |

**Query Before Partitioning:**
```sql
-- Scans all 50 million rows
SELECT * FROM Orders 
WHERE Order_Date BETWEEN '2024-01-01' AND '2024-12-31';
-- Execution time: 45 seconds
```

**Query After Partitioning:**
```sql
-- Scans only Orders_2024 partition (12 million rows)
SELECT * FROM Orders 
WHERE Order_Date BETWEEN '2024-01-01' AND '2024-12-31';
-- Execution time: 8 seconds (5.6x faster!)
```

**Result:**
> 👉 Only the **Orders_2024** partition is scanned — making queries **5-10x faster**!

---

### 3. Types of Partitioning

#### Overview Table

| Type                         | Description                         | Example                   | Best For                        |
| ---------------------------- | ----------------------------------- | ------------------------- | ------------------------------- |
| **Range Partitioning**       | Based on value range                | Order dates by year       | Time-series data, logs          |
| **List Partitioning**        | Based on specific values            | Regions: Asia, Europe, US | Categorical data, regions       |
| **Hash Partitioning**        | Hashing key distributes data evenly | Customer IDs hashed       | Even distribution, no hotspots  |
| **Composite Partitioning**   | Combination of two types            | Range + Hash              | Complex partitioning needs      |
| **Round Robin Partitioning** | Distributes rows sequentially       | Row 1 → P1, Row 2 → P2    | Load balancing, no specific key |

---

#### Range Partitioning

**Definition:**
Data divided based on **ranges of values** in a partitioning key (typically dates or numeric IDs).

**Use Cases:**
- 📅 **Time-series data:** Orders, logs, sensor data
- 📈 **Sequential IDs:** Auto-incrementing primary keys
- 📊 **Financial data:** Quarterly or yearly reports

**Example: MySQL Range Partitioning**

```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    order_date DATE NOT NULL,
    customer_id INT,
    total DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

**Benefits:**
- ✅ **Partition pruning:** Query optimizer skips irrelevant partitions
- ✅ **Easy archiving:** Drop old partitions instead of DELETE
- ✅ **Fast bulk loads:** Load directly into specific partition

**Example: Dropping Old Partition**
```sql
-- Remove all 2020 orders instantly (no DELETE scan)
ALTER TABLE orders DROP PARTITION p2020;
```

---

#### List Partitioning

**Definition:**
Data divided based on **discrete list of values** (categorical data).

**Use Cases:**
- 🌍 **Geographic regions:** Countries, continents
- 🏢 **Departments:** Sales, HR, Engineering
- 📦 **Product categories:** Electronics, Clothing, Books
- 🎯 **Status values:** Active, Inactive, Pending

**Example: PostgreSQL List Partitioning**

```sql
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(100),
    region VARCHAR(20)
) PARTITION BY LIST (region);

-- Create partitions for each region
CREATE TABLE customers_asia PARTITION OF customers
    FOR VALUES IN ('Asia', 'APAC', 'India', 'China', 'Japan');

CREATE TABLE customers_europe PARTITION OF customers
    FOR VALUES IN ('Europe', 'EU', 'UK', 'Germany', 'France');

CREATE TABLE customers_americas PARTITION OF customers
    FOR VALUES IN ('Americas', 'USA', 'Canada', 'Brazil');
```

**Benefits:**
- ✅ **Logical grouping:** Data organized by business logic
- ✅ **Geo-compliance:** Keep EU data in EU partitions
- ✅ **Targeted queries:** Marketing campaigns per region

---

#### Hash Partitioning

**Definition:**
Data distributed using a **hash function** applied to partitioning key, ensuring even distribution.

**Use Cases:**
- 👥 **User data:** Evenly distribute millions of users
- 🔑 **No natural partitioning key:** Random distribution needed
- ⚖️ **Load balancing:** Prevent data skew

**Example: PostgreSQL Hash Partitioning**

```sql
CREATE TABLE user_activity (
    user_id BIGINT,
    activity_type VARCHAR(50),
    activity_date TIMESTAMP,
    PRIMARY KEY (user_id, activity_date)
) PARTITION BY HASH (user_id);

-- Create 4 hash partitions
CREATE TABLE user_activity_0 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);

CREATE TABLE user_activity_1 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);

CREATE TABLE user_activity_2 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 2);

CREATE TABLE user_activity_3 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 3);
```

**How It Works:**
```
user_id = 12345
hash(12345) = 8732
8732 % 4 = 0  →  user_activity_0 partition
```

**Benefits:**
- ✅ **Even distribution:** No data skew or hotspots
- ✅ **Predictable:** Hash function deterministic
- ✅ **Scalable:** Add more partitions easily

---

#### Composite Partitioning

**Definition:**
**Combination of two partitioning methods** (e.g., Range + Hash, Range + List).

**Use Cases:**
- 📅 **Time + Load balancing:** Partition by year, sub-partition by hash
- 🌍 **Region + Time:** Partition by region, sub-partition by month

**Example: Range-Hash Composite**

```sql
CREATE TABLE sales (
    sale_id BIGINT,
    sale_date DATE,
    product_id INT,
    amount DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(sale_date))
SUBPARTITION BY HASH (product_id) SUBPARTITIONS 4 (
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025)
);
```

**Result:**
- 🗓️ **Level 1:** Partitioned by year (p2023, p2024)
- 🔢 **Level 2:** Each year partition split into 4 hash sub-partitions
- 📊 **Total partitions:** 2 years × 4 hash = 8 partitions

---

### 4. Advantages of Partitioning

#### Detailed Benefits

**1. Improved Query Performance**
- 📊 **Partition pruning:** Query optimizer skips irrelevant partitions
- ⚡ **Smaller scans:** Search 10M rows instead of 100M
- 🎯 **Index efficiency:** Smaller partition indexes fit in memory
- **Example:** Query for 2024 data scans only p2024 partition (90% reduction)

**2. Easier Maintenance**
- 🧹 **Partition-level operations:** VACUUM, ANALYZE per partition
- 💾 **Backup flexibility:** Backup recent partitions more frequently
- 🗑️ **Fast data deletion:** DROP PARTITION instead of DELETE
- **Example:** Drop 2020 partition in 1 second vs DELETE taking hours

**3. Efficient Archiving**
- 📦 **Cold storage:** Move old partitions to cheaper disks
- 🔄 **Data lifecycle:** Automate archiving based on age
- 💰 **Cost optimization:** Keep hot data on SSD, cold on HDD
- **Example:** Move partitions older than 1 year to archive storage

**4. Parallel Processing**
- 🔀 **Multiple partitions queried at once:** Utilize multiple CPU cores
- 🚀 **Faster aggregations:** SUM, COUNT, AVG across partitions
- ⚙️ **Concurrent maintenance:** VACUUM different partitions simultaneously
- **Example:** Aggregate 12 monthly partitions in parallel (12x speedup)

**5. Faster Indexing**
- 📇 **Smaller indexes:** Each partition has its own smaller index
- 💾 **Memory efficiency:** Indexes fit in buffer cache
- ⚡ **Faster lookups:** B-tree searches on smaller datasets
- **Example:** 1M-row partition index vs 100M-row table index

---

### 5. Limitations of Partitioning

#### Challenges and Considerations

| Limitation                        | Description                           | Mitigation                          |
| --------------------------------- | ------------------------------------- | ----------------------------------- |
| **Complex Queries**               | Joins across partitions may be slower | Use partition-wise joins            |
| **Management Overhead**           | Requires planning and maintenance     | Automate partition creation         |
| **Uneven Data Distribution**      | Skewed partitions hurt performance    | Choose partitioning key carefully   |
| **Not Ideal for Small Databases** | Overhead outweighs benefit            | Use only for tables > 10M rows      |
| **Key Selection Critical**        | Wrong key negates benefits            | Analyze query patterns first        |
| **Cross-partition Queries**       | May scan all partitions               | Include partition key in WHERE      |

**When NOT to Use Partitioning:**
- ❌ Tables < 10 million rows
- ❌ Queries don't filter on partition key
- ❌ Uniform access across all data
- ❌ Simple queries on small datasets

---

### 6. What Is Sharding?

#### Definition

**Sharding** is like partitioning — but the data is split **across multiple servers (nodes)** instead of one.

**Key Differences from Partitioning:**
- 🌐 **Multiple servers:** Each shard on different database instance
- 📈 **Horizontal scaling:** Add more servers to increase capacity
- 🔄 **Independent failure:** One shard failure doesn't affect others
- 🌍 **Geographic distribution:** Shards in different regions

**Sharding = Partitioning + Distributed Databases**

**Analogy:**
> 🧩 Think of sharding as *"horizontal scaling"* — when one machine can't handle the load, add more machines! Like having multiple warehouses in different cities instead of one giant warehouse.

---

#### Example Scenario

**Problem:**
You have **10 million customers** and queries are slow despite indexes. Your single database server is maxed out (CPU 90%, Memory 95%).

**Solution: Shard Across 5 Servers**

| Shard      | Data Range          | Server     | Rows   |
| ---------- | ------------------- | ---------- | ------ |
| Shard 1    | Customer IDs 1–2M   | DB-Server-A | 2M     |
| Shard 2    | Customer IDs 2M–4M  | DB-Server-B | 2M     |
| Shard 3    | Customer IDs 4M–6M  | DB-Server-C | 2M     |
| Shard 4    | Customer IDs 6M–8M  | DB-Server-D | 2M     |
| Shard 5    | Customer IDs 8M–10M | DB-Server-E | 2M     |

**Query Routing:**

When a query runs for `Customer_ID = 3200000`:
```
1. Application calculates: 3200000 falls in range 2M-4M
2. Route query to DB-Server-B (Shard 2)
3. Query executes on 2M rows instead of 10M
4. Result returned 5x faster
```

**Result:**
> 👉 Query directly goes to **Shard 2**, not all servers! This reduces load by 80% per server.

---

### 7. Types of Sharding

#### Sharding Strategies

**1. Range Sharding**

**Definition:** Divide data by **ranges of values** (similar to range partitioning).

**Example:**
```
Shard 1: User IDs 1 - 1,000,000
Shard 2: User IDs 1,000,001 - 2,000,000
Shard 3: User IDs 2,000,001 - 3,000,000
```

**Pros:**
- ✅ Simple to implement
- ✅ Easy to add new shards
- ✅ Range queries efficient (find users 100K-200K)

**Cons:**
- ❌ Data skew possible (newer users more active)
- ❌ Hotspots (Shard 3 gets more traffic)

---

**2. Hash Sharding**

**Definition:** Apply **hash function** to shard key, distribute evenly.

**Example:**
```java
int shardNumber = hash(userId) % 4;
switch(shardNumber) {
    case 0: connect(DB_Shard_A); break;
    case 1: connect(DB_Shard_B); break;
    case 2: connect(DB_Shard_C); break;
    case 3: connect(DB_Shard_D); break;
}
```

**Hash Function Example:**
```
user_id = 12345
hash(12345) = 8473628
8473628 % 4 = 0  →  Shard A

user_id = 67890
hash(67890) = 2938475
2938475 % 4 = 3  →  Shard D
```

**Pros:**
- ✅ **Even distribution:** No data skew
- ✅ **Load balanced:** Equal traffic to all shards
- ✅ **Predictable:** Same user always routes to same shard

**Cons:**
- ❌ **Resharding hard:** Changing shard count requires rehashing all data
- ❌ **Range queries difficult:** Finding users 100K-200K requires querying all shards

---

**3. Directory (Lookup) Sharding**

**Definition:** Use a **mapping table** that tells which shard contains which data.

**Example:**

**Shard Map Table:**
| User ID Range | Shard Server  |
| ------------- | ------------- |
| 1 - 500,000   | DB-Shard-A    |
| 500,001 - 1M  | DB-Shard-B    |
| 1M - 1.5M     | DB-Shard-C    |
| 1.5M - 2M     | DB-Shard-D    |

**Query Process:**
1. Look up `user_id = 750,000` in shard map
2. Find it belongs to DB-Shard-B
3. Route query to DB-Shard-B

**Pros:**
- ✅ **Flexible:** Easy to rebalance shards
- ✅ **No data rehashing:** Update mapping table only
- ✅ **Custom distribution:** Allocate based on actual load

**Cons:**
- ❌ **Lookup overhead:** Extra query to shard map
- ❌ **Single point of failure:** Shard map must be highly available
- ❌ **Complexity:** Maintain mapping service

---

**4. Geo Sharding (Geographic Sharding)**

**Definition:** Distribute data based on **geographic location** (country, region, continent).

**Example:**

| Shard           | Region           | Server Location | Users   |
| --------------- | ---------------- | --------------- | ------- |
| Shard Asia      | Asia-Pacific     | Singapore       | 3M      |
| Shard Europe    | EU               | Frankfurt       | 2M      |
| Shard Americas  | North/South America | US East       | 4M      |
| Shard Africa    | Africa           | South Africa    | 500K    |

**Benefits:**
- ✅ **Low latency:** Users connect to nearest shard
- ✅ **Compliance:** GDPR, data residency laws
- ✅ **Disaster recovery:** Regional isolation

**Use Cases:**
- 🌍 Global social networks (Facebook, Twitter)
- 🛒 E-commerce platforms (Amazon regional stores)
- 🎮 Gaming platforms (regional servers)

---

### 8. Differences Between Partitioning & Sharding

#### Comprehensive Comparison

| Feature            | Partitioning        | Sharding                         |
| ------------------ | ------------------- | -------------------------------- |
| **Scope**          | Within one database | Across multiple databases        |
| **Goal**           | Manage large tables | Scale system horizontally        |
| **Storage**        | Single server       | Multiple servers                 |
| **Complexity**     | Easier to implement | Harder (requires routing logic)  |
| **Failure Impact** | One server affected | One shard can fail independently |
| **Cost**           | Lower (one server)  | Higher (multiple servers)        |
| **Scalability**    | Vertical (bigger server) | Horizontal (more servers)    |
| **Management**     | Database handles it | Application layer routing        |
| **Joins**          | Easy across partitions | Hard across shards             |
| **Consistency**    | Strong (single DB)  | Eventual (distributed)           |

---

#### When to Use Each

**Use Partitioning When:**
- ✅ Table > 10M rows, single server can handle
- ✅ Query patterns filter on partition key
- ✅ Archiving/maintenance needs
- ✅ Time-series data (logs, events)

**Use Sharding When:**
- ✅ Data > single server capacity (hundreds of GB to TB)
- ✅ Need horizontal scalability
- ✅ Geographic distribution required
- ✅ High write throughput (millions/sec)
- ✅ Budget allows multiple servers

---

### 9. Real-World Examples

#### Industry Implementations

| Company       | Use of Partitioning/Sharding                        | Scale                          |
| ------------- | --------------------------------------------------- | ------------------------------ |
| **Instagram** | User media sharded by user_id (hash sharding)       | 2+ billion users               |
| **Netflix**   | Viewing history partitioned by region (geo-sharding)| 230M+ subscribers globally     |
| **YouTube**   | Videos sharded by content category and region       | 500+ hours uploaded per minute |
| **Twitter**   | Tweets distributed using hash sharding (Snowflake ID)| 500M+ tweets per day          |
| **Uber**      | Trip data sharded by city (geo-sharding)            | 100M+ trips per day            |
| **Airbnb**    | Listings sharded by geographic region               | 7M+ listings worldwide         |
| **Pinterest** | Pins sharded by user_id (hash sharding)             | 450M+ monthly users            |

---

#### Instagram Sharding Example

**Problem:**
- 📸 2 billion users uploading millions of photos daily
- 💾 Single database cannot handle write/read load

**Solution:**
```
Sharding Strategy: Hash sharding on user_id

Total Shards: 4,000+ shards
Algorithm: shard = user_id % 4000

Example:
user_id = 123456789
123456789 % 4000 = 1789
→ Route to Shard 1789
```

**Benefits:**
- ✅ Even distribution across all shards
- ✅ Each shard handles ~500K users
- ✅ Linear scalability (add more shards as users grow)

---

### 10. Sharding Architecture Example

#### System Architecture Diagram

```
                ┌────────────────────┐
                │ Application Server │
                │  (Web/Mobile App)  │
                └──────────┬─────────┘
                           │
                ┌──────────▼──────────┐
                │  Shard Router / API │
                │  (Routing Logic)    │
                └──────────┬──────────┘
                           │
         ┌─────────────────┼─────────────────┐
         │                 │                 │
   ┌─────▼─────┐    ┌──────▼──────┐   ┌─────▼─────┐
   │  Shard 1  │    │   Shard 2   │   │  Shard 3  │
   │ User 1–3M │    │ User 3M–6M  │   │ User 6–9M │
   │ DB-Server-A│   │ DB-Server-B │   │DB-Server-C│
   └───────────┘    └─────────────┘   └───────────┘
```

---

#### Routing Logic Implementation

**Application-Level Sharding (Python Example):**

```python
class ShardRouter:
    def __init__(self):
        self.shards = {
            0: "db-shard-a.example.com",
            1: "db-shard-b.example.com",
            2: "db-shard-c.example.com",
            3: "db-shard-d.example.com"
        }
        self.shard_count = len(self.shards)
    
    def get_shard(self, user_id):
        shard_id = hash(user_id) % self.shard_count
        return self.shards[shard_id]
    
    def query_user(self, user_id):
        shard_host = self.get_shard(user_id)
        connection = connect_to_db(shard_host)
        return connection.query(f"SELECT * FROM users WHERE id = {user_id}")

# Usage
router = ShardRouter()
user_data = router.query_user(12345)  # Automatically routes to correct shard
```

**Result:**
> ✅ The router ensures each query goes to the correct shard automatically, transparent to the application logic.

---

### 11. Combining Partitioning & Sharding

Large-scale systems often use **both strategies together**:

**Architecture:**
1. **Sharding** → Distribute data across multiple servers
2. **Partitioning** → Split tables inside each shard

---

#### Real-World Example: Global E-Commerce

**Level 1: Geo-Sharding (3 Shards)**
```
Shard US: Americas customers (Server: US-East)
Shard EU: European customers (Server: Frankfurt)
Shard APAC: Asia-Pacific customers (Server: Singapore)
```

**Level 2: Range Partitioning (per shard)**
```
Shard US:
  ├── orders_2022 (10M rows)
  ├── orders_2023 (12M rows)
  └── orders_2024 (15M rows)

Shard EU:
  ├── orders_2022 (8M rows)
  ├── orders_2023 (9M rows)
  └── orders_2024 (11M rows)
```

**Benefits:**
- 🌍 **Low latency:** Users query local shard
- 📅 **Fast queries:** Only relevant year partition scanned
- 🗑️ **Easy archiving:** Drop old partitions per shard
- 📈 **Scalability:** Add more shards or partitions independently

**Example Query:**
```sql
-- EU customer querying 2024 orders
-- 1. Routes to Shard EU (geo-sharding)
-- 2. Scans orders_2024 partition (range partitioning)
-- Result: Queries only 11M rows instead of 65M total
SELECT * FROM orders 
WHERE customer_id = 'EU_12345' 
  AND order_date >= '2024-01-01';
```

---

### 12. Monitoring & Maintenance

#### Key Maintenance Tasks

| Task                  | Description                             | Tools/Techniques                |
| --------------------- | --------------------------------------- | ------------------------------- |
| **Rebalancing**       | Move data evenly between shards         | Vitess, Citus, custom scripts   |
| **Resharding**        | Add/remove shards when capacity changes | Consistent hashing, downtime    |
| **Index Maintenance** | Keep partition-level indexes updated    | Automatic with partitioning     |
| **Query Routing**     | Middleware or app layer routes requests | Application logic, proxy layer  |
| **Monitoring Skew**   | Detect uneven data distribution         | Metrics, alerting               |
| **Backup Strategy**   | Partition/shard-level backups           | Incremental backups             |

---

#### Rebalancing Data

**When Needed:**
- 📊 Shard size imbalance (Shard A: 10M rows, Shard B: 50M rows)
- 🔥 Hotspot detection (Shard C getting 80% of traffic)
- 📈 Adding new shards

**Strategies:**
1. **Live migration:** Move data while serving queries
2. **Double-write:** Write to both old and new shard during migration
3. **Background sync:** Copy data asynchronously

---

#### Resharding

**Definition:** Reorganizing shards when capacity changes (add/remove shards).

**Example: Growing from 4 to 8 Shards**

**Before (4 shards):**
```
hash(user_id) % 4
Shard 0, 1, 2, 3
```

**After (8 shards):**
```
hash(user_id) % 8
Shard 0, 1, 2, 3, 4, 5, 6, 7
```

**Challenge:** 50% of data needs to move to new shards!

**Solution: Consistent Hashing**
- Minimizes data movement (only ~12.5% moves per shard added)
- Gradual migration without downtime

---

#### Automation Tools

**Vitess (used by YouTube, Slack):**
- ✅ Automatic sharding for MySQL
- ✅ Query routing and connection pooling
- ✅ Resharding automation
- ✅ Monitoring and health checks

**Citus (PostgreSQL extension):**
- ✅ Distributed PostgreSQL
- ✅ Automatic shard distribution
- ✅ Parallel query execution
- ✅ Reference tables (replicated to all shards)

**MongoDB Sharding Manager:**
- ✅ Built-in sharding support
- ✅ Automatic balancing
- ✅ Chunk-based data distribution
- ✅ Config server for metadata

---

### 13. Performance Benefits

#### Quantified Improvements

**Query Performance:**
- ✅ **5-10x faster queries** (partition pruning)
- ✅ **Parallel execution** across multiple partitions/shards
- ✅ **Smaller indexes** fit in memory
- ✅ **Reduced I/O** (scan less data)

**Scalability:**
- ✅ **Horizontal scaling:** Add more servers linearly
- ✅ **Write throughput:** Distribute writes across shards
- ✅ **Read throughput:** Serve from multiple replicas per shard
- ✅ **Storage capacity:** Unlimited (add more shards)

**Availability:**
- ✅ **High availability:** One shard failure doesn't affect others
- ✅ **Isolated failures:** Limit blast radius
- ✅ **Maintenance windows:** Update shards independently
- ✅ **Disaster recovery:** Regional shards isolated

**Maintenance:**
- ✅ **Faster VACUUM/ANALYZE:** Per partition (minutes vs hours)
- ✅ **Instant archiving:** DROP PARTITION (seconds vs hours)
- ✅ **Parallel backups:** Backup multiple shards simultaneously
- ✅ **Reduced locking:** Smaller partition locks

---

### 14. Common Challenges

#### Sharding Challenges and Solutions

| Challenge               | Description                                | Solution                               |
| ----------------------- | ------------------------------------------ | -------------------------------------- |
| **Cross-Shard Joins**   | Hard to join data across shards            | Denormalize data, application-level joins |
| **Distributed Transactions** | ACID across shards complex           | Avoid when possible, use sagas         |
| **Resharding Overhead** | Moving data between servers costly         | Consistent hashing, gradual migration  |
| **Inconsistent Data**   | Requires strong coordination               | Eventual consistency model             |
| **Complex App Logic**   | Application must know which shard to query | Shard-aware routing layer              |
| **Operational Overhead**| Managing many databases                    | Automation tools (Vitess, Kubernetes)  |
| **Monitoring Complexity**| Track health of many shards               | Centralized monitoring (Prometheus)    |
| **Backup/Restore**      | Coordinating across shards                 | Consistent snapshots, point-in-time    |

---

#### Cross-Shard Queries

**Problem:**
```sql
-- This query requires data from multiple shards
SELECT u.name, o.total
FROM users u
JOIN orders o ON u.user_id = o.user_id
WHERE u.country = 'USA';

-- Users sharded by user_id
-- Orders sharded by order_id
-- Cannot efficiently join across shards
```

**Solutions:**

**1. Denormalization:**
```sql
-- Store user info in orders table
CREATE TABLE orders (
    order_id BIGINT,
    user_id BIGINT,
    user_name VARCHAR(100),  -- Denormalized
    user_country VARCHAR(50), -- Denormalized
    total DECIMAL(10,2)
);
```

**2. Application-Level Joins:**
```python
# Query both shards separately, join in application
users = query_user_shard("SELECT * FROM users WHERE country='USA'")
orders = query_order_shard("SELECT * FROM orders WHERE user_id IN (...)")
result = join_in_memory(users, orders)
```

**3. Shard by Same Key:**
```sql
-- Shard both tables by user_id
-- users: hash(user_id) % 4
-- orders: hash(user_id) % 4
-- Now joins can execute within single shard
```

---

### 15. Interview Questions & Answers

**1. What's the difference between partitioning and sharding?**

**Answer:**
- **Partitioning:** Splits a large table into smaller pieces within **one database server**. Used for managing large tables and improving query performance.
- **Sharding:** Distributes data across **multiple database servers**. Used for horizontal scaling when one server can't handle the load.

**Key Difference:** Partitioning = one server, Sharding = multiple servers.

---

**2. How does hash partitioning work?**

**Answer:**
Hash partitioning applies a hash function to the partitioning key and uses modulo to determine which partition stores the data:
```
partition_number = hash(partition_key) % number_of_partitions
```
**Example:**
```
user_id = 12345
hash(12345) = 8732
8732 % 4 = 0  →  Partition 0
```
**Benefits:** Even data distribution, no hotspots, load balanced.

---

**3. What is resharding and when is it needed?**

**Answer:**
**Resharding** is reorganizing shards by adding or removing shard servers. Needed when:
- 📈 Data growth exceeds current shard capacity
- 📊 Uneven data distribution (hotspots)
- 🔧 Performance degradation
- 💰 Cost optimization (consolidate under-utilized shards)

**Challenge:** Requires data movement and can cause downtime if not done carefully. Use consistent hashing to minimize impact.

---

**4. Why do large applications like Netflix use sharding?**

**Answer:**
Netflix has 230M+ subscribers worldwide generating massive data:
- 🌍 **Global scale:** Single database can't handle worldwide traffic
- 📍 **Low latency:** Geo-sharding serves users from nearest region
- 📈 **Horizontal scaling:** Add more shards as users grow
- 🛡️ **Fault isolation:** Regional shard failure doesn't affect others
- 💰 **Cost efficiency:** Scale specific regions independently

---

**5. What happens if a shard fails?**

**Answer:**
**Impact:**
- ❌ Data on failed shard is unavailable
- ❌ Queries to that shard fail
- ✅ Other shards continue operating normally (isolated failure)

**Solutions:**
- 🔄 **Replication:** Each shard has replicas (promote replica if primary fails)
- 🔁 **Failover:** Automatic switchover to replica (10-60 seconds)
- 💾 **Backup:** Restore from backup (minutes to hours)
- 📊 **Monitoring:** Detect failures quickly and alert

**Best Practice:** Combine sharding with replication for high availability.

---

**6. How can you combine sharding and partitioning together?**

**Answer:**
Use **sharding for horizontal scaling** across servers and **partitioning for table management** within each shard.

**Example:**
```
Level 1: Geo-sharding (3 shards: US, EU, APAC)
Level 2: Range partitioning per shard (by year)

Shard US:
  ├── orders_2022
  ├── orders_2023
  └── orders_2024

Shard EU:
  ├── orders_2022
  ├── orders_2023
  └── orders_2024
```

**Benefits:** Low latency (geo-sharding) + fast queries (partitioning) + easy archiving.

---

### 16. Summary Table

| Concept                | Description                                 | Key Benefit                    |
| ---------------------- | ------------------------------------------- | ------------------------------ |
| **Partitioning**       | Divide table data within one DB             | Query performance, maintenance |
| **Sharding**           | Split data across multiple DB servers       | Horizontal scalability         |
| **Range Partitioning** | Based on data range (dates, IDs)           | Time-series, archiving         |
| **List Partitioning**  | Based on discrete values (regions, status) | Categorical data, compliance   |
| **Hash Partitioning**  | Based on hash key (even distribution)      | Load balancing, no hotspots    |
| **Composite**          | Combination of two types                    | Complex requirements           |
| **Resharding**         | Reorganizing shards as data grows           | Capacity management            |
| **Partition Pruning**  | Skip irrelevant partitions                  | 5-10x faster queries           |
| **Geo-Sharding**       | Shard by geographic region                  | Low latency, compliance        |
| **Read Replica**       | Replicate each shard for reads              | Read scalability per shard     |

---

### 17. Final Thought

> **"Partitioning organizes your data;**
> **Sharding scales your system."**
> 
> Together, they make databases ready for **big-data, real-time, and global applications** 🌎.

**Key Takeaways:**
- 📊 **Partitioning:** Optimize large tables on single server
- 🌐 **Sharding:** Scale beyond single server limits
- 🎯 **Choose wisely:** Right strategy depends on data size, query patterns, budget
- 🛠️ **Automate:** Use tools like Vitess, Citus, MongoDB sharding
- 📈 **Plan ahead:** Consider growth and resharding challenges
- 🔄 **Combine with replication:** High availability + scalability

**When to Start:**
- Partitioning: Tables > 10M rows
- Sharding: Database > 100GB, single server at capacity
''',
    codeSnippet: '''
-- ========================================
-- 1. RANGE PARTITIONING (MySQL)
-- ========================================

-- Create table with range partitioning by year
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    order_date DATE NOT NULL,
    customer_id INT,
    total DECIMAL(10,2),
    status VARCHAR(20)
)
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Insert data (automatically goes to correct partition)
INSERT INTO orders VALUES 
(1, '2022-05-15', 101, 250.00, 'completed'),
(2, '2023-08-20', 102, 180.00, 'shipped'),
(3, '2024-01-10', 103, 320.00, 'pending');

-- Query specific partition (partition pruning)
EXPLAIN SELECT * FROM orders 
WHERE order_date BETWEEN '2024-01-01' AND '2024-12-31';
-- Only p2024 partition scanned!

-- Drop old partition (instant archiving)
ALTER TABLE orders DROP PARTITION p2020;

-- Add new partition
ALTER TABLE orders ADD PARTITION (
    PARTITION p2026 VALUES LESS THAN (2027)
);

-- ========================================
-- 2. LIST PARTITIONING (PostgreSQL)
-- ========================================

-- Create table with list partitioning by region
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    region VARCHAR(20)
) PARTITION BY LIST (region);

-- Create partitions for each region
CREATE TABLE customers_asia PARTITION OF customers
    FOR VALUES IN ('Asia', 'APAC', 'India', 'China', 'Japan');

CREATE TABLE customers_europe PARTITION OF customers
    FOR VALUES IN ('Europe', 'EU', 'UK', 'Germany', 'France');

CREATE TABLE customers_americas PARTITION OF customers
    FOR VALUES IN ('Americas', 'USA', 'Canada', 'Brazil');

CREATE TABLE customers_other PARTITION OF customers
    DEFAULT;

-- Insert data
INSERT INTO customers VALUES 
(1, 'Alice', 'alice@example.com', 'USA'),
(2, 'Bob', 'bob@example.com', 'UK'),
(3, 'Charlie', 'charlie@example.com', 'India');

-- Query specific region (only customers_americas scanned)
SELECT * FROM customers WHERE region = 'USA';

-- ========================================
-- 3. HASH PARTITIONING (PostgreSQL)
-- ========================================

-- Create table with hash partitioning
CREATE TABLE user_activity (
    user_id BIGINT,
    activity_type VARCHAR(50),
    activity_date TIMESTAMP,
    details JSONB,
    PRIMARY KEY (user_id, activity_date)
) PARTITION BY HASH (user_id);

-- Create 4 hash partitions
CREATE TABLE user_activity_0 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);

CREATE TABLE user_activity_1 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);

CREATE TABLE user_activity_2 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 2);

CREATE TABLE user_activity_3 PARTITION OF user_activity
    FOR VALUES WITH (MODULUS 4, REMAINDER 3);

-- Insert data (automatically distributed evenly)
INSERT INTO user_activity VALUES 
(12345, 'login', NOW(), '{"ip": "192.168.1.1"}'),
(67890, 'purchase', NOW(), '{"amount": 99.99}'),
(11111, 'logout', NOW(), '{"duration": 3600}');

-- ========================================
-- 4. COMPOSITE PARTITIONING (MySQL)
-- ========================================

-- Range partitioning + Hash subpartitioning
CREATE TABLE sales (
    sale_id BIGINT,
    sale_date DATE,
    product_id INT,
    amount DECIMAL(10,2),
    PRIMARY KEY (sale_id, sale_date)
)
PARTITION BY RANGE (YEAR(sale_date))
SUBPARTITION BY HASH (product_id)
SUBPARTITIONS 4 (
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026)
);

-- Result: 3 year partitions × 4 hash subpartitions = 12 total partitions

-- ========================================
-- 5. VIEW PARTITION INFORMATION
-- ========================================

-- MySQL: View partition details
SELECT 
    PARTITION_NAME,
    PARTITION_METHOD,
    PARTITION_EXPRESSION,
    TABLE_ROWS,
    DATA_LENGTH / 1024 / 1024 AS size_mb
FROM information_schema.PARTITIONS
WHERE TABLE_SCHEMA = 'mydb' AND TABLE_NAME = 'orders';

-- PostgreSQL: View partition information
SELECT 
    inhrelid::regclass AS partition_name,
    pg_size_pretty(pg_total_relation_size(inhrelid)) AS size
FROM pg_inherits
WHERE inhparent = 'customers'::regclass;

-- ========================================
-- 6. SHARDING ROUTING LOGIC (Application-Level)
-- ========================================

-- Python example: Hash sharding router
-- class ShardRouter:
--     def __init__(self):
--         self.shards = {
--             0: "mysql://db-shard-a:3306/mydb",
--             1: "mysql://db-shard-b:3306/mydb",
--             2: "mysql://db-shard-c:3306/mydb",
--             3: "mysql://db-shard-d:3306/mydb"
--         }
--         self.shard_count = len(self.shards)
--     
--     def get_shard(self, user_id):
--         shard_id = hash(user_id) % self.shard_count
--         return self.shards[shard_id]
--     
--     def query_user(self, user_id):
--         shard_url = self.get_shard(user_id)
--         connection = create_connection(shard_url)
--         return connection.execute(
--             f"SELECT * FROM users WHERE user_id = {user_id}"
--         )

-- ========================================
-- 7. RANGE SHARDING CONFIGURATION
-- ========================================

-- Shard mapping table (directory-based sharding)
CREATE TABLE shard_map (
    shard_id INT PRIMARY KEY,
    min_user_id BIGINT,
    max_user_id BIGINT,
    shard_host VARCHAR(100),
    shard_port INT
);

INSERT INTO shard_map VALUES
(1, 1, 1000000, 'db-shard-a.example.com', 3306),
(2, 1000001, 2000000, 'db-shard-b.example.com', 3306),
(3, 2000001, 3000000, 'db-shard-c.example.com', 3306),
(4, 3000001, 4000000, 'db-shard-d.example.com', 3306);

-- Query to find shard for user
SELECT shard_host, shard_port 
FROM shard_map 
WHERE 1500000 BETWEEN min_user_id AND max_user_id;
-- Returns: db-shard-b.example.com, 3306

-- ========================================
-- 8. PARTITION MAINTENANCE
-- ========================================

-- Add new monthly partition (automated with cron)
ALTER TABLE orders ADD PARTITION (
    PARTITION p2025_01 VALUES LESS THAN ('2025-02-01')
);

-- Reorganize partition (split)
ALTER TABLE orders REORGANIZE PARTITION p_future INTO (
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Analyze partition (update statistics)
ANALYZE TABLE orders PARTITION (p2024);

-- Optimize partition
OPTIMIZE TABLE orders PARTITION (p2023, p2024);

-- ========================================
-- 9. PARTITION PRUNING VERIFICATION
-- ========================================

-- Check if partition pruning is working
EXPLAIN PARTITIONS
SELECT * FROM orders 
WHERE order_date = '2024-06-15';
-- Should show: partitions: p2024 (only one partition)

-- Bad query (scans all partitions)
EXPLAIN PARTITIONS
SELECT * FROM orders 
WHERE customer_id = 101;
-- Shows: partitions: p2020,p2021,p2022,p2023,p2024,p_future

-- ========================================
-- 10. HASH SHARDING EXAMPLE (MongoDB)
-- ========================================

-- Enable sharding for database
-- sh.enableSharding("mydb")

-- Shard collection using hash sharding
-- sh.shardCollection("mydb.users", { user_id: "hashed" })

-- MongoDB automatically distributes data across shards
-- db.users.insert({ user_id: 12345, name: "Alice", email: "alice@example.com" })
-- db.users.insert({ user_id: 67890, name: "Bob", email: "bob@example.com" })

-- Check shard distribution
-- db.users.getShardDistribution()

-- ========================================
-- 11. GEO-SHARDING CONFIGURATION
-- ========================================

-- Shard configuration for geo-distributed system
CREATE TABLE geo_shard_config (
    region VARCHAR(20) PRIMARY KEY,
    shard_host VARCHAR(100),
    shard_port INT,
    is_active BOOLEAN DEFAULT TRUE
);

INSERT INTO geo_shard_config VALUES
('US', 'db-us-east.example.com', 5432, TRUE),
('EU', 'db-eu-central.example.com', 5432, TRUE),
('APAC', 'db-apac-southeast.example.com', 5432, TRUE);

-- Routing logic: Select shard based on user region
SELECT shard_host FROM geo_shard_config 
WHERE region = 'EU' AND is_active = TRUE;

-- ========================================
-- 12. PARTITION-LEVEL BACKUP
-- ========================================

-- Backup specific partition (MySQL)
-- mysqldump --no-create-info mydb orders --where="order_date >= '2024-01-01' AND order_date < '2025-01-01'" > orders_2024.sql

-- PostgreSQL: Export partition
-- pg_dump -t customers_asia mydb > customers_asia_backup.sql

-- ========================================
-- 13. MONITORING PARTITION SIZES
-- ========================================

-- Monitor partition growth (MySQL)
SELECT 
    PARTITION_NAME,
    TABLE_ROWS,
    ROUND(DATA_LENGTH / 1024 / 1024, 2) AS data_mb,
    ROUND(INDEX_LENGTH / 1024 / 1024, 2) AS index_mb,
    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS total_mb
FROM information_schema.PARTITIONS
WHERE TABLE_SCHEMA = 'mydb' 
  AND TABLE_NAME = 'orders'
ORDER BY PARTITION_NAME;

-- PostgreSQL: Monitor partition sizes
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE tablename LIKE 'customers_%'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- ========================================
-- 14. AUTOMATIC PARTITION CREATION
-- ========================================

-- PostgreSQL: Function to auto-create monthly partitions
CREATE OR REPLACE FUNCTION create_monthly_partition()
RETURNS void AS \$\$
DECLARE
    partition_date DATE;
    partition_name TEXT;
    start_date DATE;
    end_date DATE;
BEGIN
    partition_date := DATE_TRUNC('month', CURRENT_DATE + INTERVAL '1 month');
    partition_name := 'orders_' || TO_CHAR(partition_date, 'YYYY_MM');
    start_date := partition_date;
    end_date := partition_date + INTERVAL '1 month';
    
    EXECUTE FORMAT(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF orders
         FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );
END;
\$\$ LANGUAGE plpgsql;

-- Schedule with cron: SELECT create_monthly_partition();

-- ========================================
-- 15. CROSS-SHARD QUERY (Application-Level Join)
-- ========================================

-- Problem: Join data across shards
-- Solution 1: Denormalize
CREATE TABLE orders_denormalized (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT,
    user_name VARCHAR(100),  -- Denormalized from users table
    user_email VARCHAR(100), -- Denormalized from users table
    product_name VARCHAR(100), -- Denormalized from products table
    total DECIMAL(10,2)
);

-- Solution 2: Application-level join (pseudo-code)
-- users = query_shard_1("SELECT * FROM users WHERE country='USA'")
-- orders = query_shard_2("SELECT * FROM orders WHERE user_id IN (...)")
-- result = merge_in_application(users, orders)

-- ========================================
-- 16. RESHARDING EXAMPLE
-- ========================================

-- Before: 4 shards (hash % 4)
-- After: 8 shards (hash % 8)

-- Migration strategy (double-write approach)
-- 1. Add new shards (5, 6, 7, 8)
-- 2. Start double-writing to both old and new shards
-- 3. Background copy existing data to new distribution
-- 4. Verify data consistency
-- 5. Switch reads to new shards
-- 6. Stop writing to old shards
-- 7. Remove old shards

-- ========================================
-- 17. PARTITION STATISTICS
-- ========================================

-- Check query performance improvement
SET @start = NOW();
SELECT COUNT(*) FROM orders WHERE order_date >= '2024-01-01';
SET @end = NOW();
SELECT TIMESTAMPDIFF(MICROSECOND, @start, @end) / 1000 AS query_time_ms;

-- Compare with full table scan
SET @start = NOW();
SELECT COUNT(*) FROM orders_unpartitioned;
SET @end = NOW();
SELECT TIMESTAMPDIFF(MICROSECOND, @start, @end) / 1000 AS query_time_ms;
''',
    revisionPoints: [
      'Partitioning divides large table into smaller pieces within same database server',
      'Sharding distributes data across multiple database servers for horizontal scaling',
      'Range partitioning divides data by value ranges (dates, IDs) - ideal for time-series data',
      'List partitioning divides data by discrete values (regions, categories, status)',
      'Hash partitioning uses hash function for even data distribution across partitions',
      'Composite partitioning combines two methods (e.g., Range + Hash) for complex needs',
      'Partition pruning skips irrelevant partitions, making queries 5-10x faster',
      'Partitioning benefits: improved query performance, easier maintenance, efficient archiving, parallel processing',
      'Sharding types: Range, Hash, Directory (Lookup), Geo (Geographic)',
      'Hash sharding formula: shard_number = hash(shard_key) % number_of_shards',
      'Geo-sharding distributes data by geographic location for low latency and compliance',
      'Partitioning vs Sharding: Partitioning = one server, Sharding = multiple servers',
      'Combining both: Sharding for horizontal scaling + Partitioning within each shard',
      'Resharding reorganizes shards when adding/removing servers (requires data movement)',
      'Cross-shard joins are challenging - solutions: denormalization, application-level joins, shard by same key',
      'Partition maintenance: DROP PARTITION for instant archiving, ADD PARTITION for new ranges',
      'Sharding challenges: distributed transactions, cross-shard queries, operational overhead',
      'Real-world examples: Instagram (user_id hash sharding), Netflix (geo-sharding), YouTube (category sharding)',
      'Automation tools: Vitess (MySQL), Citus (PostgreSQL), MongoDB Sharding Manager',
      'Best practice: Use partitioning for tables > 10M rows, sharding when database > 100GB',
      'Monitoring: Track partition sizes, shard distribution, query performance per partition',
      'Directory sharding uses mapping table for flexible shard assignment',
      'Consistent hashing minimizes data movement during resharding (only ~12.5% per shard added)',
      'Partition-level operations: VACUUM, ANALYZE, BACKUP execute faster on smaller partitions',
      'Final principle: Partitioning organizes data, Sharding scales system',
    ],
    quizQuestions: [
      Question(
        question: 'What is the difference between partitioning and sharding?',
        options: ['No difference', 'Partitioning is within one server, sharding across multiple', 'Sharding is faster', 'Partitioning is newer'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which partitioning type is best for time-series data?',
        options: ['Hash Partitioning', 'Range Partitioning', 'List Partitioning', 'Round Robin'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is partition pruning?',
        options: [
          'Deleting old partitions',
          'Skipping irrelevant partitions during query execution',
          'Compressing partition data',
          'Merging partitions'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Hash sharding formula is:',
        options: [
          'shard = key + number_of_shards',
          'shard = hash(key) % number_of_shards',
          'shard = key / number_of_shards',
          'shard = key * number_of_shards'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is geo-sharding used for?',
        options: [
          'Data compression',
          'Low latency by serving users from nearest region',
          'Faster indexing',
          'Better security'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which company uses hash sharding for user media?',
        options: ['Netflix', 'Instagram', 'YouTube', 'Twitter'],
        correctIndex: 1,
      ),
      Question(
        question: 'What is resharding?',
        options: [
          'Backing up shards',
          'Reorganizing shards when adding/removing servers',
          'Encrypting shard data',
          'Compressing shards'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a major challenge with sharding?',
        options: [
          'Faster queries',
          'Cross-shard joins are difficult',
          'Lower cost',
          'Simpler architecture'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'When should you start using partitioning?',
        options: ['Tables > 1,000 rows', 'Tables > 100,000 rows', 'Tables > 10 million rows', 'Always'],
        correctIndex: 2,
      ),
      Question(
        question: 'What tool automates MySQL sharding?',
        options: ['Patroni', 'Vitess', 'HAProxy', 'Redis Sentinel'],
        correctIndex: 1,
      ),
      Question(
        question: 'Which partitioning ensures even data distribution?',
        options: ['Range', 'List', 'Hash', 'Composite'],
        correctIndex: 2,
      ),
      Question(
        question: 'What is composite partitioning?',
        options: [
          'Partitioning compressed data',
          'Combination of two partitioning types',
          'Partitioning multiple tables',
          'Backup partitioning'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'query_profiling',
    title: '20. Query Profiling & Optimization',
    explanation: '''## Query Profiling & Optimization

### 1. Introduction

Database queries are the heart of any application.
But as data grows, queries that once ran fast may start taking seconds — or even minutes.

That's where **Query Profiling & Optimization** comes in.

**Definition:**
> 🧠 **Query Profiling** means **analyzing how a SQL query executes internally** to identify bottlenecks and inefficiencies.
> 
> 🚀 **Query Optimization** means **rewriting or tuning queries** for **better speed and efficiency**.

**The Problem:**
- 📊 As tables grow from thousands to millions of rows, queries slow down
- ⏱️ What took 50ms now takes 5 seconds
- 🔥 Database CPU and memory usage spike
- 😰 Users experience slow page loads and timeouts

**The Solution:**
- 🔍 Profile queries to understand execution patterns
- ⚡ Optimize using indexes, query rewriting, and execution plan analysis
- 📈 Transform slow queries into fast, efficient ones

**Analogy:**
> 💡 Think of query profiling like a **performance report card** for your SQL queries. It tells you exactly where time is being wasted — like finding a needle in a haystack by checking which part of the haystack you're searching in!

---

### 2. Why Query Optimization Matters

#### Business Impact

| Reason                       | Explanation                                                      | Example Impact                     |
| ---------------------------- | ---------------------------------------------------------------- | ---------------------------------- |
| 🚀 **Faster Response Time**  | Optimized queries return results in milliseconds, not seconds    | Page load: 5s → 0.2s               |
| 💾 **Lower Resource Usage**  | Reduces CPU, RAM, and disk I/O load                              | Server cost: \$500/mo → \$150/mo    |
| ⚡ **High Scalability**       | Database handles thousands of concurrent users smoothly          | Support 10K users instead of 500   |
| 🔁 **Better Throughput**     | More queries execute simultaneously without blocking             | 100 TPS → 1000 TPS                 |
| 💸 **Cost Efficiency**       | Saves server costs on cloud-hosted databases (AWS RDS, Azure)    | AWS bill: \$2000/mo → \$800/mo      |
| 😊 **Better User Experience** | Fast queries mean happy users and lower bounce rates             | User satisfaction: 60% → 95%       |

**Real-World Impact:**
- **E-commerce:** 100ms delay = 1% revenue loss
- **Social media:** Slow feeds = users leave
- **Banking:** Query timeouts = transaction failures

---

### 3. What Is Query Profiling?

**Query Profiling** means inspecting the **query execution plan** to understand:

- ✅ Which tables are scanned (full table scan vs index scan)
- ✅ Which indexes are used (or not used)
- ✅ How data is joined or filtered (nested loops, hash joins, merge joins)
- ✅ How much time each operation takes (parsing, execution, sorting, sending data)
- ✅ How many rows are examined vs returned (efficiency ratio)

---

#### MySQL EXPLAIN Example

```sql
EXPLAIN SELECT name, salary 
FROM employees 
WHERE department = 'Sales';
```

**Output:**

| id | select_type | table     | type | possible_keys | key      | rows | Extra       |
| -- | ----------- | --------- | ---- | ------------- | -------- | ---- | ----------- |
| 1  | SIMPLE      | employees | ref  | dept_idx      | dept_idx | 1200 | Using where |

**Interpretation:**
- ✅ **type = ref:** Index lookup (good performance!)
- ✅ **key = dept_idx:** Using the department index
- ✅ **rows = 1200:** Only scanning 1200 rows instead of millions
- ✅ **Extra = Using where:** Filtering applied efficiently

**Bad Example (Full Table Scan):**

| id | select_type | table     | type | possible_keys | key  | rows    | Extra       |
| -- | ----------- | --------- | ---- | ------------- | ---- | ------- | ----------- |
| 1  | SIMPLE      | employees | ALL  | NULL          | NULL | 5000000 | Using where |

**Problems:**
- ❌ **type = ALL:** Full table scan (very slow!)
- ❌ **key = NULL:** No index used
- ❌ **rows = 5000000:** Scanning entire table
- ❌ This query needs optimization immediately!

---

### 4. Query Execution Steps

Every SQL query goes through these stages:

**1. Parsing (Syntax Check)**
- Database checks if SQL syntax is valid
- Converts SQL text into internal structure (parse tree)
- **Time:** Usually 1-5ms

**2. Optimization (Query Planning)**
- Query optimizer generates multiple execution plans
- Chooses best plan based on table statistics, indexes, join order
- **Time:** 5-50ms (complex queries may take longer)

**3. Execution (Running the Plan)**
- Database executes the chosen plan
- Reads data from disk/memory, applies filters, performs joins
- **Time:** Varies widely (10ms to minutes depending on query)

**4. Fetching Results (Data Retrieval)**
- Results are formatted and sent to client
- May involve sorting, grouping, or limiting
- **Time:** Depends on result size (small datasets fast, large slow)

**Profiling Goal:**
> 💡 Identify which stage takes the most time and optimize that specific bottleneck.

---

### 5. Key Performance Metrics

#### Query Metrics Table

| Metric                              | Meaning                                           | Good Value        | Bad Value      |
| ----------------------------------- | ------------------------------------------------- | ----------------- | -------------- |
| **Query Execution Time**            | Total time from query submission to result return | < 100ms           | > 2 seconds    |
| **Rows Examined vs. Rows Returned** | Efficiency ratio (lower is better)                | Close to 1:1      | 1000:1 or more |
| **Index Usage**                     | Whether query used index or full table scan       | Index scan        | Full scan      |
| **Temporary Tables**                | Extra tables created in memory/disk for sorting   | 0                 | Multiple       |
| **Sort Operations**                 | Sorting increases time and CPU cost               | Using index       | Filesort       |
| **Join Type**                       | Method used to join tables                        | Using index       | ALL (full scan)|
| **Lock Wait Time**                  | Time spent waiting for locks                      | < 10ms            | > 500ms        |

**Efficiency Ratio Example:**
```
Query examines 1,000,000 rows but returns only 10 rows
Efficiency ratio: 1,000,000:10 = 100,000:1 (very inefficient!)

After adding index:
Query examines 15 rows and returns 10 rows
Efficiency ratio: 15:10 = 1.5:1 (excellent!)
```

---

### 6. Tools for Query Profiling

#### Profiling Tools Comparison

| Tool                          | Supported DB      | Purpose                           | Output Format           |
| ----------------------------- | ----------------- | --------------------------------- | ----------------------- |
| **EXPLAIN**                   | MySQL, PostgreSQL | View execution plan (estimated)   | Table format            |
| **EXPLAIN ANALYZE**           | PostgreSQL        | Actual execution with real timing | Detailed tree           |
| **SHOW PROFILE**              | MySQL             | Check time per execution stage    | Stage breakdown         |
| **pg_stat_statements**        | PostgreSQL        | Tracks all query statistics       | Summary statistics      |
| **Performance Schema**        | MySQL             | Low-level performance details     | System tables           |
| **SQL Server Profiler**       | SQL Server        | GUI-based performance analysis    | Visual timeline         |
| **Oracle SQL Trace**          | Oracle DB         | SQL runtime tracking              | Trace files             |
| **Slow Query Log**            | MySQL, PostgreSQL | Logs queries exceeding threshold  | Log file                |
| **Query Store**               | SQL Server        | Tracks query plans over time      | Historical data         |

---

### 7. PostgreSQL EXPLAIN ANALYZE Example

```sql
EXPLAIN ANALYZE
SELECT c.customer_name, o.total_amount
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.order_date > '2024-01-01';
```

**Output:**

```
Hash Join  (cost=450.00..8234.56 rows=12500 width=44) (actual time=12.456..145.678 rows=12234 loops=1)
  Hash Cond: (o.customer_id = c.id)
  ->  Seq Scan on orders o  (cost=0.00..6500.00 rows=50000 width=24) (actual time=0.023..85.234 rows=48567 loops=1)
        Filter: (order_date > '2024-01-01'::date)
        Rows Removed by Filter: 151433
  ->  Hash  (cost=300.00..300.00 rows=12000 width=28) (actual time=12.234..12.234 rows=12000 loops=1)
        Buckets: 16384  Batches: 1  Memory Usage: 789kB
        ->  Seq Scan on customers c  (cost=0.00..300.00 rows=12000 width=28) (actual time=0.012..5.678 rows=12000 loops=1)
Planning Time: 2.345 ms
Execution Time: 147.890 ms
```

**Key Insights:**
- ✅ **Hash Join:** Efficient join strategy
- ✅ **Execution Time:** 147.89ms (acceptable)
- ⚠️ **Rows Removed by Filter:** 151,433 (could benefit from index on order_date)
- ✅ **Memory Usage:** 789kB (fits in memory)

**Optimization Opportunity:**
```sql
CREATE INDEX idx_order_date ON orders(order_date);
-- Reduces rows scanned from 200,000 to 48,567 directly
```

---

### 8. Query Optimization Techniques

#### Technique 1: Use Indexes Properly

**Problem:**
```sql
-- Slow: Full table scan on 10 million rows
SELECT * FROM orders WHERE customer_id = 12345;
-- Execution time: 8.5 seconds
```

**Solution:**
```sql
CREATE INDEX idx_customer_id ON orders(customer_id);

-- Now fast: Index lookup
SELECT * FROM orders WHERE customer_id = 12345;
-- Execution time: 0.023 seconds (370x faster!)
```

**Index Types:**
- **B-Tree Index:** Default, good for most queries (=, <, >, BETWEEN, LIKE 'prefix%')
- **Hash Index:** Equality checks only (=)
- **Composite Index:** Multiple columns (customer_id, order_date)
- **Covering Index:** Includes all query columns (no table lookup needed)

---

#### Technique 2: Avoid SELECT *

**Problem:**
```sql
-- ❌ Bad: Fetches all 50 columns
SELECT * FROM products;
-- Transfers 500MB of data over network
-- Execution time: 12 seconds
```

**Solution:**
```sql
-- ✅ Good: Fetches only 2 columns
SELECT product_name, price FROM products;
-- Transfers 5MB of data
-- Execution time: 0.8 seconds (15x faster!)
```

**Why It Matters:**
- 📉 Reduces I/O (disk reads)
- 📉 Reduces network transfer
- 📉 Reduces memory usage
- ✅ Only fetch what you actually need

---

#### Technique 3: Use Joins Instead of Subqueries

**Problem (Slow Subquery):**
```sql
-- ❌ Slow: Subquery executed for each row
SELECT name FROM employees
WHERE id IN (SELECT emp_id FROM sales WHERE amount > 10000);
-- Execution time: 5.2 seconds
```

**Solution (Fast Join):**
```sql
-- ✅ Fast: Join allows optimizer to combine data efficiently
SELECT DISTINCT e.name 
FROM employees e
JOIN sales s ON e.id = s.emp_id
WHERE s.amount > 10000;
-- Execution time: 0.3 seconds (17x faster!)
```

**Why Joins Are Faster:**
- Query optimizer can choose best join strategy (hash join, merge join)
- Database can use indexes on both tables
- Avoids repeated subquery execution

---

#### Technique 4: Filter Early (WHERE Before JOIN)

**Problem:**
```sql
-- ❌ Bad: Joins all rows then filters
SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE o.total > 10000;
-- Joins 10M orders with 1M customers = 10M row processing
```

**Solution:**
```sql
-- ✅ Better: Filters first, then joins smaller dataset
SELECT * FROM
  (SELECT * FROM orders WHERE total > 10000) o
JOIN customers c ON o.customer_id = c.id;
-- Filters to 50K orders first, then joins = 50K row processing (200x less!)
```

**Using CTE (Even Clearer):**
```sql
WITH HighValueOrders AS (
  SELECT * FROM orders WHERE total > 10000
)
SELECT o.*, c.name 
FROM HighValueOrders o
JOIN customers c ON o.customer_id = c.id;
```

---

#### Technique 5: Limit Results with Pagination

**Problem:**
```sql
-- ❌ Bad: Fetches all 10 million rows
SELECT * FROM transactions ORDER BY date DESC;
-- Execution time: 45 seconds
-- Transfers 5GB of data
```

**Solution:**
```sql
-- ✅ Good: Fetches only 20 rows per page
SELECT * FROM transactions 
ORDER BY date DESC 
LIMIT 20 OFFSET 0;
-- Execution time: 0.05 seconds (900x faster!)

-- Next page
SELECT * FROM transactions 
ORDER BY date DESC 
LIMIT 20 OFFSET 20;
```

**Best Practice:**
- Always use LIMIT for web applications
- Default page size: 10-50 rows
- Add index on ORDER BY column for speed

---

#### Technique 6: Use Proper Data Types

**Problem:**
```sql
-- ❌ Bad: Storing numeric ID as VARCHAR
CREATE TABLE users (
  user_id VARCHAR(10),  -- "0000012345"
  age VARCHAR(3)        -- "025"
);
-- Wastes space, slows down comparisons
```

**Solution:**
```sql
-- ✅ Good: Use appropriate data types
CREATE TABLE users (
  user_id INT,          -- 12345 (4 bytes)
  age TINYINT           -- 25 (1 byte)
);
-- Faster comparisons, less storage, better indexing
```

**Data Type Guidelines:**
- **IDs:** INT, BIGINT (not VARCHAR)
- **Dates:** DATE, TIMESTAMP (not VARCHAR)
- **Prices:** DECIMAL(10,2) (not FLOAT for precision)
- **Status flags:** BOOLEAN or TINYINT (not VARCHAR)
- **Small numbers:** TINYINT, SMALLINT (not INT)

---

#### Technique 7: Optimize ORDER BY and GROUP BY

**Problem:**
```sql
-- ❌ Slow: Sorts 10 million rows in memory
SELECT * FROM orders 
ORDER BY order_date DESC;
-- Uses filesort (temporary disk file)
-- Execution time: 25 seconds
```

**Solution:**
```sql
-- Create index on ORDER BY column
CREATE INDEX idx_order_date ON orders(order_date DESC);

-- Now uses index for sorting (no filesort)
SELECT * FROM orders 
ORDER BY order_date DESC;
-- Execution time: 0.8 seconds (31x faster!)
```

**GROUP BY Optimization:**
```sql
-- Create index on GROUP BY column
CREATE INDEX idx_category ON products(category);

SELECT category, COUNT(*) 
FROM products 
GROUP BY category;
-- Uses index for grouping
```

---

#### Technique 8: Analyze Query Cache

**MySQL Query Cache (deprecated in MySQL 8.0+):**
```sql
-- Check cache status
SHOW VARIABLES LIKE 'query_cache%';

-- If enabled, identical queries return instantly from memory
SELECT * FROM products WHERE category = 'Electronics';
-- First run: 0.5 seconds
-- Subsequent runs: 0.001 seconds (cached!)
```

**Modern Alternative: Application-Level Caching**
- **Redis:** Cache query results with TTL
- **Memcached:** Fast key-value cache
- **CDN:** Cache API responses

---

#### Technique 9: Update Statistics

Databases rely on **table statistics** to decide the best query plan.

**MySQL:**
```sql
ANALYZE TABLE employees;
-- Updates row count, index cardinality, data distribution
```

**PostgreSQL:**
```sql
ANALYZE employees;
-- Or for entire database
ANALYZE;
```

**When to Run:**
- After bulk inserts or deletes
- After major schema changes
- Monthly for active tables
- Automatically via scheduled job

**Impact:**
```
Before ANALYZE:
  Query uses full table scan (optimizer thinks table has 1000 rows)
  
After ANALYZE:
  Query uses index (optimizer knows table has 10M rows)
  
Result: 50x faster query!
```

---

### 9. Detecting Slow Queries

#### MySQL Slow Query Log

**Enable Slow Query Logging:**
```sql
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;  -- Log queries taking > 2 seconds
SET GLOBAL log_queries_not_using_indexes = 'ON';
```

**Check Log File:**
```bash
# Find slow query log location
mysql -e "SHOW VARIABLES LIKE 'slow_query_log_file';"

# Analyze slow queries
mysqldumpslow /var/lib/mysql/slow.log

# Top 10 slowest queries
mysqldumpslow -s t -t 10 /var/lib/mysql/slow.log
```

**Example Log Entry:**
```
# Time: 2024-06-15T10:23:45.123456Z
# User@Host: app_user[app_user] @ localhost []
# Query_time: 12.456789  Lock_time: 0.000234 Rows_sent: 1500  Rows_examined: 5000000
SELECT * FROM orders WHERE status = 'pending';
```

---

#### PostgreSQL Slow Query Logging

**Configure in postgresql.conf:**
```ini
log_min_duration_statement = 2000  -- Log queries > 2 seconds (in milliseconds)
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_statement = 'all'  -- Or 'ddl', 'mod', 'none'
```

**Check Logs:**
```bash
tail -f /var/log/postgresql/postgresql-14-main.log
```

**Using pg_stat_statements Extension:**
```sql
-- Enable extension
CREATE EXTENSION pg_stat_statements;

-- View slowest queries
SELECT 
  calls,
  total_exec_time,
  mean_exec_time,
  query
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

### 10. Real-World Optimization Case Study

#### Travel App Query Optimization

**Problem:**
A travel booking app's query to fetch user bookings was taking **12 seconds**, causing timeouts and user frustration.

**Original Query:**
```sql
SELECT * FROM bookings WHERE user_id = 123;
```

**Step 1: Profile the Query**
```sql
EXPLAIN SELECT * FROM bookings WHERE user_id = 123;
```

**EXPLAIN Output:**

| id | type | table    | key  | rows       | Extra       |
| -- | ---- | -------- | ---- | ---------- | ----------- |
| 1  | ALL  | bookings | NULL | 10,000,000 | Using where |

**Analysis:**
- ❌ **type = ALL:** Full table scan
- ❌ **key = NULL:** No index on user_id
- ❌ **rows = 10,000,000:** Scanning entire table
- 🔍 **Root cause:** Missing index on user_id column

---

**Step 2: Create Index**
```sql
CREATE INDEX idx_user_id ON bookings(user_id);
```

**Step 3: Re-profile**
```sql
EXPLAIN SELECT * FROM bookings WHERE user_id = 123;
```

**New EXPLAIN Output:**

| id | type | table    | key         | rows | Extra |
| -- | ---- | -------- | ----------- | ---- | ----- |
| 1  | ref  | bookings | idx_user_id | 45   | NULL  |

**Analysis:**
- ✅ **type = ref:** Index lookup (fast!)
- ✅ **key = idx_user_id:** Using new index
- ✅ **rows = 45:** Only scanning relevant rows
- 🎯 **Result:** Query optimized!

---

**Step 4: Measure Performance**
```sql
-- Before optimization
SELECT * FROM bookings WHERE user_id = 123;
-- Execution time: 12.3 seconds

-- After optimization
SELECT * FROM bookings WHERE user_id = 123;
-- Execution time: 0.04 seconds

-- Improvement: 12.3s → 0.04s = 307x faster! 💥
```

**Business Impact:**
- 🚀 Page load time: 15s → 0.5s
- 😊 User satisfaction: 45% → 92%
- 💰 Server cost reduction: 40% (less CPU usage)
- 📈 Conversion rate: +25% (users can book faster)

---

### 11. Common Query Optimization Patterns

#### Problem-Solution Matrix

| Problem                        | Symptom                           | Fix                                      | Expected Improvement |
| ------------------------------ | --------------------------------- | ---------------------------------------- | -------------------- |
| **Full table scan**            | EXPLAIN shows type=ALL            | Add appropriate index on filter columns  | 10-100x faster       |
| **Slow joins**                 | High execution time on JOIN       | Index join keys on both tables           | 5-50x faster         |
| **Sorting delays**             | Using filesort in EXPLAIN         | Index ORDER BY columns                   | 3-20x faster         |
| **Repeated subqueries**        | Multiple nested SELECT statements | Use CTEs (WITH clause) or JOINs          | 5-30x faster         |
| **Temp tables overflow**       | Using temporary; Using filesort   | Increase memory or optimize joins        | 2-10x faster         |
| **Too many connections**       | Connection timeout errors         | Use connection pooling                   | Stable performance   |
| **Large result sets**          | Fetching millions of rows         | Add LIMIT and pagination                 | 100x+ faster         |
| **Functions on indexed cols**  | WHERE YEAR(date) = 2024           | Rewrite as date >= '2024-01-01'          | Index can be used    |
| **SELECT * overload**          | High network transfer             | Select only needed columns               | 3-20x faster         |
| **Inefficient data types**     | VARCHAR for numeric IDs           | Use INT, BIGINT for numbers              | 2-5x faster          |

---

### 12. CTE (Common Table Expressions) for Readability

**Problem: Complex Nested Subqueries**
```sql
-- ❌ Hard to read and optimize
SELECT c.name, subquery.order_id
FROM customers c
JOIN (
  SELECT order_id, customer_id
  FROM orders
  WHERE total_amount > 10000
) subquery
ON c.id = subquery.customer_id;
```

**Solution: Using CTE**
```sql
-- ✅ Clear, readable, optimizable
WITH HighValueOrders AS (
  SELECT order_id, customer_id
  FROM orders
  WHERE total_amount > 10000
)
SELECT c.name, h.order_id
FROM customers c
JOIN HighValueOrders h
ON c.id = h.customer_id;
```

**Benefits:**
- ✅ **Readability:** Named subqueries are easier to understand
- ✅ **Reusability:** Reference CTE multiple times in same query
- ✅ **Optimization:** Query optimizer can cache CTE results
- ✅ **Debugging:** Test CTE separately before joining

**Multiple CTEs Example:**
```sql
WITH 
  RecentOrders AS (
    SELECT * FROM orders WHERE order_date > '2024-01-01'
  ),
  HighValueCustomers AS (
    SELECT customer_id FROM RecentOrders WHERE total > 5000
  )
SELECT c.name, COUNT(o.order_id) as order_count
FROM customers c
JOIN HighValueCustomers hvc ON c.id = hvc.customer_id
JOIN RecentOrders o ON c.id = o.customer_id
GROUP BY c.name;
```

---

### 13. Index Hints & Query Plans

#### Forcing Index Usage

Sometimes you need to **force the optimizer** to use a specific index:

**MySQL Index Hints:**
```sql
-- USE INDEX: Suggest an index
SELECT * FROM orders USE INDEX (idx_customer_id)
WHERE customer_id = 501;

-- FORCE INDEX: Force index usage
SELECT * FROM orders FORCE INDEX (idx_customer_id)
WHERE customer_id = 501;

-- IGNORE INDEX: Prevent index usage
SELECT * FROM orders IGNORE INDEX (idx_order_date)
WHERE order_date > '2024-01-01';
```

**When to Use Hints:**
- Optimizer chooses wrong index (rare but happens)
- Testing different indexes for benchmarking
- Temporary workaround until statistics updated

**Warning:** ⚠️ Index hints can backfire if data distribution changes!

---

#### JSON Format for Detailed Plans

**MySQL:**
```sql
EXPLAIN FORMAT=JSON 
SELECT * FROM orders 
WHERE total > 1000;
```

**Output (excerpt):**
```json
{
  "query_block": {
    "select_id": 1,
    "cost_info": {
      "query_cost": "1523.45"
    },
    "table": {
      "table_name": "orders",
      "access_type": "range",
      "possible_keys": ["idx_total"],
      "key": "idx_total",
      "used_key_parts": ["total"],
      "rows_examined_per_scan": 12450,
      "filtered": "100.00"
    }
  }
}
```

**PostgreSQL:**
```sql
EXPLAIN (FORMAT JSON, ANALYZE, BUFFERS)
SELECT * FROM orders WHERE total > 1000;
```

---

### 14. Query Profiling Metrics Example

#### Stage-by-Stage Breakdown

**MySQL SHOW PROFILE:**
```sql
SET profiling = 1;

SELECT * FROM orders WHERE customer_id = 501;

SHOW PROFILES;
SHOW PROFILE FOR QUERY 1;
```

**Output:**

| Stage                | Duration (ms) | CPU (ms) | Notes                          |
| -------------------- | ------------- | -------- | ------------------------------ |
| starting             | 0.0002        | 0.0002   | Query initialization           |
| checking permissions | 0.0001        | 0.0001   | Security check                 |
| Opening tables       | 0.0003        | 0.0003   | Accessing table metadata       |
| init                 | 0.0001        | 0.0001   | Preparing execution            |
| System lock          | 0.0002        | 0.0002   | Locking resources              |
| optimizing           | 0.0008        | 0.0008   | Query plan generation          |
| statistics           | 0.0012        | 0.0012   | Analyzing table stats          |
| preparing            | 0.0005        | 0.0005   | Execution preparation          |
| executing            | 1900.4567     | 1850.234 | **BOTTLENECK: Slow execution** |
| Sorting result       | 400.1234      | 395.678  | **BOTTLENECK: Needs index**    |
| Sending data         | 12.3456       | 11.234   | Network transfer               |
| end                  | 0.0001        | 0.0001   | Cleanup                        |
| query end            | 0.0002        | 0.0002   | Finalization                   |
| closing tables       | 0.0003        | 0.0003   | Releasing locks                |
| freeing items        | 0.0005        | 0.0005   | Memory cleanup                 |
| cleaning up          | 0.0001        | 0.0001   | Final cleanup                  |
| **Total**            | **2313 ms**   | **2257 ms** | **Too slow for production**  |

**Analysis:**
- ❌ **executing stage:** 1900ms — needs index optimization
- ❌ **Sorting result:** 400ms — add index on ORDER BY column
- ✅ **Sending data:** 12ms — acceptable
- 🎯 **Action:** Create index to reduce execution and sorting time

**After Optimization:**
```sql
CREATE INDEX idx_customer_id ON orders(customer_id);
CREATE INDEX idx_order_date ON orders(order_date);
```

**New Profile:**

| Stage        | Duration (ms) | Improvement  |
| ------------ | ------------- | ------------ |
| executing    | 15.2          | 125x faster  |
| Sorting      | 5.8           | 69x faster   |
| Sending data | 12.1          | Same         |
| **Total**    | **35 ms**     | **66x faster!** |

---

### 15. Best Practices Checklist

#### Query Optimization Golden Rules

**Writing Queries:**
- ✅ Avoid `SELECT *` — only fetch needed columns
- ✅ Use `LIMIT` for pagination (default 10-50 rows)
- ✅ Filter early with WHERE before JOIN
- ✅ Use EXISTS instead of COUNT(*) for existence checks
- ✅ Rewrite subqueries as JOINs when possible
- ✅ Use CTEs (WITH clause) for complex queries

**Indexing:**
- ✅ Index your WHERE, JOIN, and ORDER BY columns
- ✅ Use composite indexes for multi-column filters
- ✅ Create covering indexes for frequently accessed columns
- ✅ Monitor index usage with EXPLAIN
- ✅ Remove unused indexes (they slow down INSERT/UPDATE)

**Maintenance:**
- ✅ Regularly ANALYZE tables to update statistics
- ✅ Enable slow query logs (threshold: 2 seconds)
- ✅ Review slow queries weekly
- ✅ Monitor query plans after schema changes
- ✅ Archive old data to keep tables lean

**Anti-Patterns to Avoid:**
- ❌ Avoid functions on indexed columns: `WHERE YEAR(date) = 2024`
  - ✅ Use: `WHERE date >= '2024-01-01' AND date < '2025-01-01'`
- ❌ Avoid `OR` in WHERE clause (prevents index usage)
  - ✅ Use: UNION or separate queries
- ❌ Avoid wildcard prefix in LIKE: `WHERE name LIKE '%smith'`
  - ✅ Use: Full-text search or suffix index
- ❌ Avoid `SELECT DISTINCT` when unique constraint exists
- ❌ Avoid correlated subqueries (execute for each row)

**Caching:**
- ✅ Cache frequent queries at application level (Redis, Memcached)
- ✅ Use result set caching for read-heavy workloads
- ✅ Set appropriate cache TTL (time-to-live)
- ✅ Invalidate cache on data updates

---

### 16. Interview Questions & Answers

**1. What's the difference between query profiling and optimization?**

**Answer:**
- **Query Profiling:** Analyzing how a query executes internally — identifying bottlenecks, examining execution plans, measuring time per stage.
- **Query Optimization:** Applying techniques to improve query performance — adding indexes, rewriting queries, tuning database configuration.

**Analogy:** Profiling is the diagnosis, optimization is the treatment.

---

**2. How does the EXPLAIN plan help in optimization?**

**Answer:**
EXPLAIN shows the query execution plan without running the query:
- **Identifies table scan type:** Index scan (fast) vs full table scan (slow)
- **Shows index usage:** Which indexes are used or ignored
- **Reveals join strategy:** Nested loop, hash join, merge join
- **Estimates rows examined:** Helps identify inefficient queries
- **Highlights optimization opportunities:** WHERE clause needs index, sorting needs optimization

**Example:**
```sql
EXPLAIN SELECT * FROM orders WHERE customer_id = 501;
-- If type=ALL, add index on customer_id
-- If type=ref, query is optimized
```

---

**3. What is a full table scan and how to avoid it?**

**Answer:**
**Full Table Scan:** Database reads every row in the table to find matching records (very slow for large tables).

**Shown in EXPLAIN as:**
- MySQL: `type=ALL`
- PostgreSQL: `Seq Scan`

**How to Avoid:**
1. **Add indexes** on WHERE, JOIN, and ORDER BY columns
2. **Use selective filters** that can leverage indexes
3. **Partition large tables** to reduce scan size
4. **Ensure statistics are updated** (ANALYZE)

**Example:**
```sql
-- Before: Full table scan (10 million rows)
SELECT * FROM users WHERE email = 'user@example.com';

-- After: Add index
CREATE INDEX idx_email ON users(email);
-- Now uses index scan (examines 1 row)
```

---

**4. How can indexes slow down performance sometimes?**

**Answer:**
While indexes speed up SELECT queries, they can slow down writes:

**Overhead:**
- **INSERT:** Index must be updated for new row (extra write)
- **UPDATE:** If indexed column changes, index must be rebuilt
- **DELETE:** Index entry must be removed
- **Storage:** Each index consumes disk space

**When Indexes Hurt:**
- Too many indexes on write-heavy tables (e.g., logging, sensor data)
- Unused indexes waste space and maintenance overhead
- Low cardinality columns (e.g., gender: M/F) — index provides little benefit

**Best Practice:**
- Index only frequently queried columns
- Remove unused indexes
- Use `SHOW INDEX FROM table` to audit indexes
- Balance read vs write performance based on workload

---

**5. What is query caching?**

**Answer:**
**Query Caching:** Storing query results in memory so identical queries return results instantly without re-execution.

**How It Works:**
1. Query executed and result computed
2. Result stored in cache with query as key
3. Identical query checks cache first
4. If found (cache hit), return cached result
5. If not found (cache miss), execute query and cache result

**MySQL Query Cache (deprecated in 8.0+):**
```sql
SHOW VARIABLES LIKE 'query_cache%';
```

**Modern Alternatives:**
- **Application-level caching:** Redis, Memcached
- **Result set caching:** Cache in application layer
- **CDN caching:** Cache API responses
- **Materialized views:** Pre-computed query results

**Invalidation:** Cache must be cleared when underlying data changes.

---

**6. How can you detect and fix slow queries in MySQL/PostgreSQL?**

**Answer:**

**MySQL:**
1. **Enable slow query log:**
```sql
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;
```

2. **Analyze slow queries:**
```bash
mysqldumpslow /var/lib/mysql/slow.log
```

3. **Profile specific query:**
```sql
EXPLAIN SELECT * FROM orders WHERE status = 'pending';
```

4. **Fix:** Add indexes, rewrite query, optimize JOINs

**PostgreSQL:**
1. **Enable logging:**
```ini
log_min_duration_statement = 2000  # 2 seconds
```

2. **Use pg_stat_statements:**
```sql
SELECT query, mean_exec_time FROM pg_stat_statements
ORDER BY mean_exec_time DESC LIMIT 10;
```

3. **Profile query:**
```sql
EXPLAIN ANALYZE SELECT * FROM orders WHERE status = 'pending';
```

4. **Fix:** Add indexes, update statistics (ANALYZE), optimize query

---

**7. What's the role of statistics in query optimization?**

**Answer:**
**Database Statistics:** Metadata about table data distribution used by the query optimizer to choose the best execution plan.

**Includes:**
- **Row count:** Number of rows in table
- **Data distribution:** Value frequency in columns
- **Index cardinality:** Number of distinct values in indexed columns
- **NULL values:** Percentage of NULL entries
- **Data size:** Table and index storage size

**How Optimizer Uses Statistics:**
- Estimates cost of different execution plans
- Decides whether to use index or full table scan
- Chooses join order (join smaller table first)
- Estimates memory needed for sorts and joins

**Updating Statistics:**
```sql
-- MySQL
ANALYZE TABLE orders;

-- PostgreSQL
ANALYZE orders;
```

**When to Update:**
- After bulk INSERT/DELETE (significant data change)
- After major schema changes
- Monthly for active tables
- Automatically via scheduled job

**Impact:** Outdated statistics = poor query plans = slow queries!

---

### 17. Summary Table

| Concept                    | Description                                    | Key Benefit                       |
| -------------------------- | ---------------------------------------------- | --------------------------------- |
| **Query Profiling**        | Analyze query execution path and bottlenecks   | Identify slow operations          |
| **EXPLAIN / ANALYZE**      | Tools to view execution plan and actual timing | Understand query behavior         |
| **Slow Query Log**         | Logs queries exceeding time threshold          | Detect problematic queries        |
| **Indexing**               | Create indexes on filter/join columns          | 10-100x faster lookups            |
| **Joins & CTEs**           | Structure complex queries efficiently          | Readability + optimization        |
| **Avoid SELECT ***         | Fetch only needed columns                      | Reduce I/O and network transfer   |
| **LIMIT & Pagination**     | Return small result sets                       | Prevent over-fetching             |
| **Query Caching**          | Store frequent results in memory               | Instant response for cached queries|
| **ANALYZE Tables**         | Update table statistics                        | Better execution plans            |
| **Connection Pooling**     | Reuse database connections                     | Reduce connection overhead        |
| **Goal**                   | Reduce latency and resource usage              | Fast, efficient, scalable system  |

---

### 18. Final Thought

> **"Good developers write queries.**
> **Great developers optimize them."**

Optimization is not just about speed — it's about **efficiency, scalability, and system reliability.**

**Key Takeaways:**
- 🔍 **Profile first:** Understand the problem before optimizing
- ⚡ **Index wisely:** Most performance gains come from proper indexing
- 📊 **Monitor continuously:** Use slow query logs and profiling tools
- 🧹 **Maintain regularly:** Update statistics, remove unused indexes
- 🎯 **Optimize iteratively:** Start with biggest bottlenecks first
- 💡 **Think like the optimizer:** Understand how database executes queries

**Performance Hierarchy (Impact):**
1. **Indexing:** 10-1000x improvement (biggest impact)
2. **Query rewriting:** 5-50x improvement
3. **Caching:** 10-100x for repeated queries
4. **Hardware:** 2-5x improvement (expensive)
5. **Configuration tuning:** 1.5-3x improvement

**Remember:**
> "Premature optimization is the root of all evil, but **neglecting obvious optimizations** is equally harmful!"

Start with profiling, fix the biggest bottlenecks first, and continuously monitor performance as your application grows. 🚀
''',
    codeSnippet: '''
-- ========================================
-- 1. MYSQL EXPLAIN - VIEW EXECUTION PLAN
-- ========================================

-- Basic EXPLAIN
EXPLAIN SELECT name, salary 
FROM employees 
WHERE department = 'Sales';

-- Extended EXPLAIN with warnings
EXPLAIN EXTENDED SELECT name, salary 
FROM employees 
WHERE department = 'Sales';
SHOW WARNINGS;

-- JSON format for detailed analysis
EXPLAIN FORMAT=JSON 
SELECT e.name, d.department_name
FROM employees e
JOIN departments d ON e.dept_id = d.id
WHERE e.salary > 50000;

-- ========================================
-- 2. POSTGRESQL EXPLAIN ANALYZE
-- ========================================

-- Basic EXPLAIN (estimated)
EXPLAIN 
SELECT customer_name, total_amount
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.order_date > '2024-01-01';

-- EXPLAIN ANALYZE (actual execution)
EXPLAIN ANALYZE
SELECT customer_name, total_amount
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.order_date > '2024-01-01';

-- Detailed with buffers and costs
EXPLAIN (ANALYZE, BUFFERS, COSTS, VERBOSE)
SELECT * FROM orders 
WHERE status = 'pending'
ORDER BY order_date DESC;

-- ========================================
-- 3. MYSQL PROFILING
-- ========================================

-- Enable profiling
SET profiling = 1;

-- Run query
SELECT * FROM orders WHERE customer_id = 501;

-- View all profiles
SHOW PROFILES;

-- Detailed profile for specific query
SHOW PROFILE FOR QUERY 1;

-- Profile with CPU and I/O details
SHOW PROFILE CPU, BLOCK IO FOR QUERY 1;

-- Disable profiling
SET profiling = 0;

-- ========================================
-- 4. CREATING INDEXES FOR OPTIMIZATION
-- ========================================

-- Single column index
CREATE INDEX idx_customer_id ON orders(customer_id);

-- Composite index (multiple columns)
CREATE INDEX idx_customer_date ON orders(customer_id, order_date);

-- Descending index for ORDER BY DESC
CREATE INDEX idx_order_date_desc ON orders(order_date DESC);

-- Covering index (includes all query columns)
CREATE INDEX idx_covering ON orders(customer_id, order_date, total_amount);

-- Partial index (PostgreSQL - index subset of rows)
CREATE INDEX idx_pending_orders ON orders(order_date)
WHERE status = 'pending';

-- Unique index
CREATE UNIQUE INDEX idx_email ON users(email);

-- Full-text index for text search
CREATE FULLTEXT INDEX idx_product_name ON products(product_name);

-- Check index usage
SHOW INDEX FROM orders;

-- ========================================
-- 5. OPTIMIZATION TECHNIQUE: AVOID SELECT *
-- ========================================

-- ❌ Bad: Fetches all columns
SELECT * FROM products;

-- ✅ Good: Fetch only needed columns
SELECT product_name, price, stock 
FROM products;

-- ❌ Bad: SELECT * in join
SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id;

-- ✅ Good: Specify columns
SELECT o.order_id, o.total, c.name, c.email
FROM orders o
JOIN customers c ON o.customer_id = c.id;

-- ========================================
-- 6. OPTIMIZATION: JOINS VS SUBQUERIES
-- ========================================

-- ❌ Slow: IN subquery
SELECT name FROM employees
WHERE id IN (SELECT emp_id FROM sales WHERE amount > 10000);

-- ✅ Fast: JOIN
SELECT DISTINCT e.name 
FROM employees e
JOIN sales s ON e.id = s.emp_id
WHERE s.amount > 10000;

-- ❌ Slow: Correlated subquery (executes for each row)
SELECT name, 
  (SELECT COUNT(*) FROM orders WHERE customer_id = c.id) as order_count
FROM customers c;

-- ✅ Fast: JOIN with GROUP BY
SELECT c.name, COUNT(o.order_id) as order_count
FROM customers c
LEFT JOIN orders o ON c.id = o.customer_id
GROUP BY c.name;

-- ========================================
-- 7. OPTIMIZATION: FILTER EARLY
-- ========================================

-- ❌ Bad: Joins all rows then filters
SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE o.total > 10000;

-- ✅ Better: Filter first using subquery
SELECT * FROM
  (SELECT * FROM orders WHERE total > 10000) o
JOIN customers c ON o.customer_id = c.id;

-- ✅ Best: Using CTE (Common Table Expression)
WITH HighValueOrders AS (
  SELECT * FROM orders WHERE total > 10000
)
SELECT o.*, c.name, c.email
FROM HighValueOrders o
JOIN customers c ON o.customer_id = c.id;

-- ========================================
-- 8. OPTIMIZATION: PAGINATION WITH LIMIT
-- ========================================

-- ❌ Bad: Fetches all rows
SELECT * FROM transactions ORDER BY date DESC;

-- ✅ Good: Pagination (first page)
SELECT * FROM transactions 
ORDER BY date DESC 
LIMIT 20 OFFSET 0;

-- Second page
SELECT * FROM transactions 
ORDER BY date DESC 
LIMIT 20 OFFSET 20;

-- Better: Keyset pagination (more efficient for large offsets)
SELECT * FROM transactions 
WHERE date < '2024-06-01'
ORDER BY date DESC 
LIMIT 20;

-- ========================================
-- 9. OPTIMIZATION: PROPER DATA TYPES
-- ========================================

-- ❌ Bad: Wrong data types
CREATE TABLE users_bad (
  user_id VARCHAR(10),        -- Should be INT
  age VARCHAR(3),             -- Should be TINYINT
  created_at VARCHAR(20),     -- Should be TIMESTAMP
  is_active VARCHAR(5)        -- Should be BOOLEAN
);

-- ✅ Good: Appropriate data types
CREATE TABLE users_good (
  user_id INT PRIMARY KEY,
  age TINYINT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  is_active BOOLEAN DEFAULT TRUE
);

-- ========================================
-- 10. DETECTING SLOW QUERIES (MySQL)
-- ========================================

-- Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;  -- Log queries > 2 seconds
SET GLOBAL log_queries_not_using_indexes = 'ON';

-- Check slow query log location
SHOW VARIABLES LIKE 'slow_query_log_file';

-- View slow query log settings
SHOW VARIABLES LIKE 'slow_query%';

-- Analyze slow queries from log (shell command)
-- mysqldumpslow -s t -t 10 /var/lib/mysql/slow.log

-- ========================================
-- 11. DETECTING SLOW QUERIES (PostgreSQL)
-- ========================================

-- Enable pg_stat_statements extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- View slowest queries
SELECT 
  calls,
  total_exec_time,
  mean_exec_time,
  max_exec_time,
  stddev_exec_time,
  rows,
  LEFT(query, 100) as query_preview
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- View queries by total time
SELECT 
  calls,
  total_exec_time,
  mean_exec_time,
  query
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 10;

-- Reset statistics
SELECT pg_stat_statements_reset();

-- ========================================
-- 12. ANALYZE TABLES (UPDATE STATISTICS)
-- ========================================

-- MySQL: Analyze single table
ANALYZE TABLE employees;

-- Analyze multiple tables
ANALYZE TABLE employees, departments, orders;

-- Check table statistics
SHOW TABLE STATUS LIKE 'employees';

-- PostgreSQL: Analyze single table
ANALYZE employees;

-- Analyze entire database
ANALYZE;

-- Verbose output
ANALYZE VERBOSE employees;

-- ========================================
-- 13. CTE (COMMON TABLE EXPRESSIONS)
-- ========================================

-- Simple CTE
WITH HighValueOrders AS (
  SELECT order_id, customer_id, total
  FROM orders
  WHERE total > 10000
)
SELECT c.name, h.order_id, h.total
FROM customers c
JOIN HighValueOrders h ON c.id = h.customer_id;

-- Multiple CTEs
WITH 
  RecentOrders AS (
    SELECT * FROM orders 
    WHERE order_date > '2024-01-01'
  ),
  HighValueCustomers AS (
    SELECT DISTINCT customer_id 
    FROM RecentOrders 
    WHERE total > 5000
  )
SELECT c.name, COUNT(o.order_id) as order_count
FROM customers c
JOIN HighValueCustomers hvc ON c.id = hvc.customer_id
JOIN RecentOrders o ON c.id = o.customer_id
GROUP BY c.name;

-- Recursive CTE (organization hierarchy)
WITH RECURSIVE EmployeeHierarchy AS (
  -- Base case: top-level employees
  SELECT id, name, manager_id, 1 as level
  FROM employees
  WHERE manager_id IS NULL
  
  UNION ALL
  
  -- Recursive case: employees with managers
  SELECT e.id, e.name, e.manager_id, eh.level + 1
  FROM employees e
  JOIN EmployeeHierarchy eh ON e.manager_id = eh.id
)
SELECT * FROM EmployeeHierarchy ORDER BY level, name;

-- ========================================
-- 14. INDEX HINTS
-- ========================================

-- MySQL: USE INDEX (suggest index)
SELECT * FROM orders USE INDEX (idx_customer_id)
WHERE customer_id = 501;

-- FORCE INDEX (force index usage)
SELECT * FROM orders FORCE INDEX (idx_customer_id)
WHERE customer_id = 501 AND total > 1000;

-- IGNORE INDEX (prevent index usage)
SELECT * FROM orders IGNORE INDEX (idx_order_date)
WHERE order_date > '2024-01-01';

-- Multiple index hints
SELECT * FROM orders 
USE INDEX (idx_customer_id) 
IGNORE INDEX (idx_status)
WHERE customer_id = 501;

-- ========================================
-- 15. QUERY OPTIMIZATION EXAMPLES
-- ========================================

-- Optimize ORDER BY (add index)
CREATE INDEX idx_order_date ON orders(order_date DESC);

SELECT * FROM orders 
ORDER BY order_date DESC 
LIMIT 20;

-- Optimize GROUP BY (add index)
CREATE INDEX idx_category ON products(category);

SELECT category, COUNT(*) as product_count, AVG(price) as avg_price
FROM products
GROUP BY category;

-- Optimize WHERE + ORDER BY (composite index)
CREATE INDEX idx_status_date ON orders(status, order_date DESC);

SELECT * FROM orders 
WHERE status = 'pending'
ORDER BY order_date DESC;

-- ========================================
-- 16. EXISTS VS IN (OPTIMIZATION)
-- ========================================

-- ❌ Slower: IN with subquery
SELECT * FROM customers
WHERE id IN (SELECT customer_id FROM orders WHERE total > 1000);

-- ✅ Faster: EXISTS
SELECT * FROM customers c
WHERE EXISTS (
  SELECT 1 FROM orders o 
  WHERE o.customer_id = c.id AND o.total > 1000
);

-- EXISTS stops at first match (more efficient)

-- ========================================
-- 17. AVOID FUNCTIONS ON INDEXED COLUMNS
-- ========================================

-- ❌ Bad: Function prevents index usage
SELECT * FROM orders 
WHERE YEAR(order_date) = 2024;

-- ✅ Good: Index can be used
SELECT * FROM orders 
WHERE order_date >= '2024-01-01' 
  AND order_date < '2025-01-01';

-- ❌ Bad: Function on indexed column
SELECT * FROM users 
WHERE UPPER(email) = 'USER@EXAMPLE.COM';

-- ✅ Good: Use case-insensitive collation or index
CREATE INDEX idx_email_lower ON users(LOWER(email));
SELECT * FROM users 
WHERE LOWER(email) = 'user@example.com';

-- ========================================
-- 18. COVERING INDEX EXAMPLE
-- ========================================

-- Query needs customer_id, order_date, total
SELECT customer_id, order_date, total
FROM orders
WHERE customer_id = 501;

-- Create covering index (includes all queried columns)
CREATE INDEX idx_covering ON orders(customer_id, order_date, total);

-- Now query reads only from index (no table lookup)
-- EXPLAIN shows "Using index" (index-only scan)

-- ========================================
-- 19. MONITORING INDEX USAGE
-- ========================================

-- MySQL: Check index usage
SHOW INDEX FROM orders;

-- View index statistics
SELECT 
  TABLE_NAME,
  INDEX_NAME,
  SEQ_IN_INDEX,
  COLUMN_NAME,
  CARDINALITY
FROM information_schema.STATISTICS
WHERE TABLE_SCHEMA = 'mydb' AND TABLE_NAME = 'orders';

-- PostgreSQL: Check unused indexes
SELECT 
  schemaname,
  tablename,
  indexname,
  idx_scan as index_scans,
  idx_tup_read as tuples_read,
  idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes
WHERE idx_scan = 0
ORDER BY schemaname, tablename;

-- ========================================
-- 20. REAL-WORLD CASE STUDY
-- ========================================

-- Before optimization: Full table scan
EXPLAIN SELECT * FROM bookings WHERE user_id = 123;
-- type: ALL, rows: 10,000,000 (12.3 seconds)

-- Create index
CREATE INDEX idx_user_id ON bookings(user_id);

-- After optimization: Index scan
EXPLAIN SELECT * FROM bookings WHERE user_id = 123;
-- type: ref, key: idx_user_id, rows: 45 (0.04 seconds)
-- Improvement: 307x faster!

-- Further optimization: Covering index
CREATE INDEX idx_user_booking ON bookings(user_id, booking_date, status, hotel_name);

SELECT booking_date, status, hotel_name 
FROM bookings 
WHERE user_id = 123;
-- Uses covering index (no table access needed)
-- Extra: Using index (0.02 seconds, 615x faster!)
''',
    revisionPoints: [
      'Query profiling analyzes how SQL executes internally to identify bottlenecks',
      'Query optimization rewrites or tunes queries for better speed and efficiency',
      'EXPLAIN shows execution plan: table scan type, index usage, rows examined, join strategy',
      'Full table scan (type=ALL) is slow - add indexes to avoid scanning entire table',
      'EXPLAIN ANALYZE (PostgreSQL) provides actual execution time and row counts',
      'SHOW PROFILE (MySQL) breaks down query time by stage: parsing, execution, sorting, sending',
      'Key metrics: execution time, rows examined vs returned, index usage, temporary tables, sorts',
      'Avoid SELECT * - fetch only needed columns to reduce I/O and network transfer',
      'Use JOINs instead of subqueries for better optimizer performance (5-30x faster)',
      'Filter early with WHERE before JOIN to reduce rows processed',
      'LIMIT with pagination prevents over-fetching large datasets',
      'Use proper data types: INT for IDs, DATE for dates, BOOLEAN for flags (not VARCHAR)',
      'Create indexes on WHERE, JOIN, and ORDER BY columns for 10-100x speedup',
      'Composite indexes cover multiple columns (customer_id, order_date)',
      'Covering indexes include all query columns, enabling index-only scans',
      'Slow query log detects queries exceeding time threshold (e.g., > 2 seconds)',
      'ANALYZE TABLE updates statistics for better query optimizer decisions',
      'CTEs (WITH clause) improve readability and allow optimizer to cache results',
      'Avoid functions on indexed columns: WHERE YEAR(date)=2024 prevents index usage',
      'Use EXISTS instead of IN for subquery checks (stops at first match)',
      'Index hints (USE INDEX, FORCE INDEX) can force specific index usage',
      'pg_stat_statements (PostgreSQL) tracks slow queries with execution statistics',
      'Connection pooling reuses connections to reduce overhead',
      'Query caching stores results in memory for instant retrieval (Redis, Memcached)',
      'Final principle: Profile first, optimize biggest bottlenecks, monitor continuously',
    ],
    quizQuestions: [
      Question(
        question: 'What does EXPLAIN show in a query execution plan?',
        options: [
          'Only query syntax errors',
          'Table scan type, index usage, rows examined, join strategy',
          'Database backup status',
          'User permissions'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does "type=ALL" in MySQL EXPLAIN indicate?',
        options: [
          'Query is fully optimized',
          'Using all available indexes',
          'Full table scan (slow, needs optimization)',
          'All columns selected'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which is faster for checking existence?',
        options: [
          'IN with subquery',
          'EXISTS (stops at first match)',
          'COUNT(*) > 0',
          'All same speed'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a covering index?',
        options: [
          'Index on all table columns',
          'Index that includes all columns needed by query (no table lookup)',
          'Index used by all queries',
          'Backup index'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Why should you avoid SELECT * in production?',
        options: [
          'It causes syntax errors',
          'Fetches unnecessary data, increases I/O and network transfer',
          'It is deprecated',
          'No reason, it is fine'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does ANALYZE TABLE do?',
        options: [
          'Deletes old data',
          'Updates table statistics for better query optimizer decisions',
          'Creates automatic indexes',
          'Backs up table'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which is an anti-pattern that prevents index usage?',
        options: [
          'WHERE status = "active"',
          'WHERE YEAR(date) = 2024 (function on indexed column)',
          'WHERE id = 123',
          'ORDER BY date'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is query profiling used for?',
        options: [
          'Creating database backups',
          'Analyzing query execution to identify bottlenecks and slow stages',
          'User authentication',
          'Data encryption'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is the purpose of slow query log?',
        options: [
          'Log all database errors',
          'Detect and log queries exceeding time threshold (e.g., > 2 seconds)',
          'Store query results',
          'Backup queries'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Why use CTE (WITH clause)?',
        options: [
          'Faster than JOINs',
          'Improves readability, allows optimizer to cache results, enables recursion',
          'Required for all queries',
          'Only for INSERT'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What improvement did the travel app case study achieve?',
        options: [
          '2x faster',
          '10x faster',
          '307x faster (12.3s to 0.04s by adding index)',
          'No improvement'
        ],
        correctIndex: 2,
      ),
      Question(
        question: 'Which has the biggest performance impact?',
        options: [
          'Hardware upgrades',
          'Proper indexing (10-1000x improvement)',
          'Configuration tuning',
          'Using CTEs'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'distributed_db',
    title: '21. Distributed Databases',
    explanation: '''## Distributed Databases

### 1. Introduction

A **Distributed Database (DDB)** is a database that is **stored across multiple physical locations** — computers, data centers, or even countries — but appears to the user as **a single logical database**.

**Definition:**
> 🧠 A Distributed Database is a collection of **interconnected databases** distributed across different network sites, managed by a **Distributed Database Management System (DDBMS)** that ensures data consistency, reliability, and transparency.

**The Problem:**
- 🌍 Single database server cannot handle global traffic
- 📈 Vertical scaling (bigger server) is expensive and has limits
- 💥 Single point of failure risks entire system
- ⏱️ Users far from server experience high latency
- 💾 Massive datasets exceed single server capacity

**The Solution:**
- ✅ Distribute data across multiple servers (horizontal scaling)
- ✅ Place data near users for low latency
- ✅ Replicate data for fault tolerance
- ✅ Scale by adding more nodes instead of upgrading one machine

**Analogy:**
> 💡 Think of a distributed database like a **global library system** — books (data) are stored in branches (nodes) across cities, but you can search the entire catalog from any branch. The system handles finding and delivering the book you need transparently!

---

### 2. Key Components

#### Architecture Components

| Component                   | Description                                            | Role                              |
| --------------------------- | ------------------------------------------------------ | --------------------------------- |
| **Sites / Nodes**           | Individual database servers storing part of the data   | Store data fragments or replicas  |
| **Network**                 | Connects all nodes for communication                   | Data transfer, coordination       |
| **DDBMS**                   | Distributed Database Management System                 | Manages distribution, queries     |
| **Global Schema**           | Logical view of entire database across all sites       | User sees unified database        |
| **Local Schema**            | Each site's own subset of data                         | Site-specific storage             |
| **Transaction Manager**     | Ensures atomicity and consistency across sites         | Coordinates distributed commits   |
| **Data Dictionary**         | Metadata about data location and structure             | Tracks where data is stored       |
| **Query Optimizer**         | Determines efficient query execution across nodes      | Minimizes network transfers       |
| **Concurrency Controller**  | Manages simultaneous access to distributed data        | Prevents conflicts                |
| **Recovery Manager**        | Handles failures and restores consistency              | Rollback, failover                |

---

### 3. Why Distributed Databases?

#### Business and Technical Benefits

| Reason                      | Explanation                                          | Example Impact                      |
| --------------------------- | ---------------------------------------------------- | ----------------------------------- |
| 🌎 **Scalability**          | Add more servers instead of upgrading one big one    | Scale from 1M to 1B users           |
| ⚡ **Performance**           | Data stored near users → faster access               | Latency: 500ms → 50ms               |
| 🧾 **Reliability**          | Failure in one node doesn't crash the system         | 99.9% → 99.99% uptime               |
| 🔒 **Availability**         | Redundancy ensures continuous operation              | Zero downtime deployments           |
| 💰 **Cost-Effective**       | Commodity hardware replaces expensive single machine | Hardware cost: \$100K → \$30K        |
| 🌍 **Geographic Distribution** | Serve global users from regional data centers     | EU users → EU data center           |
| 📊 **Load Balancing**       | Distribute queries across multiple servers           | Handle 10K concurrent users         |
| 🔐 **Data Locality**        | Comply with data residency laws (GDPR)               | Store EU data only in EU            |

**Real-World Example:**
- **Netflix:** Distributed content delivery across AWS regions worldwide
- **Facebook:** User data sharded by geographic region
- **Google Search:** Indexes distributed across global data centers

---

### 4. Types of Distributed Databases

#### Classification by Architecture

| Type                        | Description                                      | Characteristics                        | Example                      |
| --------------------------- | ------------------------------------------------ | -------------------------------------- | ---------------------------- |
| **Homogeneous DDB**         | All nodes run same DBMS software                 | Uniform, easier to manage              | All MySQL 8.0 servers        |
| **Heterogeneous DDB**       | Nodes use different DBMSs                        | Complex, requires integration layer    | MySQL + MongoDB + Oracle     |
| **Federated DDB**           | Multiple autonomous databases integrated         | Each DB independent, virtual integration | Multi-department systems   |
| **Shared-Nothing DDB**      | Each node has own storage, operates independently | High scalability, no resource contention | Google Spanner, Cassandra  |
| **Shared-Disk DDB**         | Multiple nodes share same disk storage           | Simplified consistency, storage bottleneck | Oracle RAC                |
| **Multi-Master DDB**        | Multiple nodes accept writes simultaneously      | High availability, conflict resolution needed | CouchDB, Cassandra      |
| **Master-Slave DDB**        | One master for writes, multiple slaves for reads | Simple consistency, master bottleneck  | MySQL replication            |

---

### 5. Data Distribution Techniques

Data in distributed systems can be placed in 3 main ways:

#### Technique 1: Fragmentation

**Definition:** Data is split into pieces (fragments) stored at different sites.

**Types:**

**A. Horizontal Fragmentation (Sharding)**
- **Definition:** Rows are divided across nodes
- **Use Case:** Divide by geographic region, date range, or hash key

**Example:**
```sql
-- Sales data divided by region
-- Node 1 (Asia):
CREATE TABLE sales_asia AS 
SELECT * FROM sales WHERE region = 'Asia';

-- Node 2 (Europe):
CREATE TABLE sales_europe AS 
SELECT * FROM sales WHERE region = 'Europe';

-- Node 3 (Americas):
CREATE TABLE sales_americas AS 
SELECT * FROM sales WHERE region = 'Americas';
```

**B. Vertical Fragmentation**
- **Definition:** Columns are divided across nodes
- **Use Case:** Separate frequently accessed from rarely accessed data

**Example:**
```sql
-- Node 1: Basic employee info (frequently accessed)
CREATE TABLE employee_info (
  emp_id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  department VARCHAR(50)
);

-- Node 2: Salary details (restricted access)
CREATE TABLE employee_salary (
  emp_id INT PRIMARY KEY,
  salary DECIMAL(10,2),
  bonus DECIMAL(10,2),
  bank_account VARCHAR(20)
);
```

**C. Hybrid Fragmentation**
- **Definition:** Combination of horizontal and vertical
- **Example:** Divide employees by region (horizontal), then separate personal info from salary (vertical)

---

#### Technique 2: Replication

**Definition:** Copies of same data stored at multiple sites for availability and performance.

**Types:**

| Replication Type            | Description                                 | Pros                          | Cons                          |
| --------------------------- | ------------------------------------------- | ----------------------------- | ----------------------------- |
| **Full Replication**        | Entire database copied to every node        | High availability, fast reads | Storage overhead, sync complexity |
| **Partial Replication**     | Only frequently accessed data replicated    | Balanced storage, performance | More complex management       |
| **No Replication**          | Each fragment stored in one location only   | Simple, minimal storage       | Lower availability            |
| **Master-Slave**            | One master for writes, replicas for reads   | Simple consistency            | Master bottleneck             |
| **Master-Master**           | All nodes accept writes                     | High availability             | Conflict resolution needed    |

**Example:**
```sql
-- Node A (Primary):
CREATE TABLE customers (
  customer_id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100)
) ENGINE=InnoDB;

-- Node B (Replica):
CREATE TABLE customers (
  customer_id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100)
) ENGINE=InnoDB;
-- Replication configured: Node B syncs from Node A
```

**Benefits:**
- ✅ Improved **availability** (if one node fails, others serve data)
- ✅ Faster **read performance** (load balanced across replicas)
- ✅ **Disaster recovery** (backup copies in different locations)

**Challenges:**
- ❌ **Consistency issues** (replicas may be slightly out of sync)
- ❌ **Storage overhead** (multiple copies consume space)
- ❌ **Update complexity** (all replicas must be updated)

---

#### Technique 3: Allocation

**Definition:** Decides where to store each fragment or replica.

**Strategies:**

| Allocation Type           | Description                              | Use Case                       |
| ------------------------- | ---------------------------------------- | ------------------------------ |
| **Static Allocation**     | Fixed placement decided at design time   | Stable workloads               |
| **Dynamic Allocation**    | Automatically moves data based on usage  | Variable workloads, hot data   |
| **Centralized Allocation**| Central controller decides placement     | Simple, single decision point  |
| **Distributed Allocation**| Nodes autonomously decide placement      | Scalable, no single bottleneck |

---

### 6. Example Architecture: University Database

**Scenario:** A university with 3 campuses uses distributed database.

**Data Distribution:**

| Campus     | Data Stored              | Fragmentation Type |
| ---------- | ------------------------ | ------------------ |
| Campus A   | Student personal info    | Horizontal         |
| Campus B   | Exam results             | Horizontal         |
| Campus C   | Course details           | Replicated (all)   |

**Query Example:**

```sql
-- Student portal query (appears as single database to user)
SELECT s.name, c.course_name, r.grade
FROM students s
JOIN courses c ON s.course_id = c.course_id
JOIN results r ON s.student_id = r.student_id
WHERE s.student_id = 501;
```

**Behind the Scenes:**
1. DDBMS detects `students` is on Campus A
2. Detects `results` is on Campus B
3. Detects `courses` is on Campus C
4. Executes subqueries on each campus
5. Merges results and returns to user

**User Experience:** Transparent! Sees one database, unaware of distribution.

---

### 7. Characteristics of Distributed Databases

#### Transparency Levels

| Transparency Type              | Meaning                                               | Benefit                          |
| ------------------------------ | ----------------------------------------------------- | -------------------------------- |
| **Location Transparency**      | Users don't know physical location of data            | Simplified queries               |
| **Replication Transparency**   | Data may exist in multiple sites, user sees one copy  | Automatic failover               |
| **Fragmentation Transparency** | User doesn't need to know data is fragmented          | Query single logical table       |
| **Concurrency Transparency**   | Multiple users access data without conflicts          | Safe concurrent access           |
| **Failure Transparency**       | System masks failures from users                      | Continuous operation             |
| **Performance Transparency**   | System optimizes query performance automatically      | Faster queries without tuning    |
| **Scaling Transparency**       | Adding/removing nodes doesn't affect users            | Seamless growth                  |

---

### 8. Distributed Transactions

**Definition:** A **Distributed Transaction** involves data updates across multiple sites that must succeed or fail together (atomicity).

**ACID Properties in Distributed Context:**
- **Atomicity:** All-or-nothing across all nodes
- **Consistency:** All nodes reach consistent state
- **Isolation:** Concurrent transactions don't interfere
- **Durability:** Changes persist even after node failures

---

#### Example: Money Transfer

```sql
-- Transfer \$500 from Account A (Node 1) to Account B (Node 2)
BEGIN DISTRIBUTED TRANSACTION;

-- Update on Node 1 (Branch A in Delhi)
UPDATE branch_a.accounts 
SET balance = balance - 500 
WHERE account_id = 1001;

-- Update on Node 2 (Branch B in Mumbai)
UPDATE branch_b.accounts 
SET balance = balance + 500 
WHERE account_id = 2005;

COMMIT;
```

**If Transaction Fails:**
- Node 1 succeeds but Node 2 fails (network issue)
- **Result:** Both updates are rolled back
- **Guarantee:** Either both accounts updated or neither (atomicity)

---

### 9. Two-Phase Commit Protocol (2PC)

**Purpose:** Ensures **atomicity** in distributed transactions.

**How It Works:**

#### Phase 1: Prepare Phase (Voting)

1. **Coordinator** sends PREPARE message to all participating nodes
2. Each **participant node** checks if it can commit:
   - Locks resources
   - Writes to transaction log
   - Replies **VOTE-COMMIT** (ready) or **VOTE-ABORT** (cannot commit)

#### Phase 2: Commit Phase (Decision)

3. **Coordinator** makes decision:
   - **If all voted COMMIT:** Send GLOBAL-COMMIT to all
   - **If any voted ABORT:** Send GLOBAL-ABORT to all
4. **Participants** execute decision and send acknowledgment
5. **Coordinator** completes transaction

**Diagram (in text):**
```
Coordinator                 Node A              Node B
    |                          |                   |
    |--- PREPARE ----------->  |                   |
    |--- PREPARE --------------------------------> |
    |                          |                   |
    |<-- VOTE-COMMIT --------- |                   |
    |<-- VOTE-COMMIT ------------------------------ |
    |                          |                   |
    |--- GLOBAL-COMMIT ------> |                   |
    |--- GLOBAL-COMMIT --------------------------> |
    |                          |                   |
    |<-- ACK ----------------- |                   |
    |<-- ACK --------------------------------------- |
    |                          |                   |
  [DONE]                   [COMMIT]            [COMMIT]
```

**Advantages:**
- ✅ Guarantees atomicity across all nodes
- ✅ All nodes reach same outcome (commit or abort)

**Disadvantages:**
- ❌ **Blocking:** If coordinator crashes, nodes are stuck waiting
- ❌ **Performance overhead:** Two rounds of communication
- ❌ **Single point of failure:** Coordinator is critical

**Optimization: Three-Phase Commit (3PC)**
- Adds a pre-commit phase to reduce blocking
- More resilient but adds complexity

---

### 10. Concurrency Control in Distributed Systems

**Challenge:** Multiple transactions accessing same data across different nodes simultaneously.

**Techniques:**

#### 1. Locking Mechanisms

**Two-Phase Locking (2PL):**
- **Growing Phase:** Acquire locks, no releases
- **Shrinking Phase:** Release locks, no acquisitions

**Example:**
```sql
-- Transaction 1 on Node A
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE account_id = 501 FOR UPDATE;
-- Lock acquired on row
UPDATE accounts SET balance = balance - 100 WHERE account_id = 501;
COMMIT;
-- Lock released
```

**Types:**
- **Shared Lock (S):** Multiple reads allowed, no writes
- **Exclusive Lock (X):** No reads or writes by others

---

#### 2. Timestamp Ordering

**How It Works:**
- Each transaction gets a timestamp when it starts
- Older transactions (lower timestamp) get priority
- If younger transaction accesses data modified by older, it waits or aborts

**Example:**
```
Transaction T1 (timestamp: 100)
Transaction T2 (timestamp: 150)

Both try to update same row:
→ T1 executes first (lower timestamp)
→ T2 waits for T1 to complete
```

---

#### 3. Optimistic Concurrency Control

**Assumption:** Conflicts are rare.

**How It Works:**
1. **Read Phase:** Transaction reads data without locks
2. **Validation Phase:** Before commit, check if other transactions modified data
3. **Write Phase:** If valid, commit; if conflict, abort and retry

**Pros:** Low overhead when conflicts are rare
**Cons:** High abort rate when conflicts are common

---

### 11. Data Consistency Models

Distributed databases must balance **consistency, availability, and partition tolerance** (CAP Theorem).

#### Consistency Models

| Model                          | Description                                        | Use Case                       | Example Systems     |
| ------------------------------ | -------------------------------------------------- | ------------------------------ | ------------------- |
| **Strong Consistency**         | All users see same data instantly after update     | Banking, financial systems     | Google Spanner      |
| **Eventual Consistency**       | Updates propagate over time, temporary divergence  | Social media, content delivery | DynamoDB, Cassandra |
| **Causal Consistency**         | Related updates seen in order                      | Messaging, collaborative apps  | MongoDB             |
| **Read-Your-Writes**           | User sees their own updates immediately            | User profiles, settings        | Most systems        |
| **Monotonic Reads**            | Once data read, future reads show same or newer    | Caching, timelines             | Redis               |
| **Monotonic Writes**           | Writes by same user processed in order             | Log files, append-only data    | Kafka               |

**Trade-offs:**
- **Strong Consistency:** High latency, lower availability
- **Eventual Consistency:** Low latency, high availability, temporary stale data

---

### 12. Query Processing in Distributed Databases

**Goal:** Execute queries efficiently across multiple nodes, minimizing data transfer.

**Steps:**

1. **Query Decomposition:** Break query into subqueries for each site
2. **Data Localization:** Determine which nodes have relevant data
3. **Optimization:** Choose best execution plan (minimize network cost)
4. **Execution:** Send subqueries to nodes, execute in parallel
5. **Result Merging:** Combine results from all nodes

**Example:**

```sql
-- User query (appears as single database)
SELECT SUM(sales_amount) AS total_sales
FROM sales
WHERE sale_date >= '2024-01-01';
```

**Behind the Scenes:**

```sql
-- Subquery sent to Node A (Asia)
SELECT SUM(sales_amount) AS asia_sales
FROM sales_asia
WHERE sale_date >= '2024-01-01';

-- Subquery sent to Node B (Europe)
SELECT SUM(sales_amount) AS europe_sales
FROM sales_europe
WHERE sale_date >= '2024-01-01';

-- Subquery sent to Node C (Americas)
SELECT SUM(sales_amount) AS americas_sales
FROM sales_americas
WHERE sale_date >= '2024-01-01';

-- Coordinator merges results
-- total_sales = asia_sales + europe_sales + americas_sales
```

**Optimization Strategies:**
- **Semijoin:** Transfer only necessary data for joins
- **Bloomjoin:** Use Bloom filters to reduce data transfer
- **Parallel Execution:** Execute subqueries simultaneously

---

### 13. Advantages of Distributed Databases

#### Detailed Benefits

| Benefit                       | Explanation                                      | Quantified Impact                  |
| ----------------------------- | ------------------------------------------------ | ---------------------------------- |
| ✅ **High Availability**       | Data replication ensures uptime                  | 99.9% → 99.99% uptime              |
| 🚀 **Faster Local Access**    | Users access nearby node                         | Latency: 500ms → 50ms (10x faster) |
| 💪 **Fault Tolerance**        | Node failure doesn't stop system                 | Single node failure: 0% downtime   |
| 💡 **Modular Growth**         | Add servers as data grows                        | Scale from 1TB → 100TB easily      |
| 🔐 **Improved Security**      | Sensitive data isolated to local servers         | GDPR compliance, regional isolation|
| ⚡ **Load Balancing**         | Distribute queries across nodes                  | Handle 10K concurrent users        |
| 💰 **Cost-Effective**         | Commodity hardware vs expensive single server    | Hardware cost: \$100K → \$30K       |
| 🌍 **Geographic Distribution**| Serve global users from regional data centers    | EU users → <50ms latency           |
| 📊 **Parallel Processing**    | Execute queries on multiple nodes simultaneously | Aggregate queries 5-10x faster     |

---

### 14. Disadvantages of Distributed Databases

#### Challenges and Trade-offs

| Limitation                    | Description                                   | Mitigation Strategy                |
| ----------------------------- | --------------------------------------------- | ---------------------------------- |
| ❌ **Complex Design**          | Requires careful data distribution planning   | Use proven architectures           |
| 🕓 **Higher Latency**         | Network delays between sites                  | Data locality, caching             |
| 🧩 **Data Consistency Issues** | Synchronization required across nodes        | Eventual consistency, 2PC          |
| 🧠 **Costly Maintenance**     | Replication + communication overhead          | Automation tools                   |
| 🧮 **Difficult Debugging**    | Failures harder to trace across nodes         | Centralized logging, monitoring    |
| 🔒 **Security Complexity**    | More nodes = larger attack surface            | Encryption, network segmentation   |
| 💸 **Network Costs**          | Data transfer between regions expensive       | Minimize cross-region queries      |
| 🧪 **Testing Complexity**     | Simulating distributed failures difficult     | Chaos engineering tools            |

---

### 15. Real-World Examples

#### Industry Implementations

| System                  | Type            | Description                                      | Scale                         |
| ----------------------- | --------------- | ------------------------------------------------ | ----------------------------- |
| **Google Spanner**      | SQL, Distributed| Global distributed SQL with strong consistency   | Petabytes, worldwide          |
| **Amazon DynamoDB**     | NoSQL, Key-Value| Highly available NoSQL with eventual consistency | Trillions of requests/day     |
| **Apache Cassandra**    | NoSQL, Wide-Column| Peer-to-peer distributed, no single point of failure | Instagram, Netflix         |
| **CockroachDB**         | SQL, Distributed| Cloud-native distributed SQL, PostgreSQL compatible | Global financial systems   |
| **MongoDB Cluster**     | NoSQL, Document | Horizontal sharding & replication for scale      | eBay, Adobe, Expedia          |
| **Amazon Aurora**       | SQL, Distributed| MySQL/PostgreSQL compatible, 6 replicas          | AWS customers                 |
| **Citus**               | SQL, Distributed| PostgreSQL extension for sharding                | Real-time analytics           |
| **TiDB**                | SQL, Distributed| MySQL compatible, horizontal scaling             | PingCAP customers             |

---

### 16. Real Example: Distributed Banking System

**Scenario:** National bank with branches across India.

**Architecture:**

| Branch Location | Data Stored             | Node Type     |
| --------------- | ----------------------- | ------------- |
| Delhi (HQ)      | All customer accounts   | Master node   |
| Mumbai          | Mumbai customer subset  | Replica + local|
| Bangalore       | Bangalore customer subset| Replica + local|
| Chennai         | Chennai customer subset | Replica + local|
| Kolkata         | Transaction logs        | Log server    |

**Operations:**

**1. Money Transfer (Distributed Transaction):**
```sql
BEGIN DISTRIBUTED TRANSACTION;

-- Deduct from Delhi account
UPDATE delhi.accounts 
SET balance = balance - 5000 
WHERE account_id = 'DEL123';

-- Credit to Mumbai account
UPDATE mumbai.accounts 
SET balance = balance + 5000 
WHERE account_id = 'MUM456';

COMMIT;
-- 2PC ensures both updates or none
```

**2. Balance Inquiry (Local Query):**
```sql
-- Customer in Mumbai queries their balance
-- Routed to Mumbai node (low latency)
SELECT balance FROM mumbai.accounts 
WHERE account_id = 'MUM456';
-- Response time: 10ms (local)
```

**3. National Report (Distributed Query):**
```sql
-- Head office generates total deposits report
SELECT branch, SUM(balance) AS total_deposits
FROM accounts
GROUP BY branch;

-- Behind the scenes:
-- Subquery to Delhi node
-- Subquery to Mumbai node
-- Subquery to Bangalore node
-- Subquery to Chennai node
-- Results merged at coordinator
```

**Benefits:**
- ✅ **2PC ensures atomicity:** Money transfer always consistent
- ✅ **Replication ensures availability:** If Mumbai node fails, HQ serves Mumbai customers
- ✅ **Local queries fast:** Mumbai customers get low latency
- ✅ **Logs for recovery:** Transaction logs enable rollback/replay

---

### 17. Optimization in Distributed Databases

#### Performance Strategies

| Strategy                         | Description                              | Benefit                          |
| -------------------------------- | ---------------------------------------- | -------------------------------- |
| **Local Query Processing**       | Process as much data locally as possible | Minimize network transfer        |
| **Parallel Query Execution**     | Split query across multiple nodes        | 5-10x faster aggregations        |
| **Join Optimization**            | Perform joins near data location         | Reduce data movement             |
| **Caching**                      | Cache frequently accessed data           | Instant reads for hot data       |
| **Materialized Views**           | Pre-compute and store query results      | 100x faster complex queries      |
| **Load Balancing**               | Spread requests evenly across nodes      | No single node bottleneck        |
| **Data Locality**                | Place related data on same node          | Avoid cross-node joins           |
| **Compression**                  | Compress data before network transfer    | Reduce bandwidth usage           |
| **Query Result Caching**         | Cache common query results               | Instant response for repeat queries|
| **Read Replicas**                | Offload read traffic to replicas         | Master handles only writes       |

---

### 18. Failure Recovery in Distributed Databases

#### Failure Types and Handling

| Failure Type                  | Description                       | Handling Strategy                         |
| ----------------------------- | --------------------------------- | ----------------------------------------- |
| **Site Failure**              | Node crashes or becomes unreachable| Failover to replica, continue operation  |
| **Communication Failure**     | Network partition between nodes   | Queue transactions, retry after reconnect|
| **Coordinator Crash (2PC)**   | Coordinator fails mid-transaction | Use 2PC logs for recovery, elect new coordinator|
| **Network Partition**         | Nodes split into isolated groups  | CAP theorem: Choose consistency or availability|
| **Data Corruption**           | Disk failure, data loss           | Restore from replica or backup           |
| **Transaction Failure**       | Deadlock, timeout, constraint violation| Rollback, retry with backoff         |
| **Replica Lag**               | Replica behind master             | Monitor lag, promote different replica   |
| **Byzantine Failure**         | Node sends incorrect data         | Consensus protocols (Paxos, Raft)        |

**Recovery Mechanisms:**
- **Transaction Logs:** Record all operations for replay
- **Checkpointing:** Periodic snapshots for fast recovery
- **Replication:** Automatic failover to healthy replica
- **Quorum:** Require majority of nodes to agree (Raft, Paxos)

---

### 19. CAP Theorem Overview (Preview)

**CAP Theorem Statement:**
> In a distributed database, you can only guarantee **2 out of 3** properties simultaneously:
>
> - **C (Consistency):** All nodes see same data at same time
> - **A (Availability):** Every request gets a response (success or failure)
> - **P (Partition Tolerance):** System continues despite network failures

**Trade-off Examples:**

| System Type    | Guarantees | Trade-off                              | Examples                    |
| -------------- | ---------- | -------------------------------------- | --------------------------- |
| **CP Systems** | C + P      | Sacrifice availability during partition| Google Spanner, HBase       |
| **AP Systems** | A + P      | Sacrifice consistency (eventual)       | Cassandra, DynamoDB, Riak   |
| **CA Systems** | C + A      | Cannot tolerate network partitions     | Traditional RDBMS (single node)|

**Note:** In practice, network partitions are inevitable in distributed systems, so most choose between **CP** or **AP**.

**Next Topic:** We'll explore CAP Theorem in depth with real-world examples and consistency models.

---

### 20. Interview Questions & Answers

**1. What is a distributed database?**

**Answer:**
A distributed database is a collection of interconnected databases stored across multiple physical locations (nodes, data centers) but managed as a single logical database. A DDBMS (Distributed Database Management System) ensures data consistency, transparency, and coordination across all sites.

**Key characteristics:**
- Data stored on multiple servers
- Appears as single database to users
- Ensures consistency, availability, and fault tolerance
- Supports transactions across multiple nodes

---

**2. Explain the difference between fragmentation and replication.**

**Answer:**

**Fragmentation:**
- **Definition:** Splitting data into smaller pieces stored at different sites
- **Types:** Horizontal (rows), Vertical (columns), Hybrid
- **Purpose:** Distribute load, improve performance by data locality
- **Storage:** Each fragment stored once (unless replicated)

**Replication:**
- **Definition:** Creating copies of same data at multiple sites
- **Types:** Full, Partial, Master-Slave, Master-Master
- **Purpose:** Improve availability, fault tolerance, read performance
- **Storage:** Same data stored multiple times

**Example:**
- **Fragmentation:** Sales data split by region (Asia, Europe, Americas) — each region's data on different node
- **Replication:** Customer table copied to 3 nodes — same data on all 3

---

**3. What is Two-Phase Commit protocol?**

**Answer:**
**Two-Phase Commit (2PC)** is a protocol that ensures atomicity in distributed transactions (all-or-nothing across multiple nodes).

**How It Works:**

**Phase 1 - Prepare (Voting):**
1. Coordinator asks all participants: "Can you commit?"
2. Each participant replies VOTE-COMMIT (yes) or VOTE-ABORT (no)
3. Participants lock resources and write to log

**Phase 2 - Commit (Decision):**
1. If all voted COMMIT → Coordinator sends GLOBAL-COMMIT
2. If any voted ABORT → Coordinator sends GLOBAL-ABORT
3. Participants execute decision and acknowledge
4. Transaction complete

**Pros:** Guarantees atomicity, all nodes reach same outcome
**Cons:** Blocking (nodes wait for coordinator), performance overhead

---

**4. How does a DDBMS maintain data consistency?**

**Answer:**
A DDBMS maintains consistency through multiple mechanisms:

**1. Two-Phase Commit (2PC):**
- Ensures all nodes commit or abort together

**2. Concurrency Control:**
- **Locking:** Prevents simultaneous conflicting updates
- **Timestamp ordering:** Older transactions get priority
- **Optimistic concurrency:** Validate before commit

**3. Replication Protocols:**
- **Synchronous replication:** Wait for all replicas to confirm
- **Asynchronous replication:** Update replicas eventually

**4. Consistency Models:**
- **Strong consistency:** All nodes see same data instantly
- **Eventual consistency:** Updates propagate over time

**5. Quorum Consensus:**
- Require majority of nodes to agree (e.g., 3 out of 5)

---

**5. What are the types of transparency in distributed systems?**

**Answer:**

| Transparency Type          | Description                                        |
| -------------------------- | -------------------------------------------------- |
| **Location**               | Users don't know physical location of data         |
| **Replication**            | Users unaware data is replicated                   |
| **Fragmentation**          | Users see one table, unaware it's fragmented       |
| **Concurrency**            | Multiple users access safely without knowing others|
| **Failure**                | System masks failures, users see continuous operation|
| **Performance**            | System optimizes automatically                     |
| **Scaling**                | Adding/removing nodes transparent to users         |

**Benefit:** Users interact with distributed database as if it were a single centralized database.

---

**6. What are the advantages of using a distributed database?**

**Answer:**

**Technical Advantages:**
- **Scalability:** Horizontal scaling by adding nodes
- **Performance:** Low latency via data locality
- **Fault Tolerance:** Node failures don't stop system
- **High Availability:** Replication ensures uptime (99.99%+)
- **Parallel Processing:** Execute queries on multiple nodes

**Business Advantages:**
- **Cost-Effective:** Commodity hardware vs expensive single server
- **Geographic Distribution:** Serve global users from regional data centers
- **Compliance:** Keep data in specific regions (GDPR)
- **Load Balancing:** Distribute traffic across nodes

**Real-World Impact:**
- E-commerce: Handle Black Friday traffic spikes
- Social media: Serve billions of users globally
- Banking: Ensure 24/7 availability

---

**7. Define CAP theorem.**

**Answer:**
**CAP Theorem** states that in a distributed database, you can only guarantee **2 out of 3** properties simultaneously:

- **C (Consistency):** All nodes see the same data at the same time
- **A (Availability):** Every request receives a response (success or failure)
- **P (Partition Tolerance):** System continues operating despite network failures

**Trade-offs:**
- **CP Systems (Consistency + Partition Tolerance):** Sacrifice availability during network partition (e.g., Google Spanner, HBase)
- **AP Systems (Availability + Partition Tolerance):** Sacrifice consistency, eventual consistency (e.g., Cassandra, DynamoDB)
- **CA Systems (Consistency + Availability):** Cannot tolerate partitions (traditional RDBMS on single node)

**Key Insight:** Network partitions are inevitable, so most distributed databases choose CP or AP.

---

**8. How does a distributed transaction differ from a centralized one?**

**Answer:**

| Aspect                 | Centralized Transaction         | Distributed Transaction                 |
| ---------------------- | ------------------------------- | --------------------------------------- |
| **Location**           | Single database server          | Multiple database servers               |
| **Coordination**       | Local transaction manager       | Distributed coordinator (2PC)           |
| **Communication**      | In-memory (fast)                | Network communication (slower)          |
| **Commit Protocol**    | Simple COMMIT                   | Two-Phase Commit (2PC)                  |
| **Failure Handling**   | Rollback single transaction     | Rollback across all participating nodes |
| **Complexity**         | Low                             | High (network delays, node failures)    |
| **Performance**        | Fast (local)                    | Slower (network overhead)               |
| **Consistency**        | Guaranteed (single point)       | Requires coordination protocols         |

**Example:**
- **Centralized:** Transfer money between two accounts in same database → single COMMIT
- **Distributed:** Transfer money from account in Delhi to Mumbai → 2PC ensures both update or rollback

---

### 21. Summary Table

| Concept                   | Description                                        |
| ------------------------- | -------------------------------------------------- |
| **Distributed Database**  | Data spread across multiple physical locations     |
| **DDBMS**                 | Software managing distribution, consistency, coordination|
| **Fragmentation**         | Splitting data (horizontal, vertical, hybrid)      |
| **Replication**           | Copying data to multiple sites                     |
| **Allocation**            | Placing data at specific nodes (static/dynamic)    |
| **2PC**                   | Two-Phase Commit ensures atomic transactions       |
| **Transparency**          | Users see single logical database                  |
| **Consistency Models**    | Strong (instant sync) vs Eventual (propagates)     |
| **Concurrency Control**   | Locking, timestamp ordering, optimistic            |
| **CAP Theorem**           | Trade-off between Consistency, Availability, Partition Tolerance|
| **Query Processing**      | Decompose, localize, optimize, execute, merge      |
| **Failure Recovery**      | Transaction logs, replication, quorum, checkpointing|

---

### 22. Final Thought

> **"Distributed databases turn a single point of failure into a network of resilience."**

They power modern giants — from Google Drive to Amazon — ensuring your data is **everywhere yet always available.**

**Key Principles:**
- 🌍 **Scale horizontally:** Add nodes instead of bigger servers
- 📍 **Data locality:** Place data near users
- 🔄 **Replicate for availability:** Multiple copies ensure uptime
- ⚖️ **Balance CAP trade-offs:** Choose consistency or availability based on needs
- 🛡️ **Fault tolerance:** Design for failures, not perfection

**Real-World Success:**
- **Google Spanner:** Globally distributed SQL with strong consistency
- **Netflix:** Distributes content across AWS regions worldwide
- **Facebook:** Shards user data by geographic region
- **Amazon:** DynamoDB serves trillions of requests daily

Distributed databases enable the internet-scale applications we use every day! 🚀
''',
    codeSnippet: '''
-- ========================================
-- 1. HORIZONTAL FRAGMENTATION (SHARDING)
-- ========================================

-- Original sales table (centralized)
CREATE TABLE sales (
  sale_id INT PRIMARY KEY,
  sale_date DATE,
  region VARCHAR(20),
  amount DECIMAL(10,2)
);

-- Fragmented by region across 3 nodes

-- Node 1 (Asia):
CREATE TABLE sales_asia AS 
SELECT * FROM sales WHERE region = 'Asia';

-- Node 2 (Europe):
CREATE TABLE sales_europe AS 
SELECT * FROM sales WHERE region = 'Europe';

-- Node 3 (Americas):
CREATE TABLE sales_americas AS 
SELECT * FROM sales WHERE region = 'Americas';

-- Query appears unified to user:
-- SELECT * FROM sales WHERE region = 'Asia';
-- Routed automatically to sales_asia on Node 1

-- ========================================
-- 2. VERTICAL FRAGMENTATION
-- ========================================

-- Separate frequently accessed from restricted data

-- Node 1: Basic employee info (public)
CREATE TABLE employee_info (
  emp_id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  department VARCHAR(50),
  hire_date DATE
);

-- Node 2: Salary details (restricted access)
CREATE TABLE employee_salary (
  emp_id INT PRIMARY KEY,
  salary DECIMAL(10,2),
  bonus DECIMAL(10,2),
  bank_account VARCHAR(20),
  FOREIGN KEY (emp_id) REFERENCES employee_info(emp_id)
);

-- Join when both needed:
SELECT i.name, i.department, s.salary
FROM employee_info i
JOIN employee_salary s ON i.emp_id = s.emp_id
WHERE i.emp_id = 501;

-- ========================================
-- 3. REPLICATION SETUP (MySQL)
-- ========================================

-- Master node (Node A):
CREATE TABLE customers (
  customer_id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) ENGINE=InnoDB;

-- Configure master for replication
-- my.cnf:
-- server-id=1
-- log_bin=mysql-bin
-- binlog_do_db=mydb

-- Create replication user
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

-- Get master status
SHOW MASTER STATUS;
-- Note: File and Position

-- Replica node (Node B):
-- Configure replica
-- my.cnf:
-- server-id=2
-- relay-log=mysql-relay-bin

-- Connect to master
CHANGE MASTER TO
  MASTER_HOST='master_ip',
  MASTER_USER='repl_user',
  MASTER_PASSWORD='password',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;

-- Start replication
START SLAVE;

-- Check replication status
SHOW SLAVE STATUS\\G

-- ========================================
-- 4. DISTRIBUTED TRANSACTION (2PC)
-- ========================================

-- Money transfer across two branches

-- BEGIN DISTRIBUTED TRANSACTION
START TRANSACTION;

-- Step 1: Deduct from Account A (Node 1 - Delhi)
UPDATE branch_delhi.accounts 
SET balance = balance - 5000 
WHERE account_id = 'DEL123';

-- Step 2: Credit to Account B (Node 2 - Mumbai)
UPDATE branch_mumbai.accounts 
SET balance = balance + 5000 
WHERE account_id = 'MUM456';

-- Two-Phase Commit ensures both succeed or both rollback
COMMIT;

-- If any step fails:
-- ROLLBACK;
-- Both updates are undone

-- ========================================
-- 5. DISTRIBUTED QUERY (UNION)
-- ========================================

-- User sees single result from multiple nodes

-- Query total sales across all regions
(SELECT 'Asia' AS region, SUM(amount) AS total
 FROM sales_asia
 WHERE sale_date >= '2024-01-01')
UNION ALL
(SELECT 'Europe' AS region, SUM(amount) AS total
 FROM sales_europe
 WHERE sale_date >= '2024-01-01')
UNION ALL
(SELECT 'Americas' AS region, SUM(amount) AS total
 FROM sales_americas
 WHERE sale_date >= '2024-01-01');

-- Result:
-- | region   | total      |
-- |----------|------------|
-- | Asia     | 1,250,000  |
-- | Europe   | 980,000    |
-- | Americas | 1,450,000  |

-- ========================================
-- 6. LOCKING FOR CONCURRENCY CONTROL
-- ========================================

-- Transaction 1: Lock account before update
BEGIN TRANSACTION;

-- Acquire exclusive lock
SELECT balance 
FROM accounts 
WHERE account_id = 501 
FOR UPDATE;

-- Update balance (other transactions blocked)
UPDATE accounts 
SET balance = balance - 1000 
WHERE account_id = 501;

COMMIT;
-- Lock released

-- Transaction 2 (waits until T1 commits):
SELECT balance FROM accounts WHERE account_id = 501;

-- ========================================
-- 7. PARTITIONING + REPLICATION
-- ========================================

-- Combine fragmentation and replication

-- Node 1A (Asia Primary):
CREATE TABLE sales_asia (
  sale_id INT PRIMARY KEY,
  sale_date DATE,
  amount DECIMAL(10,2)
);

-- Node 1B (Asia Replica):
-- Replicates from Node 1A
CREATE TABLE sales_asia (
  sale_id INT PRIMARY KEY,
  sale_date DATE,
  amount DECIMAL(10,2)
);

-- Benefits:
-- - Fragmentation: Data locality for Asia
-- - Replication: High availability if 1A fails

-- ========================================
-- 8. DISTRIBUTED JOIN OPTIMIZATION
-- ========================================

-- Avoid cross-node joins when possible

-- Bad: Join across nodes (expensive)
SELECT c.name, o.total
FROM customers c  -- Node A
JOIN orders o ON c.customer_id = o.customer_id  -- Node B
WHERE c.country = 'USA';

-- Good: Replicate small reference table
-- Replicate customers to Node B (orders node)
CREATE TABLE customers_replica AS 
SELECT * FROM customers;

-- Now join locally on Node B (fast)
SELECT c.name, o.total
FROM customers_replica c
JOIN orders o ON c.customer_id = o.customer_id
WHERE c.country = 'USA';

-- ========================================
-- 9. TIMESTAMP ORDERING CONCURRENCY
-- ========================================

-- Each transaction gets timestamp

-- Transaction T1 (timestamp: 100)
BEGIN TRANSACTION;  -- TS=100
UPDATE accounts SET balance = balance + 500 WHERE account_id = 1;
COMMIT;

-- Transaction T2 (timestamp: 150)
BEGIN TRANSACTION;  -- TS=150
UPDATE accounts SET balance = balance - 200 WHERE account_id = 1;
-- T2 sees T1's update (older timestamp processed first)
COMMIT;

-- Result: Updates applied in timestamp order

-- ========================================
-- 10. EVENTUAL CONSISTENCY EXAMPLE
-- ========================================

-- Update propagates asynchronously

-- Node A (Master): Write
INSERT INTO products (product_id, name, price)
VALUES (501, 'Laptop', 999.99);
-- Committed on Node A at time T0

-- Node B (Replica): Read at T0 + 50ms
SELECT * FROM products WHERE product_id = 501;
-- May not see new product yet (replication lag)

-- Node B: Read at T0 + 2 seconds
SELECT * FROM products WHERE product_id = 501;
-- Now sees new product (eventually consistent)

-- ========================================
-- 11. STRONG CONSISTENCY (SYNCHRONOUS)
-- ========================================

-- Wait for all replicas before COMMIT

BEGIN TRANSACTION;

-- Insert on master
INSERT INTO orders (order_id, customer_id, total)
VALUES (12345, 501, 1500.00);

-- Synchronous replication:
-- 1. Master writes to log
-- 2. Sends to all replicas
-- 3. Waits for all replicas to confirm
-- 4. Only then COMMIT succeeds

COMMIT;
-- All replicas updated before COMMIT returns
-- User sees consistent data on any node

-- ========================================
-- 12. DISTRIBUTED AGGREGATE QUERY
-- ========================================

-- Calculate total sales across all nodes

-- Coordinator sends subqueries to all nodes:

-- Node Asia:
SELECT SUM(amount) AS asia_total FROM sales_asia;
-- Returns: 1,250,000

-- Node Europe:
SELECT SUM(amount) AS europe_total FROM sales_europe;
-- Returns: 980,000

-- Node Americas:
SELECT SUM(amount) AS americas_total FROM sales_americas;
-- Returns: 1,450,000

-- Coordinator merges:
-- total_sales = 1,250,000 + 980,000 + 1,450,000 = 3,680,000

-- User sees:
SELECT SUM(amount) AS total_sales FROM sales;
-- Result: 3,680,000

-- ========================================
-- 13. FAILURE RECOVERY WITH LOGS
-- ========================================

-- Transaction log for recovery

-- Transaction T1:
BEGIN TRANSACTION;
-- Log: START T1

UPDATE accounts SET balance = balance - 500 WHERE account_id = 1;
-- Log: UPDATE T1, account 1, old=5000, new=4500

UPDATE accounts SET balance = balance + 500 WHERE account_id = 2;
-- Log: UPDATE T1, account 2, old=3000, new=3500

COMMIT;
-- Log: COMMIT T1

-- If node crashes before COMMIT:
-- Recovery: Read log, see no COMMIT, ROLLBACK all updates

-- If node crashes after COMMIT:
-- Recovery: Read log, replay updates if needed

-- ========================================
-- 14. QUORUM CONSENSUS (RAFT/PAXOS)
-- ========================================

-- Require majority of nodes to agree

-- 5-node cluster: Need 3 nodes to agree (quorum)

-- Write operation:
INSERT INTO users (user_id, name) VALUES (501, 'Alice');

-- Process:
-- 1. Send to all 5 nodes
-- 2. Wait for 3 nodes to confirm
-- 3. Once 3 confirm, COMMIT
-- 4. Update remaining 2 nodes asynchronously

-- Benefits:
-- - Tolerates 2 node failures (3 still available)
-- - Ensures consistency (majority agree)

-- ========================================
-- 15. MATERIALIZED VIEW FOR OPTIMIZATION
-- ========================================

-- Pre-compute expensive distributed query

-- Expensive query (runs across 3 nodes):
CREATE MATERIALIZED VIEW sales_summary AS
SELECT 
  region,
  DATE_TRUNC('month', sale_date) AS month,
  SUM(amount) AS monthly_total,
  COUNT(*) AS order_count
FROM sales  -- Distributed across nodes
GROUP BY region, DATE_TRUNC('month', sale_date);

-- Refresh periodically:
REFRESH MATERIALIZED VIEW sales_summary;

-- Fast query (reads from materialized view):
SELECT * FROM sales_summary 
WHERE region = 'Asia' AND month >= '2024-01-01';
-- 100x faster than querying distributed tables

-- ========================================
-- 16. LOAD BALANCING WITH READ REPLICAS
-- ========================================

-- Distribute read traffic across replicas

-- Master node (writes only):
-- INSERT, UPDATE, DELETE → Master

-- Replica nodes (reads):
-- SELECT → Round-robin across replicas

-- Example routing logic (application layer):
-- Write query:
-- route_to_master("INSERT INTO orders VALUES (...)")

-- Read query:
-- route_to_replica("SELECT * FROM orders WHERE customer_id = 501")
-- Load balancer chooses: Replica 1, 2, or 3

-- Benefits:
-- - Master handles only writes (not overloaded)
-- - Reads distributed across 3 replicas (3x capacity)

-- ========================================
-- 17. GEOGRAPHIC DISTRIBUTION
-- ========================================

-- Place data near users for low latency

-- US Data Center (Node US):
CREATE TABLE users_us (
  user_id INT PRIMARY KEY,
  name VARCHAR(100),
  region VARCHAR(10) DEFAULT 'US'
);

-- EU Data Center (Node EU):
CREATE TABLE users_eu (
  user_id INT PRIMARY KEY,
  name VARCHAR(100),
  region VARCHAR(10) DEFAULT 'EU'
);

-- Routing logic:
-- If user IP in USA → query Node US (latency: 20ms)
-- If user IP in EU → query Node EU (latency: 15ms)

-- Global query (admin dashboard):
SELECT COUNT(*) AS total_users FROM (
  SELECT * FROM users_us
  UNION ALL
  SELECT * FROM users_eu
) AS all_users;
''',
    revisionPoints: [
      'Distributed database stores data across multiple physical locations (nodes, data centers)',
      'DDBMS (Distributed Database Management System) manages distribution, consistency, coordination',
      'Key components: Sites/Nodes, Network, DDBMS, Global Schema, Transaction Manager',
      'Benefits: Scalability (horizontal), Performance (low latency), Reliability (fault tolerance), Availability (replication)',
      'Types: Homogeneous (same DBMS), Heterogeneous (different DBMSs), Federated, Shared-Nothing',
      'Fragmentation splits data: Horizontal (rows by region/range), Vertical (columns), Hybrid (both)',
      'Replication copies data: Full (entire DB), Partial (frequent data), Master-Slave, Master-Master',
      'Allocation decides data placement: Static (fixed) or Dynamic (automatic based on usage)',
      'Transparency types: Location, Replication, Fragmentation, Concurrency, Failure, Performance, Scaling',
      'Distributed transaction updates data across multiple sites, must succeed or fail together (atomicity)',
      'Two-Phase Commit (2PC) ensures atomicity: Phase 1 Prepare (voting), Phase 2 Commit (decision)',
      'Concurrency control: Locking (2PL), Timestamp Ordering (older first), Optimistic (validate before commit)',
      'Consistency models: Strong (instant sync), Eventual (propagates over time), Causal, Read-Your-Writes',
      'Query processing: Decompose into subqueries, localize data, optimize, execute in parallel, merge results',
      'CAP Theorem: Choose 2 of 3 - Consistency, Availability, Partition Tolerance',
      'Advantages: High availability (99.99%+), Faster local access (low latency), Fault tolerance, Modular growth',
      'Disadvantages: Complex design, Higher network latency, Data consistency issues, Costly maintenance, Difficult debugging',
      'Real-world examples: Google Spanner (strong consistency), DynamoDB (eventual), Cassandra (peer-to-peer), CockroachDB',
      'Failure recovery: Transaction logs, Replication failover, Quorum consensus, Checkpointing',
      'Optimization strategies: Local processing, Parallel execution, Join optimization, Caching, Materialized views, Load balancing',
      '2PC disadvantages: Blocking (nodes wait for coordinator), Performance overhead (two rounds), Single point of failure',
      'Distributed banking: 2PC ensures atomic money transfers, Replication ensures availability, Logs enable recovery',
      'Geographic distribution enables GDPR compliance and low latency for global users',
      'Quorum consensus (Raft, Paxos) requires majority of nodes to agree for consistency',
      'Final principle: Distributed databases turn single point of failure into network of resilience',
    ],
    quizQuestions: [
      Question(
        question: 'What is a distributed database?',
        options: [
          'Database with multiple tables',
          'Database stored across multiple physical locations appearing as single logical database',
          'Database with indexes',
          'Cloud-hosted database'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is horizontal fragmentation?',
        options: [
          'Dividing columns across nodes',
          'Dividing rows across nodes (sharding)',
          'Creating backups',
          'Indexing tables'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does Two-Phase Commit (2PC) ensure?',
        options: [
          'Fast queries',
          'Atomicity in distributed transactions (all-or-nothing across nodes)',
          'Data compression',
          'Automatic backups'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is replication transparency?',
        options: [
          'Data visible to all users',
          'Users unaware data is replicated across multiple sites',
          'Fast replication',
          'No replication'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which consistency model has updates propagate over time?',
        options: [
          'Strong Consistency',
          'Eventual Consistency',
          'No Consistency',
          'Perfect Consistency'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a key advantage of distributed databases?',
        options: [
          'Simpler design',
          'Fault tolerance - node failure does not stop system',
          'No network needed',
          'Single point of control'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does CAP theorem state?',
        options: [
          'Can guarantee all three properties always',
          'Can only guarantee 2 of 3: Consistency, Availability, Partition Tolerance',
          'Consistency is most important',
          'Partitions never happen'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is vertical fragmentation?',
        options: [
          'Dividing rows across nodes',
          'Dividing columns across nodes',
          'Replicating data',
          'Compressing data'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which system uses strong consistency?',
        options: [
          'Cassandra',
          'Google Spanner',
          'DynamoDB',
          'Riak'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is a disadvantage of distributed databases?',
        options: [
          'Too simple',
          'Data consistency issues due to synchronization complexity',
          'Cannot scale',
          'Too fast'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is DDBMS?',
        options: [
          'Database backup tool',
          'Distributed Database Management System managing distribution and coordination',
          'Data compression tool',
          'Disk storage'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'How does distributed query processing work?',
        options: [
          'Query runs on single node',
          'Decompose query, send subqueries to nodes, execute in parallel, merge results',
          'Query fails',
          'Requires manual execution'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'cap_theorem',
    title: '22. CAP Theorem & Consistency Models',
    explanation: '''## CAP Theorem & Consistency Models in Databases

### 1. Introduction

Modern distributed systems must balance **data consistency**, **availability**, and **network reliability**. But — according to the **CAP Theorem**, it's **impossible to achieve all three simultaneously**.

**CAP Theorem (Brewer's Theorem):**
> 💡 "A distributed system can guarantee only **two** out of the following three properties: **Consistency (C)**, **Availability (A)**, and **Partition Tolerance (P)**."

**The Fundamental Challenge:**
- 🌍 Distributed systems span multiple data centers and geographies
- 📡 Network failures are inevitable (partitions will happen)
- ⚖️ Must choose between consistency and availability during failures
- 🎯 Different use cases require different trade-offs

**Proposed by:** Eric Brewer in 2000, proven as theorem by Seth Gilbert and Nancy Lynch in 2002

**Why It Matters:**
- Guides architecture decisions for distributed databases
- Helps choose appropriate database for specific use cases
- Explains why different databases behave differently during failures
- Fundamental to understanding distributed systems behavior

---

### 2. Understanding the Three Properties

#### Detailed Definitions

**Consistency (C):**
> 🧠 **Definition:** Every read receives the most recent write or an error. All nodes see the same data at the same time.

**Characteristics:**
- All clients see same data simultaneously
- Reads reflect most recent successful writes
- System appears as if there's only one copy of data
- Strong data integrity guarantees

**Example:**
- ✅ You update your profile picture
- ✅ Everyone immediately sees the new picture (not the old one)
- ✅ No stale data served to anyone

---

**Availability (A):**
> ⚡ **Definition:** Every request receives a valid response (success or failure), even if some nodes fail. System never refuses requests.

**Characteristics:**
- System always responds to queries
- No downtime even if nodes fail
- Requests never time out waiting for synchronization
- May serve stale data to maintain availability

**Example:**
- ✅ You can still access website even if one server is down
- ✅ Search results always returned (even if slightly outdated)
- ✅ No "system unavailable" errors

---

**Partition Tolerance (P):**
> 🌐 **Definition:** System continues to operate even if communication between nodes is lost (network partition). System survives network failures.

**Characteristics:**
- Handles network failures gracefully
- Nodes can be temporarily isolated
- System doesn't crash when nodes can't communicate
- Essential for geographically distributed systems

**Example:**
- ✅ If network splits into two groups, both still work independently
- ✅ System continues despite fiber cut or router failure
- ✅ Temporary isolation doesn't cause total failure

---

#### Comparison Table

| Property               | Focus                      | Guarantee                           | Trade-off When Violated                |
| ---------------------- | -------------------------- | ----------------------------------- | -------------------------------------- |
| **Consistency (C)**    | Data accuracy              | All nodes see same data             | System may reject requests             |
| **Availability (A)**   | System uptime              | Always responds to requests         | May serve stale data                   |
| **Partition Tolerance (P)** | Network resilience    | Survives network failures           | Must sacrifice C or A during partition |

---

### 3. CAP Theorem Visualization

**The CAP Triangle:**

```
                Consistency (C)
                /            \\
               /              \\
              /                \\
             /                  \\
            /                    \\
           /                      \\
          /       CA Systems       \\
         /      (Impossible in      \\
        /      distributed env)      \\
       /                              \\
      /________________________________\\
  Availability (A)         Partition Tolerance (P)
  
  CP Systems                          AP Systems
  (Consistent & Tolerant)             (Available & Tolerant)
```

**Key Insight:**
- **Pick any two sides of the triangle**
- Cannot have all three simultaneously
- In practice, **P is non-negotiable** (network failures inevitable)
- Real trade-off: **CP vs AP**

---

### 4. Real-World Analogy

**WhatsApp-like Messaging App:**

**Scenario 1: Strong Consistency (CP)**
- You send: "Meeting at 3 PM"
- System ensures all friends see message instantly or none at all
- If network partition occurs → message delivery delayed until sync
- **Trade-off:** May see "sending..." for longer, but guaranteed consistency

**Scenario 2: High Availability (AP)**
- You send: "Meeting at 3 PM"
- Message appears immediately in your chat
- Friends see it within seconds (eventual consistency)
- If network partition occurs → message queued and delivered later
- **Trade-off:** Friends may see message at different times

**Network Partition Example:**
- 🌐 Your phone loses connection briefly
- **CP Approach:** App waits for connection, message not sent until online
- **AP Approach:** App stores message locally, sends when connection restored

---

### 5. Why CAP Matters

#### Real-World Impact

**For System Architects:**
- 🎯 Choose database based on business requirements
- ⚖️ Balance consistency vs availability trade-offs
- 🛠️ Design fault tolerance strategies
- 📊 Set realistic SLA expectations

**For Business Decisions:**
- 💰 Financial systems → Choose CP (consistency critical)
- 📱 Social media → Choose AP (availability critical)
- 🛒 E-commerce → Mixed (checkout CP, catalog AP)

**Why Partition Tolerance is Inevitable:**
- Network cables cut
- Router failures
- Data center connectivity issues
- Geographic latency spikes
- DDoS attacks
- Hardware failures

**Result:** All modern distributed systems must be partition-tolerant, making real choice between **C** and **A**.

---

### 6. The Three CAP Combinations

#### CP Systems (Consistency + Partition Tolerance)

**Characteristics:**
- ✅ **Prioritizes accuracy over availability**
- ✅ **Data always consistent** across all nodes
- ⚠️ **May reject requests** during partition
- ⚠️ **Higher latency** (wait for synchronization)
- ✅ **Strong consistency guarantees**

**How It Works:**
1. Write request arrives
2. System propagates to all nodes
3. Waits for majority to confirm
4. If partition detected → rejects request
5. Returns success only when all nodes consistent

**Use Cases:**
- 💰 Banking and financial transactions
- 📊 Inventory management (prevent overselling)
- 🔐 Authentication systems
- 🏥 Medical records
- 📝 Legal document management

**Example Databases:**
- **HBase:** Strong consistency, sacrifices availability during splits
- **MongoDB (strict mode):** Write concern majority for consistency
- **Google Spanner:** TrueTime API ensures global consistency
- **Redis (single master):** Consistent but may be unavailable
- **Zookeeper:** Coordination service with strong consistency

**Code Example:**

```sql
-- CP System: Transaction rejected if partition occurs
BEGIN TRANSACTION;

UPDATE accounts SET balance = balance - 500 
WHERE account_id = 'ACC001';

UPDATE accounts SET balance = balance + 500 
WHERE account_id = 'ACC002';

COMMIT;
-- If any node unreachable → transaction aborted
-- Ensures consistent balance across all replicas
```

**Benefits:**
- ✅ No stale data ever served
- ✅ All users see identical data
- ✅ Suitable for critical operations

**Drawbacks:**
- ❌ System may be unavailable during failures
- ❌ Higher latency (synchronization overhead)
- ❌ Reduced throughput

---

#### AP Systems (Availability + Partition Tolerance)

**Characteristics:**
- ✅ **Always responds to requests**
- ✅ **System never goes down**
- ⚠️ **Data might be slightly outdated** (eventual consistency)
- ✅ **Low latency** (no waiting for sync)
- ✅ **High throughput**

**How It Works:**
1. Write request arrives at any node
2. Node accepts immediately (local write)
3. Asynchronously propagates to other nodes
4. Clients may temporarily see different data
5. Eventually all nodes converge

**Use Cases:**
- 📱 Social media feeds (Facebook, Twitter)
- 🎥 Video streaming catalogs (Netflix)
- 🛒 Product catalogs (e-commerce)
- 📊 Analytics dashboards
- 💬 Chat applications
- 📧 Email systems

**Example Databases:**
- **Cassandra:** Always available, tunable consistency
- **DynamoDB:** Highly available, eventual consistency
- **CouchDB:** Multi-master, conflict resolution
- **Riak:** Distributed key-value, high availability
- **Voldemort:** Eventually consistent, always on

**Code Example:**

```sql
-- AP System: Write accepted immediately
INSERT INTO posts (user_id, content, created_at)
VALUES (123, 'Hello World!', NOW());
-- Returns success immediately

-- Post appears instantly on your timeline
-- Other users see it within seconds (eventual consistency)
-- If partition occurs → post queued and synced later
```

**Benefits:**
- ✅ Zero downtime
- ✅ Fast response times
- ✅ Handles high traffic easily

**Drawbacks:**
- ❌ Temporary data inconsistency
- ❌ Complex conflict resolution
- ❌ Read-your-writes not guaranteed

---

#### CA Systems (Consistency + Availability)

**Characteristics:**
- ✅ **Both consistent and available**
- ❌ **Cannot tolerate partitions**
- 🏠 **Only works in single location** (no distribution)
- 🎯 **Rare in modern architectures**

**Why Practically Impossible in Distributed Systems:**
- Network partitions are inevitable
- Cannot guarantee both C and A when partition occurs
- Only feasible in single-node or tightly-coupled systems

**Use Cases:**
- Single-server databases
- Standalone applications
- Local development environments
- Small-scale applications

**Example Systems:**
- **MySQL (single instance):** Consistent and available, no partition tolerance
- **PostgreSQL (standalone):** Same as MySQL
- **SQLite:** File-based, no distribution
- **Traditional RDBMS (non-replicated):** Single point of failure

**Code Example:**

```sql
-- CA System: Single node, no distribution
BEGIN TRANSACTION;
UPDATE inventory SET stock = stock - 1 WHERE product_id = 101;
INSERT INTO orders (product_id, quantity) VALUES (101, 1);
COMMIT;
-- Consistent and available, but no fault tolerance
-- If server crashes → entire system down
```

**Limitation:**
- ❌ Single point of failure
- ❌ Cannot scale horizontally
- ❌ No geographic distribution
- ❌ Not suitable for modern cloud-native apps

---

### 7. Partition Tolerance Is Non-Negotiable

**Why P is Always Required:**

In modern distributed environments:
- 🌐 **Networks will fail** (cables cut, routers crash)
- 💥 **Nodes will go offline** (power outages, hardware failure)
- ⏱️ **Latency will happen** (congestion, distance)
- 🔥 **DDoS attacks** (targeted disruptions)

**Result:** All real-world distributed systems **must be partition-tolerant (P)**.

**Real Trade-off:**
```
Since P is required:
- Choose CP → Sacrifice Availability during partition
- Choose AP → Sacrifice Consistency during partition
```

**Examples of Partitions:**
- Fiber optic cable cut between data centers
- Router misconfiguration isolates nodes
- Cloud region connectivity issues
- Firewall blocks inter-node communication
- Network congestion causes timeout

**System Behavior During Partition:**

**CP System:**
```
Partition Detected → Reject Writes → Wait for Sync → Resume Operations
Users see: "Service temporarily unavailable"
```

**AP System:**
```
Partition Detected → Accept Writes Locally → Queue Sync → Merge Later
Users see: "Operation successful" (data syncs in background)
```

---

### 8. Eventual Consistency (Used in AP Systems)

**Definition:**
> 💬 Updates **propagate over time**, and eventually, **all nodes will be consistent**. You might read slightly outdated data now, but it'll synchronize soon.

**How It Works:**
1. Write accepted at one node
2. Asynchronously replicated to others
3. During propagation, reads may return old data
4. After replication completes, all nodes consistent

**Timeline Example:**
```
T0: Write to Node A (balance = 100)
T1: Client reads from Node B (balance = 90) ← stale data
T2: Replication completes
T3: Client reads from Node B (balance = 100) ← consistent
```

**Characteristics:**
- ⏱️ **Replication lag:** Seconds to minutes
- 🔄 **Convergence:** All nodes eventually agree
- 📊 **Read anomalies:** Temporary inconsistencies
- ✅ **High availability:** Always responsive

**Code Example:**

```sql
-- Cassandra query (AP system with eventual consistency)
-- Write on Node A
INSERT INTO messages (user_id, message, timestamp)
VALUES (5, 'Hello!', NOW());
-- Returns immediately

-- Read from Node B (milliseconds later)
SELECT * FROM messages WHERE user_id = 5;
-- May not include 'Hello!' yet (replication lag)

-- Read from Node B (2 seconds later)
SELECT * FROM messages WHERE user_id = 5;
-- Now includes 'Hello!' (replicated)
```

**Real-World Examples:**

**1. Social Media Post:**
- You post: "Happy Birthday!"
- Appears instantly on your profile
- Friends see it within 1-3 seconds
- Eventually consistent across all regions

**2. Amazon Shopping Cart:**
- Add item to cart
- Item appears immediately (local write)
- Syncs to other data centers asynchronously
- Eventually consistent globally

**Benefits:**
- ✅ Fast writes (no waiting)
- ✅ High availability
- ✅ Scalable

**Challenges:**
- ❌ Temporary inconsistencies
- ❌ Conflict resolution needed
- ❌ Complex for users to understand

---

### 9. Strong Consistency (Used in CP Systems)

**Definition:**
> 🔒 Every read reflects the **most recent successful write**, regardless of network delays. System may **deny requests** if synchronization isn't possible.

**How It Works:**
1. Write request arrives
2. System locks data across all nodes
3. Propagates to all replicas
4. Waits for majority confirmation
5. Returns success only after sync complete

**Guarantees:**
- 📖 **Linearizability:** Reads always see latest write
- 🔐 **No stale data:** Ever
- ⚖️ **Total order:** All operations have global order
- 🎯 **Predictable:** System behaves like single machine

**Code Example:**

```sql
-- Google Spanner (CP system with strong consistency)
BEGIN TRANSACTION;

-- Update in USA data center
UPDATE bank_accounts 
SET balance = balance - 1000 
WHERE account_id = 'ACC123';

-- System waits for all replicas to confirm
-- Uses TrueTime API for global synchronization
-- Returns success only when all nodes consistent

COMMIT;

-- Immediate read from Tokyo data center
SELECT balance FROM bank_accounts 
WHERE account_id = 'ACC123';
-- Guaranteed to see updated balance (-1000)
```

**Consensus Protocols:**
- **Paxos:** Classic consensus algorithm
- **Raft:** Understandable consensus (etcd, Consul)
- **Zab:** ZooKeeper Atomic Broadcast
- **2PC/3PC:** Two/Three-Phase Commit

**Real-World Examples:**

**1. Bank Transfer:**
- Deduct \$500 from Account A
- Add \$500 to Account B
- Both accounts updated atomically
- All reads see consistent balances

**2. Inventory Management:**
- Decrement stock on purchase
- All warehouses see same stock level
- Prevents overselling

**Benefits:**
- ✅ No data anomalies
- ✅ Predictable behavior
- ✅ Suitable for critical operations

**Challenges:**
- ❌ Higher latency (synchronization overhead)
- ❌ Lower availability (reject during partition)
- ❌ Complex to implement

---

### 10. BASE vs ACID Philosophy

#### Comparison Table

| Aspect                  | ACID (Traditional DB)        | BASE (Distributed/NoSQL)           |
| ----------------------- | ---------------------------- | ---------------------------------- |
| **Philosophy**          | Consistency first            | Availability first                 |
| **Atomicity**           | All or nothing               | Best effort, eventual consistency  |
| **Consistency**         | Strict, immediate            | Flexible, eventual                 |
| **Isolation**           | Strong (serializable)        | Weak (read committed or less)      |
| **Durability**          | Guaranteed permanent         | Eventually durable                 |
| **Focus**               | Correctness                  | Availability + Scalability         |
| **Use Case**            | Banking, financial           | Social media, e-commerce catalog   |
| **Example Systems**     | MySQL, PostgreSQL, Oracle    | Cassandra, DynamoDB, MongoDB       |
| **CAP Choice**          | CP or CA                     | AP                                 |

---

**ACID - Traditional Databases:**
- **A**tomicity: Transaction all-or-nothing
- **C**onsistency: Valid state before and after
- **I**solation: Concurrent transactions don't interfere
- **D**urability: Committed data persists

**BASE - Distributed Databases:**
- **B**asically **A**vailable: System always responds
- **S**oft state: State may change over time (eventual consistency)
- **E**ventual consistency: Eventually all nodes agree

**When to Use Each:**

**Use ACID (CP):**
- ✅ Financial transactions
- ✅ Inventory management
- ✅ Healthcare records
- ✅ Legal documents

**Use BASE (AP):**
- ✅ Social media feeds
- ✅ Product catalogs
- ✅ Logging and analytics
- ✅ Caching layers

---

### 11. Consistency Models in Distributed Databases

#### Detailed Consistency Models

| Model                       | Description                                   | Guarantee                          | Use Case                  |
| --------------------------- | --------------------------------------------- | ---------------------------------- | ------------------------- |
| **Strong Consistency**      | Every read returns latest write               | Linearizability                    | Banking transactions      |
| **Weak Consistency**        | No guarantee on when updates visible         | Best effort                        | Video streaming           |
| **Eventual Consistency**    | All nodes converge to same value over time    | Eventually same                    | DynamoDB, Cassandra       |
| **Causal Consistency**      | Preserves cause-effect relationships          | Related updates ordered            | Social feeds              |
| **Read-Your-Writes**        | User always sees their latest updates         | Session consistency                | Profile changes           |
| **Monotonic Reads**         | Once you see new data, never see older        | No time travel                     | Newsfeed refresh          |
| **Monotonic Writes**        | Writes by same user processed in order        | Write ordering                     | Log files                 |
| **Session Consistency**     | Consistency within single user session        | Per-session guarantees             | Shopping cart             |

---

**1. Strong Consistency (Linearizability):**
```
T1: Write X=1
T2: Write X=2
T3: Read X → Always returns 2 (latest write)
All nodes see same value at same time
```

**2. Eventual Consistency:**
```
T0: Write X=1 on Node A
T1: Read X from Node B → May return old value (0)
T5: Read X from Node B → Returns 1 (synced)
```

**3. Causal Consistency:**
```
Alice posts: "Who wants pizza?" (Event A)
Bob replies: "I do!" (Event B, caused by A)
Everyone sees A before B (cause-effect preserved)
```

**4. Read-Your-Writes:**
```
User updates profile picture
Same user immediately sees new picture
Other users may see old picture briefly
```

**5. Monotonic Reads:**
```
T1: Read X → Returns X=5
T2: Read X → Returns X=5 or X=6 (never X=4)
No going back in time
```

---

### 12. Real-World Examples

#### Industry Implementations

| System               | Type            | CAP Focus | Consistency Model | Use Case                    |
| -------------------- | --------------- | --------- | ----------------- | --------------------------- |
| **Google Spanner**   | Distributed SQL | CP        | Strong            | AdWords billing, global transactions |
| **MongoDB**          | NoSQL Document  | CP (configurable) | Eventual or Strong | Content management, catalogs |
| **Cassandra**        | NoSQL Wide-Column | AP      | Tunable (Quorum)  | Netflix, Instagram messages |
| **DynamoDB**         | NoSQL Key-Value | AP        | Eventual          | Amazon cart, session storage|
| **HBase**            | Big Data        | CP        | Strong            | Facebook Messages           |
| **CouchDB**          | NoSQL Document  | AP        | Eventual          | Mobile offline-first apps   |
| **Redis**            | In-Memory Cache | AP (single master) | Eventual | Caching, session store    |
| **Riak**             | NoSQL Key-Value | AP        | Eventual          | Gaming leaderboards         |

---

### 13. Tunable Consistency (Middle Ground)

**Definition:** Some databases allow configuring consistency level per query, balancing between strong and eventual consistency.

**Cassandra Example:**

```sql
-- Set consistency level to ONE (fastest, least consistent)
CONSISTENCY ONE;
SELECT * FROM orders WHERE user_id = 501;
-- Reads from single replica, may be stale

-- Set consistency level to QUORUM (balanced)
CONSISTENCY QUORUM;
SELECT * FROM orders WHERE user_id = 501;
-- Reads from majority of replicas, more consistent

-- Set consistency level to ALL (slowest, most consistent)
CONSISTENCY ALL;
SELECT * FROM orders WHERE user_id = 501;
-- Reads from all replicas, guaranteed latest
```

**Consistency Levels:**

| Level         | Replicas Read | Speed    | Consistency | Use Case                |
| ------------- | ------------- | -------- | ----------- | ----------------------- |
| **ONE**       | 1             | Fastest  | Weakest     | Non-critical reads      |
| **TWO**       | 2             | Fast     | Weak        | Slightly more reliable  |
| **THREE**     | 3             | Moderate | Moderate    | Balanced                |
| **QUORUM**    | N/2 + 1       | Moderate | Strong      | Most common choice      |
| **ALL**       | All N         | Slowest  | Strongest   | Critical data           |
| **LOCAL_ONE** | 1 local       | Fast     | Weak        | Geo-distributed, local reads |

**MongoDB Read Concern:**

```javascript
// Read with majority concern (strong consistency)
db.accounts.find({account_id: 123})
  .readConcern("majority");
// Waits for data replicated to majority

// Read with local concern (eventual consistency)
db.accounts.find({account_id: 123})
  .readConcern("local");
// Reads from nearest replica, may be stale
```

**DynamoDB Consistency:**

```python
# Strong consistent read
response = table.get_item(
    Key={'user_id': 123},
    ConsistentRead=True  # Wait for latest data
)

# Eventually consistent read (default)
response = table.get_item(
    Key={'user_id': 123},
    ConsistentRead=False  # Faster, may be stale
)
```

---

### 14. Real Example: E-Commerce Application

**Use Case Analysis:**

| Scenario                    | Required Property            | System Choice | Consistency Model  | Reasoning                              |
| --------------------------- | ---------------------------- | ------------- | ------------------ | -------------------------------------- |
| **Checkout Payment**        | Consistency                  | CP System     | Strong             | Prevent double-charging, ensure accurate inventory |
| **Product Catalog Search**  | Availability                 | AP System     | Eventual           | Users always see results, slight staleness OK |
| **User Profile Updates**    | Tunable                      | AP or CP      | Read-Your-Writes   | User sees own changes, others eventually |
| **Shopping Cart**           | Availability                 | AP System     | Session            | Add items fast, sync in background     |
| **Inventory Count**         | Consistency                  | CP System     | Strong             | Prevent overselling, accurate stock    |
| **Product Reviews**         | Availability                 | AP System     | Eventual           | Reviews sync eventually, availability key |
| **Order History**           | Consistency                  | CP System     | Strong             | Accurate order records critical        |
| **Product Recommendations** | Availability                 | AP System     | Weak               | Stale recommendations acceptable       |

**Architecture Example:**

```
E-Commerce System Architecture:

┌─────────────────────────────────────────────┐
│         Frontend Application                │
└───────────┬─────────────────────────────────┘
            │
            ├──────────────┬──────────────┐
            │              │              │
    ┌───────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
    │  Catalog API │ │ Cart API │ │ Checkout API│
    │   (AP/Weak)  │ │ (AP/Session)│ (CP/Strong)│
    └───────┬──────┘ └────┬─────┘ └──────┬──────┘
            │              │              │
    ┌───────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
    │  Cassandra   │ │  Redis   │ │  PostgreSQL │
    │  (AP System) │ │ (Cache)  │ │  (CP System)│
    └──────────────┘ └──────────┘ └─────────────┘
```

---

### 15. Trade-offs Summary

#### Comprehensive Comparison

| Aspect                  | CP System (Consistency Priority) | AP System (Availability Priority) |
| ----------------------- | -------------------------------- | --------------------------------- |
| **Consistency**         | ✅ Strong (always latest)         | ⚠️ Eventual (may be stale)         |
| **Availability**        | ⚠️ May reject requests            | ✅ Always responds                 |
| **Partition Tolerance** | ✅ Yes                            | ✅ Yes                             |
| **Latency**             | Higher (wait for sync)           | Lower (immediate response)        |
| **Throughput**          | Lower (coordination overhead)    | Higher (no waiting)               |
| **Complexity**          | Higher (consensus protocols)     | Moderate (conflict resolution)    |
| **Data Integrity**      | Guaranteed                       | Eventually guaranteed             |
| **Use Case**            | Banking, transactions, inventory | Social media, caching, catalogs   |
| **Example Systems**     | Google Spanner, HBase, MongoDB   | Cassandra, DynamoDB, CouchDB      |
| **User Experience**     | May see "unavailable" errors     | Always responsive                 |
| **Operational Cost**    | Higher (more coordination)       | Lower (simpler operations)        |

---

### 16. How CAP Theorem Guides Design

**System Design Process:**

**Step 1: Identify Priorities**
- ❓ Is data accuracy critical? → Favor Consistency
- ❓ Is uptime critical? → Favor Availability
- ❓ Can tolerate stale data? → Eventual consistency OK
- ❓ Financial/medical data? → Strong consistency required

**Step 2: Select Architecture**
- **CP Architecture:** Master-slave with synchronous replication
- **AP Architecture:** Multi-master with asynchronous replication
- **Hybrid:** Different databases for different components

**Step 3: Define Consistency Model**
- **Strong:** Banking, inventory
- **Eventual:** Social media, product catalog
- **Causal:** Messaging, feeds
- **Tunable:** Per-query configuration

**Step 4: Implement Fault Recovery**
- **CP Recovery:** Wait for partition healing, promote replica
- **AP Recovery:** Merge conflicts, last-write-wins
- **Monitoring:** Detect partitions early, alert on inconsistency

**Decision Tree:**

```
Does system REQUIRE strong consistency?
    ├─ YES → Use CP System (MongoDB, Spanner)
    │        Accept: Lower availability during failures
    │
    └─ NO → Does system REQUIRE high availability?
            ├─ YES → Use AP System (Cassandra, DynamoDB)
            │        Accept: Eventual consistency
            │
            └─ NO → Single node system OK
                    Use traditional RDBMS
```

---

### 17. SQL-like Illustration

**CP System Example (Strong Consistency):**

```sql
-- Distributed transaction with 2PC
BEGIN DISTRIBUTED TRANSACTION;

-- Node 1: Deduct from savings
UPDATE accounts 
SET balance = balance - 200 
WHERE account_id = 'ACC001' AND account_type = 'SAVINGS';

-- Node 2: Add to checking
UPDATE accounts 
SET balance = balance + 200 
WHERE account_id = 'ACC001' AND account_type = 'CHECKING';

-- If partition occurs during transaction:
-- → Transaction halts
-- → Waits for partition resolution
-- → Then completes or rolls back
-- → All nodes see consistent balances

COMMIT;
-- Success only when all nodes synchronized
```

**AP System Example (Eventual Consistency):**

```sql
-- Accept write immediately on local node
INSERT INTO posts (user_id, content, created_at)
VALUES (123, 'Check out this amazing sunset!', NOW());
-- Returns: "Post created successfully" (instant)

-- Behind the scenes:
-- 1. Write to local Node A
-- 2. Queue for replication to Nodes B, C, D
-- 3. Asynchronously propagate (takes 100-500ms)

-- Immediate read from Node B (50ms later)
SELECT * FROM posts WHERE user_id = 123;
-- May not include new post yet (replication lag)

-- Read from Node B (2 seconds later)
SELECT * FROM posts WHERE user_id = 123;
-- Now includes new post (replicated)
```

---

### 18. CAP in NoSQL Systems

#### NoSQL Database Classification

| Database       | Type           | CAP Category | Consistency Default | Tunable? | Key Feature                  |
| -------------- | -------------- | ------------ | ------------------- | -------- | ---------------------------- |
| **MongoDB**    | Document Store | CP           | Strong              | Yes      | Write concern, read preference|
| **Cassandra**  | Wide-Column    | AP           | Eventual            | Yes      | Quorum levels (ONE to ALL)   |
| **Redis**      | Key-Value      | AP           | Eventual            | No       | Single master, async replicas|
| **HBase**      | Wide-Column    | CP           | Strong              | No       | Strong consistency, region servers|
| **CouchDB**    | Document       | AP           | Eventual            | No       | Multi-master, conflict resolution|
| **DynamoDB**   | Key-Value      | AP           | Eventual            | Yes      | ConsistentRead parameter     |
| **Riak**       | Key-Value      | AP           | Eventual            | Yes      | N, R, W quorum values        |
| **Neo4j**      | Graph          | CA (single)  | Strong              | N/A      | Single-node, no distribution |
| **Couchbase**  | Document       | CP or AP     | Tunable             | Yes      | Per-bucket configuration     |

---

### 19. Common Interview Questions & Answers

**1. What is the CAP theorem?**

**Answer:**
CAP theorem states that a distributed system can guarantee only **two out of three** properties simultaneously:
- **C (Consistency):** All nodes see the same data at the same time
- **A (Availability):** Every request receives a response (success or failure)
- **P (Partition Tolerance):** System continues despite network failures

Proposed by Eric Brewer in 2000, proven in 2002. Fundamental principle for distributed database design.

---

**2. Why can't we have all three properties together?**

**Answer:**
During a **network partition** (nodes can't communicate):
- **To maintain Consistency:** Must reject requests until nodes sync → Sacrifice Availability
- **To maintain Availability:** Must respond with potentially stale data → Sacrifice Consistency

**Example:**
- Network splits data centers into two groups
- **CP choice:** Reject writes until connectivity restored (unavailable)
- **AP choice:** Accept writes on both sides, merge later (inconsistent temporarily)

Cannot have both consistency and availability during partition, making all three impossible simultaneously.

---

**3. Difference between AP and CP systems?**

**Answer:**

| Aspect          | CP System                      | AP System                       |
| --------------- | ------------------------------ | ------------------------------- |
| **Priority**    | Consistency over availability  | Availability over consistency   |
| **Behavior**    | Rejects requests if can't sync | Always responds                 |
| **Data**        | Always latest and accurate     | May be temporarily stale        |
| **Latency**     | Higher (wait for coordination) | Lower (immediate response)      |
| **Use Case**    | Banking, inventory             | Social media, catalogs          |
| **Examples**    | Google Spanner, HBase          | Cassandra, DynamoDB             |

---

**4. Explain eventual consistency with example.**

**Answer:**
**Eventual Consistency:** Updates propagate asynchronously, and all nodes eventually reach the same state. Temporary inconsistencies acceptable for high availability.

**Example - Facebook Post:**
1. Alice posts: "Just landed in Paris!"
2. Post written to Node A instantly (Alice sees it)
3. Asynchronously replicated to Nodes B, C, D
4. Bob (connected to Node B) sees post after 500ms delay
5. Charlie (connected to Node C) sees post after 1 second
6. Eventually (2-3 seconds), all nodes consistent

**Characteristics:**
- ✅ High availability (always accepts writes)
- ✅ Low latency (no waiting)
- ⚠️ Temporary inconsistencies
- ✅ Eventually all nodes agree

---

**5. What is the BASE principle?**

**Answer:**
**BASE** is an alternative to ACID for distributed systems:

- **B**asically **A**vailable: System always responds
- **S**oft state: State may change over time (even without input)
- **E**ventual consistency: System becomes consistent eventually

**Philosophy:**
- Favor availability over consistency
- Accept temporary inconsistencies
- Optimize for scalability and partition tolerance

**Contrast with ACID:**
- ACID: Strong consistency, may sacrifice availability
- BASE: High availability, eventual consistency

**Use Case:** NoSQL databases (Cassandra, DynamoDB) use BASE for internet-scale applications.

---

**6. How does Cassandra balance availability and consistency?**

**Answer:**
Cassandra uses **tunable consistency** - configure per query:

**Write Consistency Levels:**
- `ONE`: Write to 1 replica (fastest, least consistent)
- `QUORUM`: Write to majority (N/2 + 1) (balanced)
- `ALL`: Write to all replicas (slowest, most consistent)

**Read Consistency Levels:**
- `ONE`: Read from 1 replica (may be stale)
- `QUORUM`: Read from majority (more consistent)
- `ALL`: Read from all replicas (guaranteed latest)

**Formula:** R + W > N (where R = read replicas, W = write replicas, N = total replicas) ensures strong consistency

**Example:**
```sql
CONSISTENCY QUORUM;  -- Balance between speed and consistency
SELECT * FROM users WHERE user_id = 123;
```

This flexibility allows choosing appropriate trade-off per query.

---

**7. What type of system is Google Spanner under CAP theorem?**

**Answer:**
Google Spanner is a **CP system** (Consistency + Partition Tolerance).

**Characteristics:**
- **Strong consistency:** All nodes see same data globally
- **Partition tolerance:** Survives network failures
- **TrueTime API:** Synchronized clocks for global ordering
- **Paxos:** Consensus algorithm for coordination

**Trade-off:** Sacrifices some availability for consistency
- During partition, may reject writes temporarily
- Higher latency due to global synchronization
- Suitable for financial transactions requiring accuracy

**Innovation:** Spanner pushes boundaries of CAP by minimizing availability sacrifice through advanced clock synchronization and optimistic concurrency.

---

**8. What are tunable consistency levels?**

**Answer:**
Tunable consistency allows configuring the trade-off between consistency and availability **per query**, not system-wide.

**Benefits:**
- Critical queries can use strong consistency
- Non-critical queries can use eventual consistency
- Flexibility to balance performance and accuracy

**Examples:**

**Cassandra:**
```sql
-- Strong consistency for payment
CONSISTENCY ALL;
INSERT INTO payments VALUES (123, 100.00);

-- Eventual consistency for product views
CONSISTENCY ONE;
SELECT * FROM product_catalog;
```

**MongoDB:**
```javascript
// Read from majority for consistency
db.orders.find().readConcern("majority");

// Read from nearest for speed
db.orders.find().readConcern("local");
```

Enables optimizing per use case within same system.

---

### 20. Summary Table

| Concept                    | Key Point                                                       |
| -------------------------- | --------------------------------------------------------------- |
| **CAP Theorem**            | Choose any two: Consistency, Availability, Partition Tolerance  |
| **Consistency (C)**        | Every read sees latest write, all nodes synchronized            |
| **Availability (A)**       | Every request gets response, system never refuses               |
| **Partition Tolerance (P)**| System survives network failures, non-negotiable in distributed |
| **CP Systems**             | Focus on correctness, may reject requests (HBase, Spanner)      |
| **AP Systems**             | Focus on uptime, may serve stale data (Cassandra, DynamoDB)     |
| **CA Systems**             | Impossible in distributed systems (single-node RDBMS)           |
| **Strong Consistency**     | Linearizability, all nodes see same data instantly              |
| **Eventual Consistency**   | Nodes converge over time, temporary inconsistencies OK          |
| **Tunable Consistency**    | Configure consistency level per query (Cassandra, MongoDB)      |
| **BASE vs ACID**           | Availability-focused vs Consistency-focused philosophy          |
| **Real Trade-off**         | Since P required, choose between C (accuracy) and A (uptime)    |

---

### 21. Final Thought

> **"Distributed systems are a constant trade-off — between being *always correct* and *always online*."**

Understanding CAP and consistency models helps engineers **design systems that fail gracefully, recover intelligently, and scale globally.**

**Key Principles:**
- ⚖️ **No perfect solution:** Every choice has trade-offs
- 🎯 **Match to use case:** Banking needs CP, social media needs AP
- 🔧 **Tune when possible:** Use tunable consistency for flexibility
- 📊 **Monitor behavior:** Track consistency lag, partition events
- 🧪 **Test failures:** Chaos engineering to validate partition handling

**Remember:**
- **CP:** Choose when data accuracy is critical (financial, medical, inventory)
- **AP:** Choose when uptime is critical (social media, content delivery, caching)
- **Tunable:** Best of both worlds when available (Cassandra, MongoDB)

CAP theorem doesn't limit what's possible — it clarifies what's achievable, guiding informed architectural decisions. 🚀
''',
    codeSnippet: '''
-- ========================================
-- 1. CP SYSTEM - STRONG CONSISTENCY
-- ========================================

-- Google Spanner / MongoDB with strict mode
-- Transaction ensures all replicas synchronized

BEGIN TRANSACTION;

-- Deduct from savings account
UPDATE accounts 
SET balance = balance - 500 
WHERE account_id = 'ACC001' 
  AND account_type = 'SAVINGS';

-- Add to checking account
UPDATE accounts 
SET balance = balance + 500 
WHERE account_id = 'ACC001' 
  AND account_type = 'CHECKING';

-- System behavior during partition:
-- 1. Detects nodes cannot communicate
-- 2. Rejects transaction (unavailable)
-- 3. Returns error to client
-- 4. Waits for partition resolution
-- 5. Retries after connectivity restored

COMMIT;
-- Success only when all nodes confirm

-- Result:
-- ✅ Consistency guaranteed (all nodes see same balance)
-- ⚠️ Availability sacrificed (transaction rejected during partition)

-- ========================================
-- 2. AP SYSTEM - EVENTUAL CONSISTENCY
-- ========================================

-- Cassandra / DynamoDB
-- Accept write immediately, sync asynchronously

-- Insert post (accepted instantly)
INSERT INTO posts (post_id, user_id, content, created_at)
VALUES (UUID(), 123, 'Hello World!', NOW());
-- Returns: Success (written to local node)

-- Behind the scenes:
-- T0: Written to Node A
-- T1: Queued for replication to Nodes B, C, D
-- T2-T5: Asynchronously replicated

-- Read from different node (may be stale)
SELECT * FROM posts WHERE user_id = 123;
-- T0-T1: May not include new post (replication lag)
-- T5+: Includes new post (replicated)

-- Result:
-- ✅ Availability guaranteed (always responds)
-- ⚠️ Consistency sacrificed (temporary stale data)

-- ========================================
-- 3. TUNABLE CONSISTENCY - CASSANDRA
-- ========================================

-- Set consistency level to ONE (fast, weak consistency)
CONSISTENCY ONE;
SELECT * FROM product_catalog WHERE category = 'Electronics';
-- Reads from 1 replica (may be stale)
-- Use for: Non-critical reads, product browsing

-- Set consistency level to QUORUM (balanced)
CONSISTENCY QUORUM;
INSERT INTO orders (order_id, user_id, total)
VALUES (UUID(), 501, 150.00);
-- Writes to majority of replicas (N/2 + 1)
-- Use for: Standard operations

-- Set consistency level to ALL (slow, strong consistency)
CONSISTENCY ALL;
UPDATE inventory SET stock = stock - 1 WHERE product_id = 101;
-- Writes to all replicas (guaranteed consistent)
-- Use for: Critical data, inventory updates

-- ========================================
-- 4. MONGODB WRITE CONCERN
-- ========================================

-- Write concern: majority (strong consistency)
db.payments.insertOne(
  { user_id: 123, amount: 99.99, status: "pending" },
  { writeConcern: { w: "majority", wtimeout: 5000 } }
);
-- Waits for majority of replicas to confirm
-- Timeout: 5 seconds

-- Write concern: 1 (eventual consistency)
db.logs.insertOne(
  { user_id: 123, action: "login", timestamp: new Date() },
  { writeConcern: { w: 1 } }
);
-- Writes to primary only, returns immediately

-- Read concern: majority
db.accounts.find({ account_id: "ACC123" })
  .readConcern("majority");
-- Reads data replicated to majority

-- Read concern: local (may be stale)
db.accounts.find({ account_id: "ACC123" })
  .readConcern("local");
-- Reads from nearest replica

-- ========================================
-- 5. DYNAMODB CONSISTENT READ
-- ========================================

-- Strong consistent read (CP behavior)
-- import boto3
-- dynamodb = boto3.resource('dynamodb')
-- table = dynamodb.Table('Users')
-- 
-- response = table.get_item(
--     Key={'user_id': 123},
--     ConsistentRead=True  # Wait for latest data
-- )
-- Returns: Latest data (higher latency)

-- Eventually consistent read (AP behavior, default)
-- response = table.get_item(
--     Key={'user_id': 123},
--     ConsistentRead=False  # Read from any replica
-- )
-- Returns: Possibly stale data (lower latency)

-- ========================================
-- 6. CASSANDRA QUORUM FORMULA
-- ========================================

-- For strong consistency:
-- R + W > N
-- Where:
-- R = Number of replicas for read
-- W = Number of replicas for write
-- N = Total replicas (replication factor)

-- Example: N=3, W=2, R=2
-- 2 + 2 > 3 → Ensures strong consistency

-- Write to 2 replicas
CONSISTENCY QUORUM;  -- QUORUM = 2 for N=3
INSERT INTO users (user_id, name) VALUES (123, 'Alice');

-- Read from 2 replicas
CONSISTENCY QUORUM;
SELECT * FROM users WHERE user_id = 123;

-- At least one replica overlaps → sees latest write

-- ========================================
-- 7. CAUSAL CONSISTENCY EXAMPLE
-- ========================================

-- Social media post and reply (cause-effect)

-- Event A: Alice posts question
INSERT INTO posts (post_id, user_id, content, timestamp)
VALUES (1, 'alice', 'Who wants pizza?', NOW());

-- Event B: Bob replies (caused by Event A)
INSERT INTO posts (post_id, user_id, parent_id, content, timestamp)
VALUES (2, 'bob', 1, 'I do!', NOW());

-- Causal consistency guarantees:
-- Anyone who sees Event B (reply) must also see Event A (question)
-- Preserves cause-effect relationship

-- Query with causal consistency
SELECT * FROM posts WHERE parent_id IS NULL OR parent_id IN (
  SELECT post_id FROM posts WHERE user_id = 'alice'
);
-- Returns: Alice's question before Bob's reply

-- ========================================
-- 8. READ-YOUR-WRITES CONSISTENCY
-- ========================================

-- User updates profile
UPDATE user_profiles 
SET bio = 'Loves coding and coffee' 
WHERE user_id = 123;

-- Immediate read by same user
SELECT bio FROM user_profiles 
WHERE user_id = 123;
-- Guaranteed to see own update (read-your-writes)

-- Read by other user (may be stale)
SELECT bio FROM user_profiles 
WHERE user_id = 123;
-- May see old bio (eventual consistency)

-- Implementation: Session-based routing
-- Route reads to same replica that handled write

-- ========================================
-- 9. MONOTONIC READS CONSISTENCY
-- ========================================

-- Timeline of reads (no going back in time)

-- T1: Read value X=5
SELECT balance FROM accounts WHERE account_id = 'ACC123';
-- Returns: balance = 5000

-- T2: Read again (must see same or newer)
SELECT balance FROM accounts WHERE account_id = 'ACC123';
-- Returns: balance = 5000 or 5500 (never 4500)
-- No time travel, no seeing older versions

-- ========================================
-- 10. STRONG CONSISTENCY WITH 2PC
-- ========================================

-- Two-Phase Commit for distributed transaction

-- Coordinator sends PREPARE to all participants
-- PREPARE TRANSACTION 'txn_001';

-- Participant 1: Deduct from account
UPDATE accounts 
SET balance = balance - 200 
WHERE account_id = 'ACC001';
-- Locks data, writes to log, replies: VOTE-COMMIT

-- Participant 2: Add to account
UPDATE accounts 
SET balance = balance + 200 
WHERE account_id = 'ACC002';
-- Locks data, writes to log, replies: VOTE-COMMIT

-- Coordinator receives all votes
-- If all VOTE-COMMIT → sends GLOBAL-COMMIT
-- If any VOTE-ABORT → sends GLOBAL-ABORT

-- COMMIT PREPARED 'txn_001';
-- All participants commit atomically

-- ========================================
-- 11. EVENTUAL CONSISTENCY - CONFLICT RESOLUTION
-- ========================================

-- Two users update same document simultaneously

-- User A (Node 1): Updates bio at T1
UPDATE profiles 
SET bio = 'Loves hiking' 
WHERE user_id = 123;
-- Version: v1, timestamp: T1

-- User B (Node 2): Updates bio at T2 (during replication)
UPDATE profiles 
SET bio = 'Loves swimming' 
WHERE user_id = 123;
-- Version: v2, timestamp: T2

-- Conflict detected during merge
-- Resolution strategies:
-- 1. Last-Write-Wins (LWW): Keep T2 (newer timestamp)
-- 2. Version Vectors: Keep both, let user choose
-- 3. Merge: Combine both updates

-- Result: bio = 'Loves swimming' (LWW)

-- ========================================
-- 12. BASE TRANSACTION EXAMPLE
-- ========================================

-- ACID approach (CP system):
BEGIN TRANSACTION;
UPDATE inventory SET stock = stock - 1 WHERE product_id = 101;
INSERT INTO orders (order_id, product_id, quantity) VALUES (1, 101, 1);
COMMIT;
-- All-or-nothing, strong consistency

-- BASE approach (AP system):
-- Step 1: Update inventory (local node)
UPDATE inventory SET stock = stock - 1 WHERE product_id = 101;
-- Returns immediately

-- Step 2: Create order (async)
INSERT INTO orders (order_id, product_id, quantity) 
VALUES (1, 101, 1);
-- Queued for replication

-- Step 3: Eventually consistent
-- Inventory and orders sync across nodes over time
-- Compensating transactions if needed

-- ========================================
-- 13. CP vs AP SYSTEM COMPARISON
-- ========================================

-- Scenario: Bank transfer during network partition

-- CP System (e.g., Google Spanner):
BEGIN TRANSACTION;
UPDATE accounts SET balance = balance - 1000 WHERE id = 1;
UPDATE accounts SET balance = balance + 1000 WHERE id = 2;
-- If partition detected:
--   → Transaction rejected
--   → Error: "Service temporarily unavailable"
--   → Guarantees: No inconsistent state
COMMIT;

-- AP System (e.g., Cassandra):
-- Node A: Deduct from account 1
UPDATE accounts SET balance = balance - 1000 WHERE id = 1;
-- Returns: Success (local write)

-- Node B: Add to account 2
UPDATE accounts SET balance = balance + 1000 WHERE id = 2;
-- Returns: Success (local write)

-- If partition occurs:
--   → Both writes accepted independently
--   → Queued for replication
--   → Eventually consistent after partition heals
--   → Guarantees: Always available

-- ========================================
-- 14. TUNABLE CONSISTENCY - PER QUERY
-- ========================================

-- E-commerce application with mixed requirements

-- Critical: Payment processing (strong consistency)
CONSISTENCY ALL;
BEGIN BATCH;
INSERT INTO payments (payment_id, order_id, amount) 
VALUES (UUID(), 12345, 99.99);
UPDATE orders SET status = 'paid' WHERE order_id = 12345;
APPLY BATCH;
-- Waits for all replicas

-- Non-critical: Product search (eventual consistency)
CONSISTENCY ONE;
SELECT * FROM products 
WHERE category = 'Electronics' 
LIMIT 20;
-- Fast read from single replica

-- Balanced: User reviews (quorum)
CONSISTENCY QUORUM;
INSERT INTO reviews (product_id, user_id, rating, comment)
VALUES (101, 501, 5, 'Excellent product!');
-- Majority of replicas confirm

-- ========================================
-- 15. MONITORING CONSISTENCY LAG
-- ========================================

-- Check replication lag (PostgreSQL)
SELECT 
  client_addr,
  state,
  pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS lag_bytes,
  EXTRACT(EPOCH FROM (NOW() - pg_last_xact_replay_timestamp())) AS lag_seconds
FROM pg_stat_replication;

-- Alert if lag > threshold
-- If lag_seconds > 5:
--   → Investigate replication issues
--   → Consider routing reads to different replica

-- MongoDB replication status
db.printReplicationInfo();
-- Shows oplog size and replication lag

-- Cassandra nodetool
-- nodetool status
-- Shows node status and hints (queued updates)
''',
    revisionPoints: [
      'CAP theorem: Distributed system can guarantee only 2 of 3 - Consistency, Availability, Partition Tolerance',
      'Consistency (C): All nodes see same data at same time, every read returns latest write',
      'Availability (A): Every request gets response (success/failure), system never refuses',
      'Partition Tolerance (P): System continues despite network failures, non-negotiable in distributed systems',
      'CP systems (Consistency + Partition Tolerance): Reject requests during partition for data accuracy',
      'AP systems (Availability + Partition Tolerance): Always respond, may serve stale data temporarily',
      'CA systems (Consistency + Availability): Impossible in distributed systems, only single-node RDBMS',
      'Real trade-off: Since P required, choose between C (accuracy) and A (uptime) during partitions',
      'Strong consistency: Every read sees latest write immediately, linearizability guarantee',
      'Eventual consistency: Updates propagate over time, nodes eventually converge to same state',
      'Causal consistency: Preserves cause-effect relationships, related updates seen in order',
      'Read-Your-Writes: User always sees their own updates immediately, session consistency',
      'Monotonic reads: Once you see new data, never see older version, no time travel',
      'BASE (Basically Available, Soft-state, Eventually consistent): Alternative to ACID for distributed systems',
      'ACID (Atomicity, Consistency, Isolation, Durability): Traditional database guarantees, CP focus',
      'Tunable consistency: Configure consistency level per query (Cassandra QUORUM, MongoDB read concern)',
      'CP examples: Google Spanner (TrueTime), HBase, MongoDB strict mode, use for banking/inventory',
      'AP examples: Cassandra, DynamoDB, CouchDB, Riak, use for social media/caching',
      'Quorum formula: R + W > N ensures strong consistency (R=read replicas, W=write replicas, N=total)',
      'Network partitions inevitable: Cables cut, router failures, data center connectivity issues',
      'Consistency levels: ONE (fast, weak), QUORUM (balanced), ALL (slow, strong)',
      'E-commerce architecture: Checkout CP (strong), catalog AP (eventual), cart AP (session)',
      'Google Spanner pushes CAP boundaries with TrueTime API and Paxos consensus',
      'Design decision: Identify priorities → Select architecture → Define consistency model → Implement fault recovery',
      'Final principle: Trade-off between always correct (CP) and always online (AP), no perfect solution',
    ],
    quizQuestions: [
      Question(
        question: 'According to CAP theorem, what can a distributed system guarantee?',
        options: [
          'All three properties simultaneously',
          'Any two of three properties: Consistency, Availability, Partition Tolerance',
          'Only one property',
          'None of the properties'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Consistency (C) in CAP theorem?',
        options: [
          'System never crashes',
          'Every read receives most recent write, all nodes see same data',
          'Fast queries',
          'Data backup'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What does a CP system prioritize?',
        options: [
          'Availability over consistency',
          'Consistency + Partition Tolerance, may reject requests during partition',
          'Speed over accuracy',
          'All properties equally'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is eventual consistency?',
        options: [
          'Never becomes consistent',
          'Updates propagate over time, nodes eventually converge to same state',
          'Instant consistency',
          'No consistency'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'Which system is an example of AP (Availability + Partition Tolerance)?',
        options: [
          'Google Spanner',
          'Cassandra',
          'Single-node MySQL',
          'HBase'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is BASE principle?',
        options: [
          'Basically Available, Soft-state, Eventually consistent',
          'Backup And Storage Engine',
          'Basic Availability System Engine',
          'Best Available Storage Environment'
        ],
        correctIndex: 0,
      ),
      Question(
        question: 'Why is Partition Tolerance (P) non-negotiable in distributed systems?',
        options: [
          'It is optional',
          'Network failures inevitable (cables cut, router failures)',
          'Too expensive',
          'Not needed'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is strong consistency?',
        options: [
          'Data eventually syncs',
          'Every read reflects most recent write, linearizability guarantee',
          'No consistency',
          'Weak guarantees'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is tunable consistency in Cassandra?',
        options: [
          'Fixed consistency level',
          'Configure consistency per query (ONE, QUORUM, ALL)',
          'No consistency options',
          'Automatic only'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What type of system is Google Spanner under CAP theorem?',
        options: [
          'AP system',
          'CP system with strong consistency and TrueTime API',
          'CA system',
          'No classification'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'What is Read-Your-Writes consistency?',
        options: [
          'Read random data',
          'User always sees their own latest updates immediately',
          'Never read own writes',
          'Slow reads'
        ],
        correctIndex: 1,
      ),
      Question(
        question: 'For e-commerce, which component should use CP system?',
        options: [
          'Product catalog search',
          'Checkout payment processing (prevent double-charging)',
          'Product recommendations',
          'User reviews'
        ],
        correctIndex: 1,
      ),
    ],
  ),
  Topic(
    id: 'etl_pipelines',
    title: '23. ETL Processes & Data Pipelines',
    explanation: '''ETL (Extract, Transform, Load) processes move and transform data from sources to destinations. Data pipelines automate this flow for analytics and reporting.''',
    codeSnippet: '''-- ETL process example (pseudo-code)
-- Extract
SELECT * FROM source_db.sales;

-- Transform
UPDATE staging_table SET total = price * quantity;

-- Load
INSERT INTO warehouse.fact_sales 
SELECT * FROM staging_table;''',
    revisionPoints: [
      'ETL: Extract data from sources, Transform for consistency, Load into warehouse',
      'Data pipelines automate data flow for real-time or batch processing'
    ],
    quizQuestions: [
      Question(
        question: 'What does ETL stand for?',
        options: ['Extract, Transfer, Load', 'Extract, Transform, Load', 'Execute, Transform, Link', 'Export, Transfer, Log'],
        correctIndex: 1,
      ),
    ],
  ),
];

// Course List
final List<Course> courses = [
  Course(
    id: 'java',
    name: 'Java Programming',
    description: 'Learn the fundamentals of Java (23 Topics)',
    icon: '<>',  // Using code bracket icon
    topics: javaTopics,
  ),
  Course(
    id: 'dbms',
    name: 'Database Management Systems',
    description: 'Master SQL and database concepts (23 Topics)',
    icon: '≡',  // Using database/list icon
    topics: dbmsTopics,
  ),
];
